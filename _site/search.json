[
  {
    "objectID": "showcase.html",
    "href": "showcase.html",
    "title": "showcase",
    "section": "",
    "text": "Online Campaign Performance\n\n\nMarketing Analysis\n\n\n\n\nbusiness analytics\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2023\n\n\nLeonardo Cerliani\n\n\n\n\n\n\n  \n\n\n\n\nAnalysis of time-to-purchase (TTP)\n\n\nProduct Analysis Graded Task\n\n\n\n\n\n\n\n\n\nJul 8, 2023\n\n\nLeonardo Cerliani\n\n\n\n\n\n\n  \n\n\n\n\nInteractive data exploration in neuroimaging: a simple example\n\n\n\n\n\n\n\nshiny app\n\n\nneuroimaging\n\n\n\n\n\n\n\n\n\n\n\nDec 28, 2022\n\n\nLC\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LC",
    "section": "",
    "text": "leonardo cerliani\ndata analysis & brain imaging"
  },
  {
    "objectID": "secreto/monty-hall.html",
    "href": "secreto/monty-hall.html",
    "title": "LC",
    "section": "",
    "text": "Leonardo C.\n12 April 2023\nI finally got an understanding of the solution of the Monty Hall problem after watching this video from Lazy Programmer. Another explanation I found to be really clear is this Medium article from GreekDataGuy.\nI had tried a few times to solve the problem either by myself, or by reading other people’s solutions. I noticed that one of the reasons I was previously getting stuck with was by switching continously between my scribbles on the paper and the mental image of ****************************************where actually the car and the goats are****************************************. However in a real Monty Hall situation you do not get to see where the car is, and this is an important point that in my opinion should be kept as it is also when trying to solve the problem. After all, the rules of probability were devised exactly to reason in situations of uncertainty, but if we get to know in advance where the car actually is, the core uncertainty in the problem dissolves.\nSo what worked for me - and differently from all the other expositions of the Monty Hall problem I have seen so far - was not to start with “let’s assume that the car is behind door n”. Instead, let’s not assume anything about where the car actually is, and let’s focus only on the information that are actually available to us:\n\nthe door I choose, for instance door 1\nthe door that Monty Hall opens ************************after I have chosen door 1************************. For instance door 2.\n\nIndeed Monty **************************does not open one of the two remaining doors at random**************************. His choice of the door to be opened is conditioned - and in 2 out of 3 cases determined - by which door I choose, and by which door has the car behind it. In other words, he can only choose one door with a goat behind, among the two that I have not chosen.\nThis observation - the fact that the choice of which door Monty Hall opens is conditioned on which door I chose - is so natural and given for granted that I did not notice at first how crucial it is for calculating the probability of winning the car.\nSo, we know which door I initially chose, and which door Monty opens given my choice. The only ********information which is still uncertain is ****************where the car is****************, and we should focus our probability calculation on this, given what we know.\nSpecifically, we should focus our attention on calculating the probability that Monty Hall would choose to open the door he did open - door 2 in this case - in either one of the three cases in which the car is behind door 1, 2 or 3.\n\n\n\n\n\n\n\nProbability that Hall opens door 2 given that the car is behind door 1,2,3 and I chose door 1\nMotivation\n\n\n\n\n\\(p(H = 2 | C = 1) = 0.5\\)\nbecause goats are behind both doors 2 and 3\n\n\n\\(p(H = 2 | C = 2) = 0\\)\nbecause Monty Hall cannot open the door with the car\n\n\n\\(p(H = 2 | C = 3) = 1\\)\nbecause I chose door 1, and the car is behind door 2, so he can only open door 3\n\n\n\nOnce we have calculated these probabilities, we can get to the most important question: what is the probability that I will win the car in either of the two options: keep the initial choice of door 1, or switch to door 3 - as Monty Hall opened door 2.\n\n\n\n\n\n\n\n\\(p(C=1|H=2)\\)\nprobability of winning the car if I keep my original choice of door 1\n\n\n\n\n\\(p(C=3 | H = 2)\\)\nprobability of winning the car if I switch to door 3\n\n\n\nNote that the conditional probabilities I would like to calculate here are the opposite of those I previously estimated.\nTherefore to estimate \\(p(C|H)\\) I just need \\(p(H|C)\\) and Bayes’ rule:\n\\[\np(C|H) = \\frac{p(H|C) \\cdot p(C)}{p(H)}\n\\]\nWe know already our Prior, that is the probability of choosing the door with the car before knowing anything else: \\(p(C) = 1/3\\).\nWe also know the denominator of the equation, that is \\(p(H)\\) - or ***************************Marginal probability** of H* - since this is the weighted sum of all the initially calculated \\(p(H|C) = (1/2 + 0 + 1)/3 = 1/2\\).\nFinally, we know - again from the initially calculated conditional probabilities - what is the ******************************Likelihood********************* of both\n\n\\(p(H=2 | C=1) = p(Keep Door 1) = 0.5\\)\n\\(p(H=2 | C=3) = p(SwitchToDoor3) = 1\\)\n\nNow we just have to plug in our numbers to have the answers:\nIf I keep my original choice of door 1, the probability of having picked the door with the car is 1/3.\n\\[\np(C=1|H=2) = \\frac{p(H=2|C=1) \\cdot p(C=1)}{p(H=2)} = \\frac{1/2 \\cdot 1/3}{1/2} = 1/3\n\\]\nOn the other hand, if I switch to door 3, my probability of winning the car increases to 2/3.\n\\[\np(C=3|H=2) = \\frac{p(H=2|C=3) \\cdot p(C=3)}{p(H=2)} = \\frac{1 \\cdot 1/3}{1/2} = 2/3\n\\]"
  },
  {
    "objectID": "secreto/monty-hall.html#focus-on-the-right-question",
    "href": "secreto/monty-hall.html#focus-on-the-right-question",
    "title": "LC",
    "section": "Focus on the right question",
    "text": "Focus on the right question\nWhen we hear all the details of the problem, we realize there are ****many****, and we immediately try to form a mental movie with images - showing us where the car actually is - and sequences of event - first the guest’s choice of the door, then the choice of Monty Hall about which door to open. With this mental construct in mind, we start to work on the problem.\nThe main question is “what is the probability of me getting the car if I keep my choice or if I switch”. This is correct, but not complete. Probability is *******always******* conditional to something happening. When we formulate this question, we forget that the probability of getting the car is conditional to both:\n\nthe door that we just choose\nthe door behind which there is a car\nthe door that Monty Hall will open\n\nprecisely in ****this**** order. In fact, the decision of Monty Hall to open a certain door is conditional to both which door I chose and which door hides the car"
  },
  {
    "objectID": "blog/purrr_RManova/purrr_RManova.html",
    "href": "blog/purrr_RManova/purrr_RManova.html",
    "title": "How to replace for loops using purrr::map",
    "section": "",
    "text": "In many cases you need to repeat the same action across multiple objects, for instance loading many files, or computing summary statistics across many vectors of observations. Instead of repeating the same operation manually for every object - which is not only time consuming, but especially prone to mistakes - you can use for loops.\nHowever for can be quite verbose, and especially in case you need to nest them - i.e. running a loop inside a loop - it can be difficult to inspect the code for errors during the analysis and especially in the future.\nBase R already provides some functions to avoid the creation of for loops, with the family of apply functions. However sometimes the syntax can be different across functions, and still a bit verbose.\nThe tidyverse provides functions that help getting rid of for loops for good using the purrr package. Below there is just an example. More details can be found in the iteration chapter of R for Data Science and in the functionals chapter of Advanced R\n\n\nCode\nlibrary(tidyverse)\nlibrary(reactable)\noptions(digits=2)\n\n\nLet’s say you collected data in 8 different runs of an experiment. For instance the time, in seconds, spent freezing, running or grooming in 10 participants after a given stimulus in each subsequent run.\nFor our example we will create some random data. The code below creates 8 dataframes with 10 observations for three distinct variables. It already uses the map function that we are going to explain later, so for now you can just disregard it, and come back later to understand what it does as an excercise.\n\n\nCode\n1:8 %>% map( function(x) {\n  tibble(\n    SUBID = map(1:10, ~ paste0(\"sub_\",.x) ) %>% unlist(),\n    freezing = runif(10)*10 * log(x+1),\n    running = runif(10)*10,\n    grooming = runif(10)*10\n  ) %>%\n    write_csv(paste0(\"run_\",x,\".csv\"))\n})\n\n\nWe obtain 8 csv files with our data.\n\n\nCode\nmyfiles <- list.files(pattern = \".csv\", full.names = T)\nmyfiles\n\n\n[1] \"./run_1.csv\" \"./run_2.csv\" \"./run_3.csv\" \"./run_4.csv\" \"./run_5.csv\"\n[6] \"./run_6.csv\" \"./run_7.csv\" \"./run_8.csv\"\n\n\nCode\nread.csv(\"run_1.csv\")\n\n\n    SUBID freezing running grooming\n1   sub_1      2.7     9.7     1.24\n2   sub_2      6.9     5.1     6.35\n3   sub_3      1.1     5.6     0.32\n4   sub_4      1.7     8.6     9.19\n5   sub_5      2.6     7.6     3.53\n6   sub_6      5.4     8.5     0.74\n7   sub_7      4.1     5.9     0.29\n8   sub_8      2.3     4.2     1.02\n9   sub_9      5.8    10.0     3.53\n10 sub_10      3.6     6.8     2.60"
  },
  {
    "objectID": "blog/purrr_RManova/purrr_RManova.html#purrrmap",
    "href": "blog/purrr_RManova/purrr_RManova.html#purrrmap",
    "title": "How to replace for loops using purrr::map",
    "section": "purrr::map",
    "text": "purrr::map\nNow you want to load everything in the same dataframe (i.e. table), for instance to carry out a RM-ANOVA. You could use a for loop to load all the files:\n\n\nCode\nallruns = vector(mode = \"list\", length = 8)\n\nfor (run in 1:length(allruns)) {\n  allruns[[run]] <- read.csv( myfiles[[run]] )\n}\n\n# allruns\n\n\nOr you could use the map function inside the purrr package\n\n\nCode\nallruns <- map(myfiles, read.csv)\n\n# allruns\n\n\nIn other words you passed to every element of the list myfiles the function read.csv\nNote the advantages:\n\nyou do not need to write extra code to initialize an empty list, since the result is automatically stored in a list\nyou don’t need to provide the total number of files,\nthe syntax is much more concise (and when you get used to it, also much more readable)."
  },
  {
    "objectID": "blog/purrr_RManova/purrr_RManova.html#purrrmap2",
    "href": "blog/purrr_RManova/purrr_RManova.html#purrrmap2",
    "title": "How to replace for loops using purrr::map",
    "section": "purrr::map2",
    "text": "purrr::map2\nTo carry out the RM-ANOVA, you need to combine all the tables into one singe dataframe, but also retain information about the different run.\nThe idea is the same as before: you have a function that creates a column with the run numba in each run’s data table. This means that you want to provide two lists: (1) the list containing the table of each run and (2) the list of filenames.\n\n\nCode\nalldata <- map2(\n allruns, myfiles, function(run, file) {\n  run %>% mutate(run = file)\n }\n) %>% bind_rows()\n\n\nor with a more concise syntax:\n\n\nCode\nalldata <- map2_dfr(allruns, myfiles, ~ .x %>% mutate(run = .y))\n\n\nYou might have noticed that here I used a specific flavor of map, that is map_dfr, which returns a dataframe (or a tibble in the tidyverse language) instead of the default list, so that I can drop the final bind_rows()."
  },
  {
    "objectID": "blog/purrr_RManova/purrr_RManova.html#purrrpmap",
    "href": "blog/purrr_RManova/purrr_RManova.html#purrrpmap",
    "title": "How to replace for loops using purrr::map",
    "section": "purrr::pmap",
    "text": "purrr::pmap\nAs you might expect, there is also a function pmap which allows you to pass an arbitrary number of tables. I personally prefer this syntax since it allows me to pipe the list into it:\n\n\nCode\nalldata <- list(allruns, myfiles) %>% pmap_df(~ .x %>% mutate(run = .y))\n\nalldata %>% reactable(\n defaultColDef = colDef(\n  format = colFormat(digits = 2), minWidth = 50\n ),\n style = list(fontFamily = \"Arial Narrow\")\n)"
  },
  {
    "objectID": "blog/purrr_RManova/purrr_RManova.html#map-is-similar-to-group_by-for-dataframes",
    "href": "blog/purrr_RManova/purrr_RManova.html#map-is-similar-to-group_by-for-dataframes",
    "title": "How to replace for loops using purrr::map",
    "section": "map is similar to group_by for dataframes",
    "text": "map is similar to group_by for dataframes\nFinally, note that the map function - and its variation, such as pmap, is a similar operator for list to the group_by operator inside dataframes.\nFor instance let’s say that you want to get the mean and standard deviation for every variable in each run:\n\n\nCode\ndescriptives <- alldata %>% \n  group_by(run) %>%\n  summarise(\n    across(where(is.numeric), list(mean = mean, sd = sd)),\n    .groups = \"drop\"\n  ) %>% ungroup() \n\n\ndescriptives %>% reactable(\n defaultColDef = colDef(\n  format = colFormat(digits = 2), minWidth = 50\n ),\n style = list(fontFamily = \"Arial Narrow\")\n)"
  },
  {
    "objectID": "blog/post_001/post_001.html",
    "href": "blog/post_001/post_001.html",
    "title": "It is time to make a blog…",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "showcase/dimred_app/dimred_app.html",
    "href": "showcase/dimred_app/dimred_app.html",
    "title": "Interactive data exploration in neuroimaging: a simple example",
    "section": "",
    "text": "Konrad Lorenz once said that “It is a good morning exercise for a research scientist to discard a pet hypothesis every day before breakfast. It keeps him young” (ref). Having spent quite some years doing scientific research, I can comfortably say that this is one of the mantra for the morning ritual of every researcher. Another useful mantra on the same line is a quote by Richard Feynman: “you must not fool yourself and you are the easiest person to fool” (ref).\nFrom a practical standpoint, these two quotes provide an interesting perspective on why I think visualization is so useful and actually indispensable for a researcher and for everyone who works with data: images are a great device to show us when our hypotheses are correct and when they are wrong (or when there is something wrong in the data).\nVisualization is key not just to communicate a complex result in an intuitive way, but also for inspecting every step of the analysis, in order to validate - or invalidate - previous assumptions, avoid wrong conclusions, get useful insights to re(de)fine our hypotheses, and design the next analytic step.\nAnother reason why data visualization is tremendously helpful in data analysis is that data is always multifaceted: when you look at it from different perspectives (e.g. different summary statistics, for different variables, observations, or groups of them), it shows one of its many aspect, and usually a meaningful result stands out only when there is consistency among most of the perspectives from which we can look at our data: only if it squeaks like a duck, looks like a duck, walks like a duck, then most likely it is… an interesting result. I find images great for this: being able to inspect at once different images showing different aspects of the data is very helpful to reveal consistencies and inconsistencies both in our data and in our hypotheses.\nInteractive visualization takes this ability one step forward. Now we can carry out some kind of transformation in one feature of our data - e.g. selecting different groups of variables - and see immediately the effect of this in other aspects of our data.\nIn neuroimaging - the field where I worked for most time - this can be particularly useful, since the variables associated with both data pre-processing and actual data analysis are so many, that it is of paramount importance to check as much as possible each step of the process. Even restricing our focus to the actual analysis, the complexity associated with our research questions can grow very quickly: are the data from my participants homogeneous? How do they compare between the different tasks presented in the experiment? Is the effect restricted to only specific regions? Or maybe is it more evident when looking at sets (and potentially networks) of brain regions?\nIn the following I will present a basic example of how different visualizations of the same data can help to inspect and gain insights on the results of a (fictitious) fMRI experiment. I hope to show that while exploring even a simple dataset, the complexity of questions that come to the mind grows very fast. This prompts us to go back to the code, modify it, create new code chunks in our notebook for intermediate checks, and so on. This process can become very complex and - frankly - very messy.\nAt this point, we might think about creating a device that allows us to look at all the different questions we have generated, and see how they affect the representation that we are creating in our mind about the data. Therefore, I will show an interactive version of it, where the different tables and plots we generated react when something is changed in one of them (e.g. selecting different brain regions).\nNot necessarily exploring data will prompt us to create an interactive app all the time, however its utility might become apparent for instance if we realize that such exploration steps might be useful for similar datasets in the future."
  },
  {
    "objectID": "showcase/dimred_app/dimred_app.html#a-simple-fmri-experiment",
    "href": "showcase/dimred_app/dimred_app.html#a-simple-fmri-experiment",
    "title": "Interactive data exploration in neuroimaging: a simple example",
    "section": "A simple fMRI experiment",
    "text": "A simple fMRI experiment\nSuppose you want to compare brain activity during the execution of three different tasks: AAA, BBB and CCC. These tasks are behaviourally different, and your hypothesis is that this is reflected in which brain regions are recruited to perform each task. To test this hypothesis you set up an fMRI experiment where 30 participants execute many times each task while being in the scanner (this is an extremely simplified description of an fMRI experiment). The brain activity is recorded in each brain region, but for simplicity here we will focus only on nine of them.\nFor the purpose of showing the utility of our visual exploration of the data, we will generate fictitious data which is in accordance with the hypothesis. Specifically, I will generate summary statistics (mean activity) from 30 participants, showing that some brain regions are more active in task AAA and some in task CCC, while all examined brain regions do not appear to be particularly activated by task BBB.\n\n\nCode\nN = 30\n\nA <- function(mu) rnorm(N, mean = mu)\n\ndf <- tibble(\n task = c(rep(\"AAA\",N), rep(\"BBB\",N), rep(\"CCC\",N)) %>% as.factor(),\n  BA44  = c(A(2), A(2), A(5)),\n  BA45 = c(A(2), A(2), A(5)),\n  BA46  = c(A(2), A(3), A(5)),\n  V1  = c(A(2), A(2), A(2)),\n  V4  = c(A(2), A(2), A(2)),\n  V5  = c(A(2), A(2), A(2)),\n  SI   = c(A(5), A(2), A(2)),\n  SPL  = c(A(5), A(2), A(2)),\n  IPL  = c(A(5), A(3), A(2))\n) %>% \n group_by(task) %>%    # assign participant numbers\n # mutate(sub = paste0(\"sub_\",row_number())) %>%\n mutate(sub = paste0(\"sub_\",row_number(),\"_\",task)) %>%\n relocate(sub) %>%\n ungroup()\n \n\n# # plant an outlier\n# df[c(30,60,90),] %<>%  mutate(across(BA44:IPL, ~ .x + 4))\n\n# define a reusable function to print the table\nshow_table <- function(df) {\n \n BuYlRd <- function(x) {\n   rgb(colorRamp(c(\"#7fb7d7\", \"#ffffbf\", \"#fc8d59\"))(x), maxColorValue = 255)\n }\n \n  reactable(\n    df,\n   resizable = T,\n   # selection = \"multiple\",\n   # onClick = \"select\",\n   defaultColDef = colDef(\n     style = function(value) {\n        vals <- df %>% select(where(is.numeric))\n        if (!is.numeric(value)) return()\n        normalized <- (value - min(vals)) / (max(vals) - min(vals))\n        color <- BuYlRd(normalized)\n        list(background = color)\n      },\n     format = colFormat(digits = 2), minWidth = 50\n   ),\n   style = list(fontFamily = \"Arial narrow\", fontSize = \"13px\")\n  )\n}\n\ndf %>% show_table()\n\n\n\n\n\n\n\nThis is our complete dataset. It is very simplified (and clean) with respect to a real fMRI dataset, but it shows the many levels at which an fMRI can be examined:\n\nacross participants\nacross tasks\nacross brain regions\n\nand of course all the combinations of these levels. For instance we might be interested at the effect of a given task with respect to the other two in a specific brain region or in a specific set of brain regions.\nHere we start to see how quickly the complexity increases for the questions that we might ask even in a simple experiment like this. (It’s here that the interactive exploration will reveal its potential as we will see later).\nAlthough in this table I already color-coded brain the mean activity in each region for each task and each subject, it is quite difficult to have a clear view of the results."
  },
  {
    "objectID": "showcase/dimred_app/dimred_app.html#effect-of-task-in-different-brain-regions-across-participants",
    "href": "showcase/dimred_app/dimred_app.html#effect-of-task-in-different-brain-regions-across-participants",
    "title": "Interactive data exploration in neuroimaging: a simple example",
    "section": "Effect of task in different brain regions across participants",
    "text": "Effect of task in different brain regions across participants\nAt this point we want to check whether the effect we hypothesize is consistent across participants. This is done to achieve a robust estimate of our effect of interest (and of its variability). One basic visualization of this mean result is in the form of a table:\n\n\nCode\ndf_mean <- df %>% \n group_by(task) %>%\n summarise(\n  across(where(is.numeric), ~ mean(.x, na.rm = T)),\n  .groups = \"drop\"\n )\n\n# df_mean %>% show_table()\n\n# pivot to group all the brain regions in a variable\nt_df_mean <- df_mean %>% \n pivot_longer(\n  cols = !task, names_to = \"JU\", values_to = \"Zmean\"\n ) %>% \n pivot_wider(\n  names_from = task, values_from = Zmean\n )\n\nt_df_mean %>% show_table()\n\n\n\n\n\n\n\n\n\nNow the situation appears much more clear than when we looked at the data at the participant level, and indeed reflects how we actually generate the data: tasks CCC and AAA activate mostly the three top and bottom brain regions, respectively, while task BBB does not appear to activate any of the examined brain regions above baseline."
  },
  {
    "objectID": "showcase/dimred_app/dimred_app.html#a-different-visualization",
    "href": "showcase/dimred_app/dimred_app.html#a-different-visualization",
    "title": "Interactive data exploration in neuroimaging: a simple example",
    "section": "A different visualization",
    "text": "A different visualization\nConsider however that here we are looking only at 9 brain regions, while in reality - in a whole-brain analysis - we look at least at 50+ regions (e.g. the 52 Brodmann areas). That is why I prefer to look at the same result with a graphical representation such as the following one.\nNB: in the following graph you can select specific tasks by clicking on the legend\n\n\nCode\ndraw_spiderplot <- function(t_df_mean) {\n  \n  p <- plot_ly(\n    type = 'scatterpolar', mode = 'lines+markers', fill = 'toself', opacity = 0.5\n  ) %>% config(displayModeBar = F)\n  \n  df_vals <- t_df_mean %>% select(-JU)\n  ticks <-  t_df_mean$JU %>% as.character()\n  groups <- df_vals %>% colnames()\n  \n  for (ith_col in 1:length(groups)) {\n    \n    onecol <- df_vals[,ith_col] %>% pull()\n    \n    p <- p %>%\n      add_trace(\n        r = c(onecol, onecol[1]),\n        theta = c(ticks, ticks[1]),\n        name = groups[ith_col],\n        line = list(\n          dash = \"solid\",\n          shape = \"spline\",\n          smoothing = 1,\n          width = 2\n        )\n      ) %>% layout(font = list(family = \"arial narrow\"))\n  }\n  \n  return(p)\n}\n\ndraw_spiderplot(t_df_mean)"
  },
  {
    "objectID": "showcase/dimred_app/dimred_app.html#reducing-the-dimensionality-for-a-sharper-view",
    "href": "showcase/dimred_app/dimred_app.html#reducing-the-dimensionality-for-a-sharper-view",
    "title": "Interactive data exploration in neuroimaging: a simple example",
    "section": "Reducing the dimensionality for a sharper view",
    "text": "Reducing the dimensionality for a sharper view\nI find this visualization much easier to inspect than the table above. However, you might have noticed that we don’t know anymore what is going on at the level of the single participants. Of course are insights on the results so far are purely based on descriptive statistics, and appropriate inferential methods (e.g. ANOVA) will tell us whether the variability across tasks is substantially higher than that between participants. However, it can be useful to have a first look into this by looking at the main trajectories of variability across the entire initial dataset.\nLet’s see for instance what happens when we feed our initial participants-by-brain region table into UMAP.\n\n\nCode\ndo_dimred <- function(df, method, compX, compY, maxcomp=3) {\n  \n  # nsub <- length(df$task)\n  vals <- df %>% select(!task) %>% as.matrix()\n  \n  switch(method,\n         mds = {\n           mds <- cmdscale(dist(vals), eig = T, k = maxcomp)\n           pcs <- list(mds$points[,compX], mds$points[,compY])\n         },\n         umap = {\n           u <- umap(dist(vals), n_components = maxcomp)\n           pcs <- list(u[,compX], u[,compY])\n         },\n         tsne = {\n           tsne <- Rtsne(dist(vals), dims = 3, perplexity = 10)\n           pcs <- list(tsne$Y[,compX], tsne$Y[,compY])\n         }\n  )    \n  \n  df_lowdim <- tibble(\n    task = df$task,\n    sub = df$sub,\n    pc1 = pcs[[1]],\n    pc2 = pcs[[2]],\n  )\n  \n  plot_ly(type = 'scatter', mode = 'markers', source = \"A\") %>%\n    add_trace(\n      data = df_lowdim,\n      x = ~pc1,\n      y = ~pc2,\n      customdata = ~sub,\n      color = ~task,\n      text = ~sub,\n      hoverinfo = 'text'\n      # hovertemplate = paste('%{text}')\n    ) %>% \n    layout(dragmode = \"lasso\") %>% \n    config(displayModeBar = F)\n}\n\ndo_dimred(df,\"umap\", compX = 1, compY = 2)\n\n\n\n\n\n\n\n\nHere each point represents one participant performing one task. The data is grouped in three clusters reflecting - not surprisingly - the difference in brain activity across all particiants and between tasks.\nInterestingly, we see that the brain activity for task BBB in one participant is grouped with a cluster which is mostly populated with activity for task AAA (hover on the orange dot in the green cloud). To inspect this potential anomaly, we can look into the initial data table and inspect the original values. Eventually we can go back to the original fMRI data and - through appropriate examination - decide if something went wrong with the data of this participant, e.g. during data acquisition or pre-processing.\nThis situation is made more clear if we artificially “plant” an anomaly in our data. I will not run this here, but you can download this RMarkdown notebook and run it on your computer. Check out how the UMAP is able to spot the “outliers” which were planted in the data.\n\n\nCode\n# plant an outlier\ngf <- df\ngf[c(29,30,59,60,89,90),] %<>%  mutate(across(BA44:IPL, ~ .x + 4))\n\ndo_dimred(gf,\"umap\", compX = 1, compY = 2)\n\ngf %>% show_table()"
  },
  {
    "objectID": "showcase/dimred_app/dimred_app.html#bringing-all-together-in-an-interactive-app",
    "href": "showcase/dimred_app/dimred_app.html#bringing-all-together-in-an-interactive-app",
    "title": "Interactive data exploration in neuroimaging: a simple example",
    "section": "Bringing all together in an interactive app",
    "text": "Bringing all together in an interactive app\nWe have seen so far how many questions naturally arise even when looking at a intentionally very simplified dataset like the one we generated above.\nIf you read so far you have probably come up with a few other features that you would like to explore in one of the proposed visualization, and see how they affect the other perspectives from which we can look at the data.\nIt is probably clear by now that I am very intrigued by the possibility of exploration offered by dimensionality reduction methods such as UMAP. In the app which is displayed below, that you can open in full screen here, I implemented a few other possibilities that I encourage you to explore:\n\nFor instance, we might want to assess - for data exploration or based on a specific hypothesis - whether the expected effect would hold if you consider only a subset of the brain regions we looked at. This can be done by selecting the regions in the table, and observing the effect in the low-dimensional embedding.\nWe might also want to try to use other dimensionality reduction methoods besides UMAP (check for instance here). In the app, there is a choice of three different methods: Multi-dimensional Scaling (from the stats package), tSNE (from the Rtsne package) and UMAP (from the uwot package).\nAlso, to inspect the original values for each participant in each task, you just have to draw a lasso around the points you are interested in the low-dimensional embedding, and a heatmap of those values will appear in the lower-right corner of the app\n\nHopefully the potential usage of the app are self-explanatory, but if in doubt, check the animated gif at the beginning of this post."
  },
  {
    "objectID": "showcase/dimred_app/dimred_app.html#conclusion",
    "href": "showcase/dimred_app/dimred_app.html#conclusion",
    "title": "Interactive data exploration in neuroimaging: a simple example",
    "section": "Conclusion",
    "text": "Conclusion\nThis post has already become much bigger than what I expected. I hope you found it useful or at least interesting, and I would be glad to know what you think about it by dropping me an email.\nI just want to offer two small considerations to conclude.\n\nBuilding an interactive app for data exploration/analysis is something that I dreamt about ever since I started to learn Matlab many years ago. Nowadays with the introduction of libraries such as Shiny for R (and recently also for Python), it has become a task that anyone with an interest in coding can get started with in a relatively short time, and that I personally find satisfying on many different levels. When you get a bit of experience, prototyping a basic app can take as short as one or a few days (as a matter of fact it took me way more time to write this post than the app)\nIn this example I looked only at the participant-level results. Instead in neuroimaging a large amount of time and code is spent on pre-processing the data and checking the results of the many steps involved therein - e.g. checking the outcome of registering one brain to a template, or the whole-brain temporal profile of an fMRI volume after bandpass filtering or de-noising. Although many neuroimaging suite already did a great job in implementing GUIs, you might want to look into other features which are not currently offered, or you might just want to implement an interface for the pre-processing pipelines you wrote. Having this in mind, I believe that libraries like Shiny bring a great potential for developing such personalized tools for researcher in neuroimaging."
  },
  {
    "objectID": "showcase/Product_Analysis_psaraki/Product_Analysis.html",
    "href": "showcase/Product_Analysis_psaraki/Product_Analysis.html",
    "title": "Analysis of time-to-purchase (TTP)",
    "section": "",
    "text": "Code\nlibrary(bigrquery)\nlibrary(tidyverse)\nlibrary(highcharter)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(echarts4r)\nlibrary(broom)\nlibrary(flextable)\nlibrary(reactable)\nlibrary(GGally)\nlibrary(jtools)\nlibrary(ggstatsplot)\nlibrary(viridis)\n\n\n# avoid the conflict with MASS::select\nselect <- dplyr::select"
  },
  {
    "objectID": "showcase/Product_Analysis_psaraki/Product_Analysis.html#load-data-and-define-factors",
    "href": "showcase/Product_Analysis_psaraki/Product_Analysis.html#load-data-and-define-factors",
    "title": "Analysis of time-to-purchase (TTP)",
    "section": "Load data and define factors",
    "text": "Load data and define factors\nIn addition, we remove some non-informative details about the campaign.\n\n\nCode\ndf <- read_csv(\"Graded_Task_Data.csv\") %>% \n mutate_at(\n  c(\"user_pseudo_id\",\"country\",\"device\",\"OS\",\"browser\"), \n  factor\n ) %>% \n filter(country != \"(not set)\") %>% \n mutate(campaign = ifelse(campaign %in% c(\"<Other>\",\"(data deleted)\"), NA, campaign) ) %>% \n mutate(campaign = factor(campaign)) %>% \n filter(total_due > 0) %>%\n relocate(TTP)\n\n# summary(df)"
  },
  {
    "objectID": "showcase/Product_Analysis_psaraki/Product_Analysis.html#explore-overall-ttp-and-datapoints-per-country",
    "href": "showcase/Product_Analysis_psaraki/Product_Analysis.html#explore-overall-ttp-and-datapoints-per-country",
    "title": "Analysis of time-to-purchase (TTP)",
    "section": "Explore overall TTP and datapoints per country",
    "text": "Explore overall TTP and datapoints per country\nWe first notice that the Time-to-purchase (TTP) is very skewed towards short values, but has a very long right tail (high TTP).\nThe highest values are in part due to countries with very high median TTP values and very few datapoints - see the adjacent barplot and table.\nWe will therefore start by removing the countries with a median \\(TTP < 1.5*IQR(TTP)\\) of the distribution of the median TTP, where IQR(TTP) is the interquartile range of the TTP distribution. About 1% of all the datapoints are removed.\n\nTTP Distribution across countriesBarplot median TTPTable median TTPRemove countries with extreme medianTTP\n\n\nCountries above \\(1.5 * IQR(medianTTP)\\) will not be considered\n\n\nCode\n# Create a ggplot histogram\nggplot(df, aes(x = TTP)) +\n  geom_histogram(binwidth = 1200, color = \"black\", fill = \"lightblue\") +\n  labs(title = \"Distribution of TTP in seconds across all countries\", \n       x = \"Time to Purchase (TTP)\") +\n  # scale_x_continuous(labels = function(x) seconds_to_period(x)) +\n  theme_minimal() -> gg\n\nggplotly(gg)\n\n\n\n\n\n\n\n\n\n\nCode\ndf %>% \n select(country, TTP) %>% \n group_by(country) %>% \n summarise(\n  median_TTP = median(TTP),\n  percent_data = round(n()/nrow(df),4)*100\n ) %>% \n arrange(median_TTP) %>%\n mutate(name = fct_reorder(country, desc(median_TTP))) %>% \n ggplot(aes(x = median_TTP, y = name, \n            text = paste0(country,\"\\n percent data: \",percent_data) )) +\n theme_minimal() +\n theme(panel.grid = element_blank()) +\n geom_bar(stat = \"identity\", fill = \"lightblue\") +\n labs(\n  title = \"Median TTP per country\",\n  subtitle = \"Hover to inspect the % of datapoints in that country\",\n  x = \"median TTP\", \n  y = \"Country\"\n ) -> gg\n\nggplotly(gg)\n\n\n\n\n\n\n\n\nIt is possible to display the values in ascending/descending by clicking on the column header.\n\n\nCode\ndf %>% \n select(country, TTP) %>% \n group_by(country) %>% \n summarise(\n  country = unique(country),\n  median_TTP = median(TTP),\n  `% data` = round(n()/nrow(df),4)*100 \n ) %>% \n arrange(desc(`% data`)) %>% \n reactable(\n  searchable = T,\n  style = list(fontFamily = \"Calibri\")\n )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate the median TTP for each country\nmedianTTP_tbl <- df %>% \n select(country,TTP) %>% \n group_by(country) %>% \n summarise(median_TTP = median(TTP)) %>% \n arrange(desc(median_TTP))\n\nggplot(medianTTP_tbl %>% filter(median_TTP < 1e4), aes(y = median_TTP)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Median Time to Purchase\",\n       y = \"Median Time to Purchase (seconds)\") +\n  theme_minimal() -> gg\n\nggplotly(gg)\n\n\n\n\n\n\nCode\n# Remove countries with median TTP above 1.5 * IQR(medianTTP)\nthr_median_TTP <- medianTTP_tbl$median_TTP %>% median + 1.5 * IQR(medianTTP_tbl$median_TTP)\n\ndf <- df %>% \n group_by(country) %>% \n mutate(medianTTP = median(TTP)) %>% \n relocate(medianTTP) %>% \n filter(medianTTP <= thr_median_TTP) %>% \n ungroup()"
  },
  {
    "objectID": "showcase/Product_Analysis_psaraki/Product_Analysis.html#choice-of-the-reference-ttp",
    "href": "showcase/Product_Analysis_psaraki/Product_Analysis.html#choice-of-the-reference-ttp",
    "title": "Analysis of time-to-purchase (TTP)",
    "section": "Choice of the reference TTP",
    "text": "Choice of the reference TTP\nEven in countries where the median TTP is within acceptable range, we still have very long sessions. Very long sessions do not necessarily signal “outliers”, however before carrying out any analysis, we need to ask whether it makes sense to consider sessions of any length.\nSince eventually we are interested in assessing the effect of session length on revenues, we will operate a choice that takes into consideration both metrics - i.e. TTP and total_due.\nWe observe that:\n\nWhile plotting the raw TTP apparently shows that order amount decreases with increasing TTP, plotting order amount vs. log(TTP) reveals that order amount increase - at least in some orders - for sessions up to 40-60 minutes.\n80% of sessions last up to an hour, while 90% of sessions up to three hours\n\nWe will consider sessions up to an hour - and later assess the impact of shorter or longer sessions\n\nSession length decreases exponentiallyOrder value increases up to ~ 1 hourAverage Order ValueTest the increase up to ~ 1 hourRatio of high/low revenues increases up to ~ 1 hourTheshold TTP to 1 hour\n\n\nThe TTP appear to follow a power-law: the length of the sessions decreases exponentially, as shown by the log-transformed TTP and by the red lines indicating the 80th, 90th and 95th quantiles of the distribution of TTP.\nIn this plot, the order value appear to be decreasing with session time, however this is just due to the concentration of TTP in short sessions.\n\n\nCode\npar(mfrow = c(1,2))\n\npdf_TTP <- hist(df$TTP, main = \"Distribution of TTP\", xlab = \"TTP\", col = \"lightblue\")\npdf_logTTP <- hist(log(df$TTP), main = \"Distribution of log(TTP)\", xlab = \"log(TTP)\",\n                   col=\"lightblue\")\n\n\n\n\n\nCode\npar(mfrow = c(1,1))\n\npTTP <- plot(df$TTP, df$total_due, pch = 21, bg = \"lightblue\",\n     xlab = \"Time to Purchase (seconds)\",\n     ylab = \"Total Due in US$\",\n     main = \"TTP vs Total Due\")\n\n\nq <- quantile(df$TTP, probs = c(0.80, 0.90, 0.95)) %>% round\n\n# construct a plot function for the quantiles lines\nplot_quantiles <- function(q,n) {\n abline(v = q[[n]], col = \"red\", lwd = 2, lty = 2)\n text(q[[n]] -2000, 1500, names(q[n]), col = \"red\")\n}\n\n# plot the quantile lines\n1:length(q) %>% walk(~ plot_quantiles(q,.x))\n\n\n\n\n\n\n\nBy log-transforming TTP we observe that the length of the session has at least some effect on the amount of the order, up to 40-60 minutes. We will choose 1 hour as our threshold for the maximum session length, which gives us about 80% of the whole data.\nImportantly we will carry out analyses on log-transformed data to decrease the deviation of the data from normality.\n\n\nCode\nbreaks_log <- quantile(log(df$TTP), probs = seq(0.1,1,0.1))\n\nbreaks_hms <- quantile(log(df$TTP), probs = seq(0.1,1,0.1)) %>%\n exp() %>% \n round() %>% \n seconds_to_period %>% \n map_chr(~ sprintf(\"%02d:%02d:%02d\",hour(.x),minute(.x),second(.x)))\n\nxlabels <- paste0(seq(10,100,10),\"% - \", breaks_hms)\n\n# plot usd ~ logTTP\ndf %>% \n  ggplot(aes(x = log(TTP), y = total_due)) +\n  geom_point(fill = \"lightblue\", shape = 21, size = 2) +\n  geom_vline(xintercept = breaks_log, linetype = \"solid\", color = \"red\", linewidth = 0.3) +\n  geom_smooth(\n    method = \"loess\", formula = \"y ~ x\", color = \"lightgreen\", linetype = \"solid\", size = 1\n   ) +\n  theme_minimal() +\n  labs(title = \"TTP vs Total Due\",\n       x = \"Time to Purchase (seconds)\",\n       y = \"Total Due in US$\") +\n scale_x_continuous(\n  breaks = breaks_log, \n  labels = xlabels\n ) +\n theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nCode\n# # plot log(usd) ~ logTTP\ndf %>% \n # filter(TTP <= 3600) %>%\n filter(total_due >= 1) %>%\n ggplot(aes(x = log(TTP), y = total_due)) +\n  geom_point(fill = \"lightblue\", shape = 21, size = 2) +\n  geom_vline(xintercept = breaks_log, linetype = \"solid\", color = \"red\", linewidth = 0.3) +\n  geom_smooth(\n   method = \"loess\", formula = \"y ~ x\", \n   color = \"lightgreen\", linetype = \"solid\", linewidth = 1\n  ) +\n  theme_minimal() +\n  labs(title = \"TTP vs Total Due\",\n       x = \"Time to Purchase (seconds)\",\n       y = \"Log(Total Due) in US$\") +\n scale_x_continuous(\n  breaks = breaks_log, \n  labels = xlabels\n ) +\n scale_y_log10() +\n # coord_cartesian(ylim = c(0, 750)) +  # Set the y-axis limits\n theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(2,2))\n\nboxplot(df$total_due[df$total_due < 300], \n        horizontal = T, col = \"lightgreen\", \n        outpch = 3, outcol = \"lightgreen\", outcex = 1.5,\n        main = \"Total due (limited to < 200 US$)\")\n\nAOV <- round(sum(df$total_due) / nrow(df), 2)\n\nmedian_order_value <- median(df$total_due)\n\nhist(df$total_due, col = \"lightgreen\", main = \"Distribution of total_due\", xlab = \"total_due\")\n\nhist(df$total_due %>% log, col = \"lightgreen\", main = \"Distribution of log(total_due)\", xlab = \"log(total_due)\")\n\nqqnorm(log(df$total_due), col = \"lightgreen\")\nqqline(log(df$total_due),col = \"red\")\n\n\n\n\n\nAs a side note, the median order value is 48, very different from the average order value of 70.42 because there are orders extending up to 1500 USD. The order value follows a log-linear distribution\n\n\n\n\nCode\n# TTP between seconds_to_period(logTTP_range)\nlogTTP_range <- 6:9\n\n# Limit to 500 to have a better view of the distribution. \n# Stats with log are virtually identical \nthr_total_due <- 500\n\n# function to use hms of logTTP as grouping variable for the plot\nlogTTP_to_hms <- function(logTTP) {\n hms <- round(logTTP) %>% exp() %>% round %>% seconds_to_period() %>% \n  map_chr(~ sprintf(\"%02d:%02d:%02d\",hour(.x),minute(.x),second(.x)))\n}\n\n\nddf <- df %>% \n select(TTP, total_due) %>% \n mutate(logTTP = log(TTP)) %>% \n filter(logTTP >= min(logTTP_range) & logTTP <= max(logTTP_range)) %>% \n mutate(logTTP_hms = logTTP_to_hms(logTTP) ) %>% \n mutate(logUSD = log(total_due + 1)) %>% \n filter(total_due <= 500)\n\n\nggbetweenstats(\n data = ddf,\n x = logTTP_hms,\n y = total_due\n)\n\n\n\n\n\n\n\n\n\nCode\n# Proportion of high- vs. low-amount orders over time\n\nddf <- df %>% \n mutate(logTTP = log(TTP)) %>% \n select(logTTP, total_due) %>% \n mutate(ntile_logTTP = ntile(logTTP, 10)) %>% \n mutate(ntile_logUSD = ntile(log(total_due),10)) %>% \n group_by(ntile_logTTP, ntile_logUSD) %>% \n count() %>% \n ungroup(ntile_logUSD) %>% \n mutate(perctent_of_purchase = round(n/sum(n),4)*100 ) %>% \n select(-n) %>% \n arrange(ntile_logTTP)\n\n\n# Quantiles of TTP (for xaxis labels)\nq_TTP <- quantile(log(df$TTP), probs = seq(0.1, 1, 0.1) ) %>% \n exp() %>% seconds_to_period() %>% round() %>% \n map_chr(~ sprintf(\"%02d:%02d:%02d\",hour(.x), minute(.x),second(.x)))\n\n# Create a custom color palette using viridis colormap\ncustom_palette <- viridis(10)\n\n# Define the quantile values and labels\nquantile_probs <- seq(0.1, 1, 0.1)\nquantile_labels <- quantile(df$total_due %>% log(), probs = quantile_probs) %>% exp() %>% as.character()\n\n# Modify the levels and labels of ntile_logUSD\nddf$ntile_logUSD <- factor(ddf$ntile_logUSD, levels = 1:10, labels = quantile_labels)\n\nggplot(ddf, aes(x = factor(ntile_logTTP), y = perctent_of_purchase, fill = factor(ntile_logUSD))) + \n  geom_bar(stat = \"identity\", position = position_stack(reverse = TRUE), show.legend = TRUE) +\n  scale_fill_manual(values = custom_palette) +\n  xlab(\"ntile_logTTP\") +\n  ylab(\"Proportion of Purchase\") +\n  labs(fill = \"ntile_logUSD\") +\n  theme_minimal() +\n  scale_x_discrete(labels = as.character(q_TTP)) +\n  labs(\n    title = \"Proportion of low- and high-revenue orders in increasingly longer sessions\",\n    x = \"Session duration in hh:mm\",\n    y = \"Proportion of Purchases\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) -> gg\n\n# # If I use the following, the legend disappears in ggplotly\n# p <- p + guides(fill = guide_legend(reverse = TRUE), show.legend = TRUE)  # Reverse the order of the legend\n\nggplotly(gg, config = list(displayModeBar = FALSE))\n\n\n\n\n\n\n\n\nIf we wish to modify the range of TTP we consider, please enter the value of TTP in seconds below.\n\n\nCode\n# enter the desired value here to override the choice of 95% of TTP\nmaximum_TTP <- 3600\n\ndf_before_removing_extreme_TTP <- df\n\n# df <- df_before_removing_extreme_TTP\n\n# filter TTP < maximum_TTP and also create a column with its log\ndf <- df %>% \n filter(TTP <= maximum_TTP) %>% \n mutate(logTTP = log(TTP)) %>% \n relocate(TTP,logTTP)"
  },
  {
    "objectID": "showcase/Product_Analysis_psaraki/Product_Analysis.html#relationship-with-count-of-other-events",
    "href": "showcase/Product_Analysis_psaraki/Product_Analysis.html#relationship-with-count-of-other-events",
    "title": "Analysis of time-to-purchase (TTP)",
    "section": "Relationship with count of other events",
    "text": "Relationship with count of other events\nThere appear to be an association between TTP and some events, namely # of scrolls, page views, item views and user engagement. This is not surprising. However, such association is not straightforward: users with short sessions record few events, while users with long sessions can record either few or many events. Overall this is not an interesting finding.\nA better analysis of these associations should consider not only the number but also the duration of the events. This will be explored in the future.\n\nTTPlogTTP\n\n\n\n\nCode\ndf %>%\n select(TTP, total_due, n_scrolls, n_page_view, n_user_engagement, n_view_item) %>% \n GGally::ggpairs()\n\n\n\n\n\n\n\n\n\nCode\ndf %>%\n select(logTTP, total_due, n_scrolls, n_page_view, n_user_engagement, n_view_item) %>% \n GGally::ggpairs()"
  },
  {
    "objectID": "showcase/Product_Analysis_psaraki/Product_Analysis.html#user-who-buy-on-sale-products-have-longer-sessions",
    "href": "showcase/Product_Analysis_psaraki/Product_Analysis.html#user-who-buy-on-sale-products-have-longer-sessions",
    "title": "Analysis of time-to-purchase (TTP)",
    "section": "User who buy ON SALE products have longer sessions",
    "text": "User who buy ON SALE products have longer sessions\n\nAbout 1 in 8 users buy goods which are in the ON SALE category of the webstore. We hypothesized that people buying items on sale would have quicker sessions, speculating that the sale might not last long or other people might take advantage of that.\nInstead, exactly the opposite happens: people who buy items on sale have overall longer sessions that people choosing items from categories with regular prices.\nNB: The ‘Sale’ information is taken from the items added to the cart, not just from the page viewed.\n\nTTP for ON SALE vs. other categoriesNumber of events in ON SALE vs regular price purchasesRevenues for ON SALE vs regular priceOrder amount vs time for ON SALE and regular price\n\n\n\n\nCode\nddf <- df %>% \n select(TTP, is_on_sale, total_due) %>%\n mutate(logTTP = log(TTP)) %>% \n mutate(is_on_sale = ifelse(is_on_sale == 0, \"regular_price\",\"on_sale\")) %>% \n mutate(is_on_sale = factor(is_on_sale)) \n\n\nggbetweenstats(\n   data = ddf,\n   x = is_on_sale,\n   y = TTP,\n   type = \"nonparametric\"\n) + scale_y_continuous(trans = \"log\", labels = function(x) seconds_to_period(round(x)))\n\n\n\n\n\n\n\nThe longer session is not specifically linked to any kind of event, although a more accurate analysis would require to quantify the duration of different kind of events.\n\n\nCode\ndf %>% \n select(is_on_sale, n_scrolls, n_page_view, \n        n_view_item, n_user_engagement) %>%\n mutate(is_on_sale = ifelse(is_on_sale == 0, \"regular_price\",\"on_sale\")) %>% \n # mutate(is_on_sale = factor(is_on_sale)) %>% \n pivot_longer(cols = starts_with(\"n_\"), names_to = \"event_type\") %>% \n ggplot(aes(x = event_type, y = value, fill = is_on_sale)) +\n geom_boxplot() +\n theme_minimal() +\n labs(\n  title = \"Events for purchases on sale vs. with regular price\",\n  x = \"Type of event\", y = \"# of recorded events\"\n )\n\n\n\n\n\n\n\nInterestingly, people who buy items from the ON SALE section of the website spend on average 25% more than from all the other categories.\nNote: total_due values are limited to a max of 300 USD for clarity of visualization.\n\n\nCode\nddf %>% \n # filter(total_due < 300) %>%\nggbetweenstats(\n x = is_on_sale,\n y = total_due,\n ylab = \"total due in US $\",\n type = \"nonparametric\"\n) + scale_y_continuous(trans = \"log\", labels = function(x) round(x))\n\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\nThe fact that people who choose the ON SALE category spend more is true for sessions of almost all durations\n\n\nCode\nddf %>%\n mutate(qtile_logTTP = ntile(logTTP, 10)) %>%\n group_by(qtile_logTTP) %>% \n mutate(median_qtile_logTTP = median(logTTP)) %>%\n ungroup() %>% \n group_by(is_on_sale,median_qtile_logTTP) %>% \n reframe(\n  is_on_sale = unique(is_on_sale),\n  median_qtile_logTTP = unique(median_qtile_logTTP) %>% exp %>% round,\n  median_total_due = median(total_due)\n ) %>% \n ggplot(aes(x = median_qtile_logTTP, y = median_total_due, color = is_on_sale)) +\n  geom_line() +\n  geom_point(size = 1.5, shape = 16) +\n  # scale_x_continuous(breaks = x_breaks, labels = x_labels) +\n  labs(title = \"Total Due vs LogTTP Decile\",\n       x = \"TTP\",\n       y = \"Total Due\") +\n  theme_minimal() +\n  scale_x_continuous(trans = \"hms\") -> gg\n\nggplotly(gg)"
  },
  {
    "objectID": "showcase/Product_Analysis_psaraki/Product_Analysis.html#returning-customers-are-slightly-faster-to-purchase",
    "href": "showcase/Product_Analysis_psaraki/Product_Analysis.html#returning-customers-are-slightly-faster-to-purchase",
    "title": "Analysis of time-to-purchase (TTP)",
    "section": "Returning customers are (slightly) faster to purchase",
    "text": "Returning customers are (slightly) faster to purchase\nThe range of session length for returning customers is comparable to new customers, however on average they spend a few minutes less (~ 4 minutes) on the website.\n\nReturning customers TTPReturning customers eventsReturning customers order amount\n\n\n\n\nCode\nddf <- df %>% \n select(TTP, logTTP, user_pseudo_id, event_date, total_due) %>%\n mutate(log_total_due = log(total_due)) %>% \n group_by(user_pseudo_id) %>% \n mutate(is_returning_customer = ifelse( event_date == min(event_date), 0, 1 )) %>% \n ungroup()\n\nggbetweenstats(\n data = ddf,\n x = is_returning_customer,\n y = TTP,\n type = \"nonparametric\"\n) + scale_y_continuous(labels = function(x) round(seconds_to_period(x)))\n\n\n\n\n\n\n\nOverall returning customers spend less time on any event, however the difference is not substantial.\n\n\nCode\ndf %>% \n group_by(user_pseudo_id) %>% \n mutate(is_returning_customer = ifelse( event_date == min(event_date), \"new_customer\", \"returning_customer\" )) %>%\n mutate(is_returning_customer = factor(is_returning_customer)) %>% \n ungroup() %>% \n select(is_returning_customer, n_scrolls, n_page_view, \n        n_view_item, n_user_engagement) %>%\n pivot_longer(cols = starts_with(\"n_\"), names_to = \"event_type\") %>% \n ggplot(aes(x = event_type, y = value, fill = is_returning_customer)) +\n geom_boxplot() +\n theme_minimal() +\n labs(\n  title = \"Events for purchases on sale vs. with regular price\",\n  x = \"Type of event\", y = \"# of recorded events\"\n )\n\n\n\n\n\n\n\nAs an additional information, returning customers do NOT spend more than new customers.\n\n\nCode\nggbetweenstats(\n data = ddf,\n x = is_returning_customer,\n y = total_due,\n type = \"nonparametric\"\n) + scale_y_continuous(trans = \"log\", labels = function(x) round(x))"
  },
  {
    "objectID": "showcase/Product_Analysis_psaraki/Product_Analysis.html#mobiledestop-and-browser-type-have-no-effect-on-ttp",
    "href": "showcase/Product_Analysis_psaraki/Product_Analysis.html#mobiledestop-and-browser-type-have-no-effect-on-ttp",
    "title": "Analysis of time-to-purchase (TTP)",
    "section": "Mobile/Destop and browser type have no effect on TTP",
    "text": "Mobile/Destop and browser type have no effect on TTP\n\nNo effect of mobile/desktop deviceNo effect of browser typeReferrals TTPReferrals order amount\n\n\n\n\nCode\nddf <- df %>%\n select(TTP, device, total_due) %>% \n mutate(logTTP = log(TTP)) %>% \n filter(device != \"tablet\")\n\n\nggbetweenstats(\n data = ddf,\n x = device,\n y = TTP,\n type = \"nonparametric\"\n) + scale_y_continuous(labels = function(x) seconds_to_period(x))\n\n\n\n\n\nCode\nggbetweenstats(\n data = ddf,\n x = device,\n y = total_due,\n ylab = \"total due in US$\",\n type = \"nonparametric\"\n) + scale_y_continuous(\n trans = \"log\", labels = function(x) format(round(x))\n)\n\n\n\n\n\n\n\n\n\nCode\nddf <- df %>%\n select(TTP, browser, total_due) %>% \n mutate(logTTP = log(TTP)) %>% \n filter(browser %in% c(\"Chrome\",\"Safari\"))\n\n\nggbetweenstats(\n data = ddf,\n x = browser,\n y = TTP,\n type = \"nonparametric\"\n) + scale_y_continuous(labels = function(x) seconds_to_period(x))\n\n\n\n\n\nCode\nggbetweenstats(\n data = ddf,\n x = browser,\n y = total_due,\n ylab = \"total due in US$\",\n type = \"nonparametric\"\n) + scale_y_continuous(\n trans = \"log\", labels = function(x) format(round(x))\n)\n\n\n\n\n\n\n\nEffect of referrals\nPurchases from referral links are slightly faster, but the effect is not substantial (~ 1 minute faster).\n\n\n\n\n\nCode\nddf <- df %>%\n ungroup() %>% \n select(TTP,campaign,total_due) %>%\n mutate(logTTP = log(TTP)) %>% \n mutate(campaign = as.character(campaign)) %>% \n mutate(campaign = ifelse(is.na(campaign),\"(none)\",campaign)) %>% \n filter(campaign %in% c(\"(none)\",\"(referral)\"))\n # group_by(campaign) %>% \n # count()\n\nggbetweenstats(\n data = ddf,\n x = campaign,\n y = TTP,\n type = \"nonparametric\"\n) + scale_y_continuous(labels = function(x) seconds_to_period(x))\n\n\n\n\n\nCode\n# df$campaign %>% unique\n\n\n\n\nAs an additional information, returning customers do NOT spend more than new customers.\n\n\nCode\nggbetweenstats(\n data = ddf,\n x = campaign,\n y = total_due,\n ylab = \"total due in US$\",\n type = \"nonparametric\"\n) + scale_y_continuous(\n trans = \"log\", labels = function(x) format(round(x))\n)"
  },
  {
    "objectID": "showcase/Product_Analysis_psaraki/Product_Analysis.html#effect-of-referrals",
    "href": "showcase/Product_Analysis_psaraki/Product_Analysis.html#effect-of-referrals",
    "title": "Analysis of time-to-purchase (TTP)",
    "section": "Effect of referrals",
    "text": "Effect of referrals\nPurchases from referral links are slightly faster, but the effect is not substantial (~ 1 minute faster)."
  },
  {
    "objectID": "showcase/Product_Analysis_psaraki/Product_Analysis.html#seasonality",
    "href": "showcase/Product_Analysis_psaraki/Product_Analysis.html#seasonality",
    "title": "Analysis of time-to-purchase (TTP)",
    "section": "Seasonality",
    "text": "Seasonality\n\nDay levelWeek levelBy part of the dayBy hour\n\n\n\n\nCode\n# df %>%\n#  select(TTP,event_date) %>%\n#  group_by(event_date) %>% \n#  reframe(\n#   median_TTP = median(TTP),\n#   MAD_TTP = mad(TTP)\n#  ) %>% \n#  ggplot(aes(x = event_date, y = median_TTP)) +\n#  geom_line() +\n#   geom_ribbon(\n#    aes(ymin = median_TTP - MAD_TTP, ymax = median_TTP + MAD_TTP), \n#    fill = \"lightblue\", alpha = 0.5\n#   ) +\n#  theme_minimal() +\n#  theme(axis.text.x = element_text(angle = 90, hjust = 1)) -> gg\n# \n# gg\n# ggplotly(gg)\n\n\n\nddf <- df %>%\n select(TTP,event_date) %>%\n group_by(event_date) %>%\n reframe(\n  median_TTP = median(TTP),\n  MAD_TTP = mad(TTP),\n  MAD_low = floor(median(TTP) - mad(TTP)),\n  MAD_high = ceiling(median(TTP) + mad(TTP))\n )\n\nddf %>% \n  hchart(type = \"line\", hcaes(x = event_date, y = median_TTP), name = \"median TTP\") %>%\n  hc_add_series(\n   data = ddf, type = \"arearange\", name = \"MAD\", \n   hcaes(x = event_date, low = MAD_low, high = MAD_high),\n   color = \"lightblue\", opacity = 0.5 \n  ) %>%\n  hc_xAxis(type = \"datetime\") %>%\n  hc_chart(zoomType = \"x\") %>%\n  hc_tooltip(shared = TRUE) %>% \n  hc_title(text = \"TTP Range per day\") \n\n\n\n\n\n\n\n\n\n\n\nCode\nddf <- df %>% \n select(TTP,event_date) %>%\n mutate(year = year(event_date)) %>% \n mutate(week_number = paste0(year,\"-\",week(event_date)) ) %>%\n group_by(week_number) %>%\n reframe(\n  median_TTP = median(TTP),\n  MAD_TTP = mad(TTP),\n  MAD_low = floor(median(TTP) - mad(TTP)),\n  MAD_high = ceiling(median(TTP) + mad(TTP))\n ) %>% \n ungroup()\n \n\nddf %>% \n hchart(type = \"column\", hcaes(y = median_TTP, x = week_number), name = \"median TTP\") %>% \n hc_add_series(\n  data = ddf, type = \"errorbar\", hcaes(y = median_TTP, low = MAD_low, high = MAD_high),\n  name = \"MAD\"\n ) %>% \n hc_title(text = \"TTP Range per week\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nddf <- df %>% \n select(TTP, total_due, min_purchase_time) %>%\n rename(purchase_time = min_purchase_time) %>% \n mutate(\n    day_segment = case_when(\n      between(hour(purchase_time), 7, 12) ~ \"Morning\",\n      between(hour(purchase_time), 13, 17) ~ \"Afternoon\",\n      between(hour(purchase_time), 18, 24) ~ \"Evening\",\n      TRUE ~ \"Night\"\n    )\n  ) %>% \n group_by(day_segment) %>% \n reframe(\n  median_TTP = median(TTP),\n  MAD_TTP = mad(TTP),\n  MAD_low = floor(median(TTP) - mad(TTP)),\n  MAD_high = ceiling(median(TTP) + mad(TTP))\n ) %>% \n ungroup()\n\n# plot\nddf %>% \n arrange(match(day_segment, c(\"Morning\", \"Afternoon\", \"Evening\", \"Night\") )) %>% \n hchart(type = \"column\", hcaes(y = median_TTP, x = day_segment), name = \"day segment\") %>% \n hc_add_series(\n  data = ddf, type = \"errorbar\", hcaes(y = median_TTP, low = MAD_low, high = MAD_high),\n  name = \"MAD\"\n ) %>% \n hc_title(text = \"TTP Range per day segment\")\n\n\n\n\n\n\n\nCode\n# --- comparison below -----\nddf <- df %>% \n select(TTP, total_due, min_purchase_time) %>%\n rename(purchase_time = min_purchase_time) %>% \n mutate(\n    day_segment = case_when(\n      between(hour(purchase_time), 7, 12) ~ \"Morning\",\n      between(hour(purchase_time), 13, 17) ~ \"Afternoon\",\n      between(hour(purchase_time), 18, 24) ~ \"Evening\",\n      TRUE ~ \"Night\"\n    )\n  )\n\nggbetweenstats(\n data = ddf,\n x = day_segment,\n y = total_due,\n type = \"nonparametric\"\n) + scale_y_continuous(trans = \"log\", labels = function(x) round(x))\n\n\n\n\n\n\n\n\n\nCode\n# median TTP\nddf <- df %>% \n select(TTP, total_due, min_purchase_time) %>%\n mutate(hour_purchase = hour(min_purchase_time)) %>%  \n group_by(hour_purchase) %>% \n reframe(\n  median_TTP = median(TTP),\n  MAD_TTP = mad(TTP),\n  MAD_low = floor(median(TTP) - mad(TTP)),\n  MAD_high = ceiling(median(TTP) + mad(TTP))\n ) %>% \n ungroup()\n\n\n# plot\nddf %>% \n hchart(type = \"column\", hcaes(y = median_TTP, x = hour_purchase), name = \"hour\") %>% \n hc_add_series(\n  data = ddf, type = \"errorbar\", hcaes(y = median_TTP, low = MAD_low, high = MAD_high),\n  name = \"MAD\"\n ) %>% \n hc_title(text = \"TTP Range per hour segment\")\n\n\n\n\n\n\n\nCode\n# median total_due\nddf <- df %>% \n select(TTP, total_due, min_purchase_time) %>%\n mutate(hour_purchase = hour(min_purchase_time)) %>%  \n group_by(hour_purchase) %>% \n reframe(\n  median_USD = median(total_due),\n  MAD_USD = mad(total_due),\n  MAD_low = floor(median(total_due) - mad(total_due)),\n  MAD_high = ceiling(median(total_due) + mad(total_due))\n ) %>% \n ungroup()\n\n\n\n# plot\nddf %>% \n hchart(type = \"column\", hcaes(y = median_USD, x = hour_purchase), name = \"hour\") %>% \n hc_add_series(\n  data = ddf, type = \"errorbar\", hcaes(y = median_USD, low = MAD_low, high = MAD_high),\n  name = \"MAD\"\n ) %>% \n hc_title(text = \"US$ per hour segment\")"
  },
  {
    "objectID": "showcase/Marketing_Analysis_psaraki/Marketing_Analysis.html",
    "href": "showcase/Marketing_Analysis_psaraki/Marketing_Analysis.html",
    "title": "Online Campaign Performance",
    "section": "",
    "text": "Code\nlibrary(bigrquery)\nlibrary(tidyverse)\nlibrary(highcharter)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(echarts4r)\nlibrary(broom)\nlibrary(flextable)\nlibrary(reactable)\nlibrary(GGally)\nlibrary(jtools)\nlibrary(ggstatsplot)\nlibrary(viridis)\nlibrary(ggpubr)\nlibrary(htmltools)\nlibrary(ggpubr)\n\n# library(dlookr)\n\n\n# avoid the conflict with MASS::select\nselect <- dplyr::select\n\n# local folder\n# \"~/Dropbox/turing_college/Modules/Marketing_Analysis/\""
  },
  {
    "objectID": "showcase/Marketing_Analysis_psaraki/Marketing_Analysis.html#fail-to-increase-conversion-rate",
    "href": "showcase/Marketing_Analysis_psaraki/Marketing_Analysis.html#fail-to-increase-conversion-rate",
    "title": "Online Campaign Performance",
    "section": "Fail to increase conversion rate",
    "text": "Fail to increase conversion rate\nOverall, we don’t obseve a difference in the conversion rate - from browsing only to purchasing - due to campains\nWhen no campaigns are present on the website, 7% of all the visits result in a purchase. Comparably, when a campaign is running, 6% of the visitors end up purchasing.\nThe conversion rate for campaign sessions appears to be slightly higher on Thursday and Friday, but not more than what can be expected by chance.\n\nOverall conversion ratePer weekdayNo significant difference in proportions\n\n\n\n\nCode\ncontingency_table <- df %>% \n  select(campaign, purchase_session) %>% \n  group_by(campaign, purchase_session) %>% \n  count() %>% \n  pivot_wider(names_from = purchase_session, values_from = n, values_fill = 0)\n\n# --------------  ggbarstats --------------------------\n\n# Conversion rate across all days\nggbarstats(\n data = df,\n x = purchase_session,\n y = campaign,\n label = \"both\",\n results.subtitle = F\n) + \n scale_fill_manual(values = c(\"#F8766D\",\"#00BFC4\")) +\n labs(\n  title = \"Conversion rate in all sessions (campaign and no-campaign)\",\n  x = \"Campaign\"\n )\n\n\n\n\n\nCode\n# The following two are replaced by the line plot in the next tab\n\n# # Conversion rate for each day in campaign sessions\n# ggbarstats(\n#  data = df %>% filter(campaign == \"YES\"),\n#  x = purchase_session,\n#  y = weekday_name,\n#  label = \"both\",\n#  results.subtitle = F\n# ) +\n#  scale_fill_manual(values = c(\"#F8766D\",\"#00BFC4\")) +\n#  labs(\n#   title = \"Conversion rate in campaign sessions\",\n#   x = \"Day of the week\"\n#  )\n# \n# \n# \n# # Conversion rate for each day in no-campaign sessions\n# ggbarstats(\n#  data = df %>% filter(campaign == \"NO\"),\n#  x = purchase_session,\n#  y = weekday_name,\n#  label = \"both\",\n#  results.subtitle = F\n# ) +\n#  scale_fill_manual(values = c(\"#F8766D\",\"#00BFC4\")) +\n#  labs(\n#   title = \"Conversion rate in NO-campaign sessions\",\n#   x = \"Day of the week\"\n#  )\n\n\n\n\n\n\nCode\n# Barplot and line plot of the conversion rates\nddf <- df %>%\n group_by(weekday_name, campaign, purchase_session) %>% \n count() %>% \n group_by(weekday_name, campaign) %>% \n mutate(total = sum(n)) %>% \n filter(purchase_session == \"YES\") %>% \n mutate(conversion_rate = round(n/total*100,1))  \n # select(weekday_name, campaign, conversion_rate)\n\n\nddf %>%\n mutate(campaign = fct_rev(campaign)) %>% \n ggplot(\n  aes(x = weekday_name, y = conversion_rate, color = campaign, \n      group = campaign,\n      text = paste(\"Campaign:\", campaign, \"<br>Conversion Rate:\", conversion_rate, \"%\"))\n ) +\n # geom_bar(aes(fill = campaign), stat = \"identity\", position = \"dodge\") +\n geom_line() +\n geom_point() +\n theme_minimal() +\n labs(\n  title = \"Conversion rate for campaing and no-campaign sessions\",\n  x = \"Day of the week\",\n  y = \"Conversion Rate\"\n ) +\n scale_y_continuous(labels = scales::percent_format(scale = 1)) -> gg\n\nggplotly(gg, tooltip = \"text\", config = list(displayModeBar = FALSE))\n\n\n\n\n\n\n\n\n\n\nCode\n# Wednesday\nprop_Thu <- ddf %>% \n filter(weekday_name == \"Wed\") %>% \n rename(n_converted = n) %>% \n ungroup() %>% \n select(n_converted, total) %>% \n arrange(total)\n\ncat(\"Wednesday\")\n\n\nWednesday\n\n\nCode\nprop.test(x = prop_Thu$n_converted, n = prop_Thu$total, alternative = \"greater\")\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  prop_Thu$n_converted out of prop_Thu$total\nX-squared = 1.388e-28, df = 1, p-value = 0.5\nalternative hypothesis: greater\n95 percent confidence interval:\n -0.03071573  1.00000000\nsample estimates:\n    prop 1     prop 2 \n0.07179487 0.07021663 \n\n\nCode\n# Thursday\nprop_Thu <- ddf %>% \n filter(weekday_name == \"Thu\") %>% \n rename(n_converted = n) %>% \n ungroup() %>% \n select(n_converted, total) %>% \n arrange(total)\n\ncat(\"Thursday\")\n\n\nThursday\n\n\nCode\nprop.test(x = prop_Thu$n_converted, n = prop_Thu$total, alternative = \"greater\")\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  prop_Thu$n_converted out of prop_Thu$total\nX-squared = 1.2833e-28, df = 1, p-value = 0.5\nalternative hypothesis: greater\n95 percent confidence interval:\n -0.02947896  1.00000000\nsample estimates:\n    prop 1     prop 2 \n0.06349206 0.06127591 \n\n\nCode\n# Friday\nprop_Fri <- ddf %>% \n filter(weekday_name == \"Fri\") %>% \n rename(n_converted = n) %>% \n ungroup() %>% \n select(n_converted, total) %>% \n arrange(total)\n\n\ncat(\"Friday\")\n\n\nFriday\n\n\nCode\nprop.test(x = prop_Fri$n_converted, n = prop_Fri$total, alternative = \"greater\")\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  prop_Fri$n_converted out of prop_Fri$total\nX-squared = 2.6122e-29, df = 1, p-value = 0.5\nalternative hypothesis: greater\n95 percent confidence interval:\n -0.03484194  1.00000000\nsample estimates:\n    prop 1     prop 2 \n0.08187135 0.07921039"
  },
  {
    "objectID": "showcase/Marketing_Analysis_psaraki/Marketing_Analysis.html#session-length-differ-but-not-in-an-interesting-way",
    "href": "showcase/Marketing_Analysis_psaraki/Marketing_Analysis.html#session-length-differ-but-not-in-an-interesting-way",
    "title": "Online Campaign Performance",
    "section": "Session length differ, but not in an interesting way",
    "text": "Session length differ, but not in an interesting way\nCampaign sessions and purchase sessions (either on- or off-campaign) are significantly slighltly longer (up to 5 minutes) , however there is no interaction between campaign and session type (browse-only or purchasing). For instance, it is not the case that campaign sessions are longer for purchase and shorter for browse-only with respect to no-campaign sessions.\nAs expected from the previous anova, campaign sessions are slightly longer each day of the week, although the median value is associated with high variability.\nOn Saturday the purchasing campaign session is on average much shorter than the sessions not related to a campaign. However, a closer inspection shows that these are just 4 purchasing events, which makes a statistical comparison meaningless.\n\nSession time by conversion and campaignSession length during or outside a campaignSession length for Purchasing sessions (TTP)Browse-only sessions (no purchase)\n\n\nNB: Boxplots show session time in seconds, however anova2 was run on log(session_time) to approximate normally distributed values.\n\n\nCode\nddf <- df %>% \n select(TTP, campaign, purchase_session) %>%\n mutate(logTTP = log(TTP)) %>% \n mutate(campaign = fct_rev(campaign))\n\n# # Table\n# ddf %>%\n#  group_by(campaign, purchase_session) %>%\n#  reframe(mean_TTP = mean(TTP))\n\n\nddf %>% \n select(logTTP, campaign, purchase_session) %>% \n ggboxplot(\n  x = \"purchase_session\", y = \"logTTP\", color = \"campaign\"\n ) +\n scale_y_continuous(trans = \"exp\", labels = function(x)(round(exp(x)))) +\n labs(\n  title = \"Difference in session length\",\n  subtitle = \"for purchase/browse-only during and off campaign\",\n  y = \"Session length in seconds\",\n  x = \"Is purchase session\"\n )\n\n\n\n\n\nCode\nanova2 <- aov(formula = logTTP ~ campaign * purchase_session, data = ddf)\ncat(\"Session time is higher for campaign and purchase, but there is no interaction\")\n\n\nSession time is higher for campaign and purchase, but there is no interaction\n\n\nCode\nsummary(anova2)\n\n\n                             Df Sum Sq Mean Sq F value   Pr(>F)    \ncampaign                      1      5    4.99  13.181 0.000283 ***\npurchase_session              1    300  299.77 791.219  < 2e-16 ***\ncampaign:purchase_session     1      1    0.55   1.458 0.227199    \nResiduals                 55688  21099    0.38                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# plot(anova2)\n# car::Anova(anova2, type = \"III\")\n\n\n\n\n\n\nCode\n# define a fn to produce the three following plots\nplot_session_length <- function(df, title) {\n df %>% \n group_by(weekday_name, campaign) %>% \n reframe(\n  weekday_name = unique(weekday_name),\n  median_TTP = median(TTP),\n  MAD_low = floor(median(TTP) - mad(TTP)),\n  MAD_high = ceiling(median(TTP) + mad(TTP))\n ) %>%\n mutate(campaign = fct_rev(campaign)) %>%\n ggplot(aes(x = weekday_name, y = median_TTP, fill = campaign)) +\n geom_col(position = \"dodge\") +\n geom_errorbar(\n  aes(ymin = MAD_low, ymax = MAD_high),\n  position = position_dodge(width = 0.9),  # Adjust width for dodge position\n  width = 0.2, color = \"grey\"  # Width of error bars\n ) +\n labs(title = title, \n      x = \"Day of the week\",\n      y = \"Median TTP\") +\n theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    panel.grid.major.x = element_blank(),  # Remove vertical grid lines\n    panel.grid.minor = element_blank()     # Remove minor grid lines\n  ) -> gg\n\n return(ggplotly(gg) %>% config(displayModeBar = FALSE))\n}\n\n\n# overall session length (purchase and browse-only sessions)\nplot_session_length(\n df,\n title = \"Overall session length in- and outside campaign\"\n)\n\n\n\n\n\n\n\n\n\n\nCode\nplot_session_length(\n df %>% filter(purchase_session == \"YES\"),\n title = \"Purchase sessions length in- and outside campaign\"\n)\n\n\n\n\n\n\nCode\n# # The following compares TTP for campaign and no-campaign\n# # on Saturday, when the TTP appears shorter. However, a closer\n# # inspection reveals that these are just 4 observations, therefore\n# # a statistical test is meaningless\n\n# ddf <- df %>%\n#  filter(purchase_session == \"YES\", weekday_name == \"Sat\") %>%\n#  select(campaign, TTP)\n# \n# ggbetweenstats(\n#  data = ddf,\n#  x = campaign,\n#  y = TTP\n# ) + \n#  scale_y_continuous(trans = \"log\")\n#\n# ddf %>% filter(campaign == \"YES\")\n\n\n# # The following carries out an anova TTP ~ weekday * campaign\n# # There are no significant differences\n# ddf <- df %>% \n#  filter(purchase_session == \"YES\") %>% \n#  select(TTP, campaign, weekday_name) %>% \n#  mutate(logTTP = log(TTP))\n# \n# anova2 <- aov(TTP ~ campaign * weekday_name, data = ddf)\n# summary(anova2)\n# car::Anova(anova2, type = \"III\")\n\n\n\n\n\n\nCode\nplot_session_length(\n df %>% filter(purchase_session == \"NO\"),\n title = \"Browse-only sessions length in- and outside campaign\"\n)"
  },
  {
    "objectID": "showcase/Marketing_Analysis_psaraki/Marketing_Analysis.html#revenues-highest-at-mid-week",
    "href": "showcase/Marketing_Analysis_psaraki/Marketing_Analysis.html#revenues-highest-at-mid-week",
    "title": "Online Campaign Performance",
    "section": "Revenues highest at mid-week",
    "text": "Revenues highest at mid-week\nThe highest revenues for campaign-related orders were generated at mid-week, while they were much lower during the weekend and on Monday.\nThere are also differences in the median amount order between campaign and no-campaign sessions in some days, however not more than what could be expected by chance (assessed with an anova2).\n\nTotal revenue due to campaignsRange of order valuesTotal revenues for in- and out-campaign\n\n\n\n\nCode\ndf %>%\n  filter(purchase_session == \"YES\", campaign == \"YES\") %>%\n  select(weekday_name, revenue) %>% \n  group_by(weekday_name) %>% \n  reframe(\n    total_revenue = sum(revenue)\n  ) %>% \n  ggplot(aes(x = weekday_name, y = total_revenue, \n   text = paste(weekday_name, \"\\n\",\"Total Revenue: \", total_revenue, \"US$\"))\n  ) +\n  geom_bar(stat = \"identity\", fill = \"#F8766D\") +\n  labs(\n   title = \"Total revenue from campaign orders\",\n   x = \"Day of the week\",\n   y = \"Total revenue in USD\"\n  ) +\n  theme_minimal() -> gg\n\nggplotly(gg, tooltip = \"text\") %>% config(displayModeBar = FALSE)\n\n\n\n\n\n\n\n\n\n\nCode\ndf %>%\n filter(purchase_session == \"YES\") %>%\n select(weekday_name, revenue, campaign) %>% \n group_by(weekday_name, campaign) %>% \n mutate(campaign = fct_rev(campaign)) %>%\n ggplot(aes(x = weekday_name, y = revenue, fill = campaign)) +\n geom_boxplot() +\n # scale_y_continuous(limits = c(0,120)) +\n scale_y_continuous(breaks = seq(0, max(df$revenue), by = 100)) +\n labs(\n  title = \"Order value in- and outside-campaign\",\n  x = \"Day of the week\",\n  y = \"Revenue\"\n ) +\n theme_minimal() -> gg\n\n\nggplotly(gg) %>%\n layout(yaxis = list(range = c(0, 140))) %>% \n layout(boxmode = \"group\")\n\n\n\n\n\n\nCode\n# Anova 2 : revenue ~ weekday_name * campaign\n# No significant difference.\n\nddf <- df %>% \n filter(purchase_session == \"YES\") %>% \n select(revenue, weekday_name, campaign) %>% \n mutate(log_revenue = log(revenue))\n\ncat(\"No significant effect of of mean log_revenue or campaign\")\n\n\nNo significant effect of of mean log_revenue or campaign\n\n\nCode\nanova2 <- aov(log_revenue ~ weekday_name * campaign, data = ddf)\nsummary(anova2)\n\n\n                        Df Sum Sq Mean Sq F value Pr(>F)\nweekday_name             6    4.5  0.7583   0.994  0.427\ncampaign                 1    0.0  0.0211   0.028  0.868\nweekday_name:campaign    6    3.4  0.5662   0.742  0.615\nResiduals             3870 2951.9  0.7628               \n\n\nCode\n# Note that revenues are log-normally distributed\n# however this has no effect on the anova2\ndf$revenue[df$revenue > 0] %>% log() %>% hist(main = \"Log(revenue)\")\n\n\n\n\n\n\n\n\n\nCode\ndf %>%\n  filter(purchase_session == \"YES\") %>%\n  select(weekday_name, revenue, campaign) %>% \n  group_by(weekday_name, campaign) %>% \n  mutate(campaign = fct_rev(campaign)) %>%\n  reframe(\n    total_revenue = sum(revenue)\n  ) %>% \n  ggplot(aes(\n   x = weekday_name, y = total_revenue, \n   fill = campaign, \n   text = paste(weekday_name, \"\\n\",\"Total Revenue: \", total_revenue, \"\\n\", \"Campaign: \", campaign))\n  ) +\n  geom_bar(stat = \"identity\") +\n  # scale_y_continuous(\n  #   trans = \"log\",\n  #   labels = function(x) round(x)\n  # ) +\n theme_minimal() -> gg\n\nggplotly(gg, tooltip = \"text\")"
  },
  {
    "objectID": "showcase/Marketing_Analysis_psaraki/Marketing_Analysis.html#roi-91.17",
    "href": "showcase/Marketing_Analysis_psaraki/Marketing_Analysis.html#roi-91.17",
    "title": "Online Campaign Performance",
    "section": "ROI: 91.17%",
    "text": "ROI: 91.17%\n\n\nCode\n# NB: here we consider again the initial data, with no threshold for session duration\n\nnov <- read_csv(\"01_nov.csv\")\ndec <- read_csv(\"02_dec.csv\")\njan <- read_csv(\"03_jan.csv\")\n\ntotal_df <- rbind(nov, dec, jan)\n\nmarketing_cost <- read_csv(\"adsense_monthly.csv\")\ncost_of_campaigns <- sum(marketing_cost$Cost) %>% round(2)\n\n\ncampaigns_revenues <- total_df[total_df$campaign != \"none\", \"revenue\"] %>% na.omit() %>% sum\n\n# campaigns_revenues <- total_df %>% \n#  filter(campaign != \"none\", !is.na(revenue)) %>%\n#  select(revenue) %>% pull %>% sum\n\n\nROI <- campaigns_revenues / cost_of_campaigns\n\nROI_pct <- (round(ROI,4) - 1) * 100\n\nKPI_name <- \"Return on Investment\"\n\n\nReturn on Investment: The total cost for marketing campaigns was 2432.45 US$. The revenues generated by these campaigns were 4650 US$.\n\n\nCode\ndiv(\n  style = \"display: flex; flex-wrap: wrap; justify-content: space-around; align-items: center; font-family: sans-serif;\",\n  div(\n    style = \"background-color: #f39c12; color: white; padding: 10px; border-radius: 5px; margin: 10px; text-align: center;\",\n    div(style = \"font-size: 28px;\", KPI_name),\n    div(style = \"font-size: 36px;\", paste(ROI_pct, \"%\"))\n  )\n)\n\n\n\n\nReturn on Investment\n91.17 %\n\n\n\n\nCode\n# table with the cost of every marketing campaign\nmarketing_cost %>% \n group_by(Campaign) %>%\n reframe(\n  cost = round(sum(Cost),2)\n ) %>% flextable()\n\n\n\nCampaigncostBlackFriday_V1164.49BlackFriday_V2364.14Data Share Promo1,430.17Holiday_V1143.78Holiday_V2129.10NewYear_V144.30NewYear_V2156.47"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "blog",
    "section": "",
    "text": "How to replace for loops using purrr::map\n\n\n\n\n\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2022\n\n\nLC\n\n\n\n\n\n\n  \n\n\n\n\nIt is time to make a blog…\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nDec 23, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  }
]