{
  "hash": "b1ae6e94a42c0a71823d7cb8acb4b7ad",
  "result": {
    "markdown": "---\ntitle: \"Customer Segmentation\"\nsubtitle: \"RFM and CLV analysis\"\nauthor: \"Leonardo Cerliani\"\ndate: \"7/8/2023\"\noutput:\n  html_document:\n    self_contained: true\n    toc: true\n    toc_depth: 3\n    toc_float: true\n    code_folding: hide\n    highlight: pygments\n    theme: cerulean\n    css: styles.css\nimage: \"TTP.jpg\"\n---\n\n\n\n\nThis page describes the development of ****both**** the **Recency, Frequency, Monetary (RFM)** and the **Customer Lifetime Value (CLV)** project.\n\nThe CLV Hands-on is also present as the CLV Graded Task is an extension of the former.\n\n# RFM\n\n## RFM task description\n\nFor your graded task, you have to do RFM analysis with a given data set. The data set table is called \"rfm\" and can be found under \"turing_data_analytics\" database, placed in the [Turing College BigQuery project](https://console.cloud.google.com/bigquery?authuser=1&project=tc-da-1&d=turing_data_analytics&p=tc-da-1&t=rfm&page=table&ws=!1m5!1m4!4m3!1stc-da-1!2sturing_data_analytics!3srfm). Your tasks are:\n\n- Use only one year of data, 2010-12-01 to 2011-12-01.\n- Use SQL for calculation and data selection.\n- Calculate recency, frequency and money value and convert those values into R, F and M scores by using Quartiles, 1 to 4 values. In BigQuery, a function APPROX_QUANTILES is used to set the quartiles. You can check your results with rfm_value table and rfm_quantiles. Those tables contain intermediate calculations and are used in next steps(1 step: calculate RFM values, possible answer - rfm_value table, 2 step: calculate RFM quantiles from RFM values, possible answer - rfm_quantiles). **Important note: the answers in the tables are one of the possible answers. The results might vary due to data filtering options.**\n- Calculate recency from date 2011-12-01.\n- Calculate common RFM score. An example of a possible answer is given in the table rfm_score.\n- Present your analyses with a dashboard in Looker Studio or a similar visualisation tool. You can export data from BigQuery directly to Looker Studio.\n    - *This criteria is not applicable if your batch has started learning at Turing College in 2021, since for earlier batches, Visualisation Tools sprint comes later in the course*\n- Present some insights which customer group/customer groups should the marketing team get focus on.\n\n## RFM task development\n\nWe first collect the data and **calculate RFM scores as quartiles**, using SQL. \n\nThe **customer segmentation** will be done in Tableu. The reason for this separation of the steps is that the customer segmentation - in the way which is requested by the task - is intrinsically qualitative and benefits from several iterations with intermediate visual inspection of the results. In this perspective, doing the customer segmentation in Tableau is much more efficient than doing it in SQL. \n\nThe categories of customers were inspired by [this document from the Data & Marketing Association](https://dma.org.uk/article/use-rfm-modelling-to-generate-successful-customer-segments). \n\n### Toggle SQL query to calculate RFM scores\n\n```sql\n/*\n  The following query calculates Recency, Frequency, Monetary scores (quartiles)\n  from the `turing_data_analytics.rfm` table.\n  Higher quartiles/scores (e.g. 4-4-4) are better, i.e. more recent and frequent orders, \n  as well as higher amount \n*/\n\nwith\n-- calculate the RFM values, to prepare for calculating the RFM scores (quartiles)\nRFM_values as \n(\n  with\n-- get the total of each sale for each customer (i.e. each invoice)\ntotal_per_invoice as\n(\n  select\n    CustomerID,\n    InvoiceNO,\n    date(InvoiceDate) as InvoiceDate,\n    round(sum(UnitPrice * Quantity),2) as TotalDue,\n  from  \n    `turing_data_analytics.rfm`\n  where\n    CustomerID is not null\n  group by \n    CustomerID, InvoiceNO, InvoiceDate, country\n)\n-- main query to calculate RFM values (to prepare for RFM scores)\nselect\n  CustomerID,\n  min(date_diff(date \"2011-12-01\", InvoiceDate, DAY)) as recency, -- days since the oldest order\n  count(InvoiceNO) as frequency, -- total number of orders (# of invoices)\n  round(sum(TotalDue),2) as monetary -- sum across all invoices for that customer\nfrom\n  total_per_invoice\nwhere\n  InvoiceDate between date \"2010-12-01\" and date \"2011-12-01\"\ngroup by\n  CustomerID\nhaving \n  monetary > 0  -- for some reason some US$ amounts were negative, so we exclude them\norder by\n  recency desc, customerID\n),\n\n-- RFM_scores (quartiles)\nRFM_scores as \n(\n  select \n    CustomerID,\n    -- Higher scores (444) are better than lower scores (111)\n    -- Therefore the rank (1-4) is left as it is for Frequency (# orders) and Monetary (TotalDue)\n    -- and reversed for Recency (less days from last order is better) \n    recency,    5 - ntile(4) over (order by recency) as R_qtile,\n    frequency,  ntile(4) over (order by frequency) as F_qtile,\n    monetary,   ntile(4) over (order by monetary) as M_qtile,\n  from \n    RFM_values\n  order by R_qtile desc, F_qtile desc, M_qtile desc\n)\n\nselect * from RFM_scores\norder by monetary desc\n\n-- /* \n--   To check that the values have been assigned to the right quartiles, comment the select * above\n--   and uncomment ONLY ONE of the lines below AT A TIME \n-- */\n-- select\n--   -- distinct M_qtile as quartiles,  max(monetary) over (partition by M_qtile order by M_qtile) as M_quartiles,\n--   -- distinct R_qtile as quartiles,  max(recency) over (partition by R_qtile order by R_qtile) as R_quartiles,\n--   -- distinct F_qtile as quartiles,  max(frequency) over (partition by F_qtile order by F_qtile) as F_quartiles,\n-- from\n--   RFM_scores\n-- order by quartiles\n-- ;\n```\n\n### Tableau dashboard\n\nBelow is a snapshot of the dashboard. [Please open and interact with it in full screen following this link](https://public.tableau.com/views/RFM_V3/MRF_dash_V3?:language=en-GB&:display_count=n&:origin=viz_share_link). \n\nRead further for a description of the rationale behind this dashboard design.\n\n![Untitled](RFM%20and%20CLV%20graded%20task%2096a42877b0f84b4aa172900918cf5532/Untitled.png)\n\nThe dashboard was designed with the following concepts in mind:\n\n- **The dashboard highlights the fact that the Customer Segmentation was based on quartiles** - since this is *not* the only possible choice. In other words we want to make clear that the Customer Segmentation was yielded by the discretization of the RFM values, which are in a more fine-grained range and with peculiar distributions (more on this later)\n\n- At the same time **the dashboard should allow the possibility to drill down the information to the level of the single customer**. Due to the high amount of customers (~ 4000) and the peculiar distributions of the values (mostly a power law on continous or discrete values - see below), scatterplots were impractical and hiding both the information and the possibility to select a specific customer. **Therefore we opted for a discrete plot with markers**. This allows to easily inspect the main metrics for each customer (also using the tooltip), **although it comes with a cost in terms of resposivity of the dashboard (up to 2 seconds)**\n\n```sql\n Recency          Frequency         Monetary       \n Min.   :  0.00   Min.   :  1.000   Min.   :     2.9  \n 1st Qu.: 14.00   1st Qu.:  1.000   1st Qu.:   299.3  \n Median : 45.00   Median :  3.000   Median :   647.6  \n Mean   : 88.52   Mean   :  4.987   Mean   :  1872.6  \n 3rd Qu.:140.25   3rd Qu.:  5.000   3rd Qu.:  1583.3  \n Max.   :365.00   Max.   :241.000   Max.   :267761.0\n```\n\n- The RFM scores are divided into **16 regions showing the relationship between Recency and Frequency quartiles, while the color and size of the markers show the Monetary quartile and amount, respectively**. The latter choice is due to the fact that there are huge differences in Total amount within each quartile (and especially for the highest quartile). Therefore, only visualizing the quartile without the actual amount of US$ (also in ascending order within each quartile) would have provided an incomplete and biased information. The same is true also for Recency and Frequency, but this information can be accessed using the rugplots in the bottom-right part of the dashboard.\n\n- **Due to the peculiar - definitively non-gaussian - distribution of the values, we do *not* provide mean or median values for any of the RFM values**, as this summary statistics would be not only useless but plainly wrong in representing the relation between one or more customers’s RFM with respect to the other customers in the sample.\nInstead this information can be correctly retrieved using the rugplots in the bottom-right of the dashboard.\n\n- FInally, the **Categories of Customers** and their main stats can be inspected in the table on the bottom-left. By clicking on one row, the corresponding customers will be highlighted in the carpet plot on top.\n\n### Main observations\n\n- It is peculiar the fact that in this dataset the customers with the highest sales (Monetary dimension) are all placed in the same quartile in terms of Recency and Frequency. This means that most likely **in this case using 3 dimensions (RFM) and 9 categories is not actually necessary to appropriately describe the data**. \nIn a work environment, I would have asked for a few days to run a PCA/MDS/Clustering analysis that would have likely revealed that the segmentation is actually more simple than the 9 categories used. For instance more loyal customers appear to be also the ones that buy more and more frequent, which is not a priori necessary\n- In general, to simplify the point above, there is a (non-linear) progression of the % of customers from the worst to the best across all three dimensions (RFM), which is probably not to be expected in a real-world situation (**too many optimal customers, and a small base of average customers - too good to be true**)\n- The distribution of RFM is highly non-gaussian, therefore **the Median RFM should be used with caution for describing the groups**. For instance in the lowest and highest Monetary quartile **the difference between the min and max span two orders of magnitude (!)**. This is also why the rugplot of Monetary was presented on a log scale.\n\n# CLV Hands-on\n\n### Task description\n\nYou are a data analyst working in an e-commerce shop. Your commerce shop is quite new and previously relied on 3rd party tools to analyze user behaviour. As it grew this was not enough and they hired you as their first data analyst to help them understand business better. Particularly t**hey are interested if their marketing spending is justifiable**. They know that **it costs them 2USD on average to acquire a new customer on their website**.\n\nYou have a single parsed events table which contains various frontend actions done on your site. Table is called [turing_data_analytics.raw_events](https://console.cloud.google.com/bigquery?authuser=3&project=tc-da-1&ws=!1m14!1m3!8m2!1s756497109418!2s1b8fa7cb3d73401f81fea1591fa615a7!1m4!4m3!1stc-da-1!2sturing_data_analytics!3sstackoverflow_posts!1m4!4m3!1stc-da-1!2sturing_data_analytics!3sraw_events&d=turing_data_analytics&p=tc-da-1&t=raw_events&page=table) and columns description are available in the table details section. Data in the table contains records from 2020-11-01 until 2021-01-31.\n\nYour manager has read on Shopify blog that you can calculate Customer Lifetime Value (CLV). He did not get all the details or how to actually calculate and asked your help to do it. He needs the following information:\n\n- Calculate Average Order Value (AOV) for full dataset\n- Calculate Purchase Frequency of all your customers\n- Customer Value\n- Estimate Customer Lifetime Value based on assumption that customer lifespan is 3 years\n\nYou have a look at the raw data and decide that you will use `user_pseudo_id` to distinguish between different customers and use `event_name = ‘purchase’` to filter for events when customers purchased something. You also need to identify the correct column in the table which represents the purchase amount in monetary units.\n\nYou should write a query which pulls necessary data to calculate CLV.\n\n### Development\n\n<aside>\n👉 ********************************Formulas********************************\n\n**Average Order Value (AOV) = Total US$ / # Orders\n\nAverage Purchase Frequency = # Orders / # of unique Customers\n\nCustomer Value = AOV * Average Purchase Frequency\n\nCustomer Lifetime Value = Average Purchase Frequency * Lifetime**\n\n</aside>\n\n- Toggle query calculating Average Order Value (AOV), Average Purchase Frequency, Customer [Lifetime] Value\n    \n    ```sql\n    /* \n      The following query calculates AOV, APF, CV \n      and eventually the CLV for the hands-on task.\n      Note that here we consider only customers who did purchase something > 0 $\n    */\n    \n    with\n    -- Average Order Value (per customer)\n    AOV as \n    (\n      select \n        user_pseudo_id,\n        sum(purchase_revenue_in_usd) as totaldue,\n        count(event_name) as n_orders,\n        round(sum(purchase_revenue_in_usd) / count(event_name),2) as average_order_value \n      from `tc-da-1.turing_data_analytics.raw_events`\n      where event_name = \"purchase\"\n      group by user_pseudo_id\n      having totaldue > 0 \n      order by n_orders desc\n    ),\n    \n    -- Average Purchase Frequency (across purchasing users)\n    APF as \n    (\n      select\n        count(*) as n_orders,\n        count(distinct user_pseudo_id) n_users,\n        round(count(*) / count(distinct user_pseudo_id),3) as average_purchase_frequency \n      from `turing_data_analytics.raw_events`\n      where event_name = \"purchase\"\n    )\n    \n    -- Customer Value as AOV * APF\n    -- CLV for 3 years = CV * 3\n    select\n      user_pseudo_id, \n      average_order_value,\n      average_purchase_frequency,\n      round(average_order_value * average_purchase_frequency,2) as customer_value,\n      round(average_order_value * average_purchase_frequency * 3, 2)  as CLV_3yrs\n      from AOV, APF\n    ;\n    ```\n    \n\n```\nCustomer Lifetime Value 3 years\n\nMin.  1st Qu.   Median    Mean      3rd Qu.  Max. \n3.86  108.19    189.34    271.03    332.30   5911.92\n```\n\n![Untitled](RFM%20and%20CLV%20graded%20task%2096a42877b0f84b4aa172900918cf5532/Untitled%201.png)\n\n![Untitled](RFM%20and%20CLV%20graded%20task%2096a42877b0f84b4aa172900918cf5532/Untitled%202.png)\n\nHowever, here the AOV is calculated ************per customer************. By looking at the [proposed solution](https://github.com/TuringCollegeSubmissions/DA.MAT.CRFM.3.solved/blob/main/DA_MAT_CRFM_3_solved.ipynb), it looks like we need to calculated it ********************across all customers********************. So the query and the calculation reduces to:\n\n \n\n```sql\n/* AOV and Purchase frequency across all customers */\nSELECT\n   SUM(purchase_revenue_in_usd)/COUNT(event_name) AS AOV,\n   COUNT(event_name)/COUNT(DISTINCT user_pseudo_id) AS purchase_frequency\nFROM\n`tc-da-1.turing_data_analytics.raw_events`\nWHERE\n   event_name = 'purchase';\n\n/*\nAOV\t    purchase_frequency\n63.63\t  1.29\n\nCustomer Value (CV) = AOV x Purchase Frequency = 63.63 x 1.29 =  ~82 USD\nCLV = CV x Lifespan (3 years) = 82.08 x 3 = ~246 USD\n*/\n```\n\n# CLV Graded task\n\n## CLV task description\n\nYou got a follow up task from your manager as he read some article that calculating CLV using Shopify’s formula is too simplistic. He has heard somewhere that using cohorts produces more reliable and actionable results. Once again please use [turing_data_analytics.raw_events](https://console.cloud.google.com/bigquery?authuser=3&project=tc-da-1&ws=!1m14!1m3!8m2!1s756497109418!2s1b8fa7cb3d73401f81fea1591fa615a7!1m4!4m3!1stc-da-1!2sturing_data_analytics!3sstackoverflow_posts!1m4!4m3!1stc-da-1!2sturing_data_analytics!3sraw_events&d=turing_data_analytics&p=tc-da-1&t=raw_events&page=table) table to answer follow up questions from your manager.\n\nTIP: imagine that current week is 2021-01-24 (the last weekly cohort you have in your dataset).\n\nAdditionally, he identified 2 problems with your previous analysis:\n\n1. You included only customers who purchased something, while marketing is counting all user registrations that they manage to bring to your ecommerce site. Thus, you need to adjust your calculations to include all users who have been on your website, not only the one who purchased something.\n2. Your average customer does not tend to stay with your ecommerce site for too long. He wants to see weekly picture using cohorts. He expects customers to stay no longer than 12 weeks with your ecommerce site.\n\nAs the first step you should write 1 or 2 queries to pull data of weekly revenue divided by registrations. Since in this particular site there is no concept of registration, we will simply use the first visit to our website as registration date (registration cohort). Do not forget to use `user_pseudo_id` to distinguish between users. Then divide revenue in consequent weeks by the number of weekly registration numbers. Once you apply conditional formatting to your data, the end result should look something like this:\n\n![https://i.imgur.com/TPWDMq6.png](https://i.imgur.com/TPWDMq6.png)\n\nNext you will produce the same chart, but the revenue / registrations for a particular week cohort will be expressed as a cumulative sum. For this you simply need to add previous week revenue to current week’s revenue. Down below you will calculate averages for all week numbers (weeks since registration). Down below that you will calculate percentage growth, which will be based on those average numbers:\n\n![https://i.imgur.com/KcVBI7s.png](https://i.imgur.com/KcVBI7s.png)\n\nNote: for cumulative weekly averages calculation use first table averages\n\nBasically, the chart above gives you growth of revenue by registered users in cohort for n weeks after registration. While numbers below summarize those values in monetary terms (red marked numbers) and percentage terms (green marked numbers). This provides you with a coherent view of how much revenue you can expect to grow based on your historical data.\n\nNext, we will focus on the future and try to predict the missing data. In this case missing data is the revenue we should expect from later acquired user cohorts. For example, for users whose first session happened on 2021-01-24 week we have only their first week revenue which is 0.19USD per user who started their first session in this week. Though we are not sure what will happen in the next 12 weeks.\n\nFor this we will simply use previously calculated Cumulative growth % (red marked area in chart aboove) and predict all 12 future weeks values (ex. for this cohort we can calculate expected revenue for week 1 as 0.19USD x (1 + 23.29%) = 0.24USD, for week 2 as 0.24USD x (1 + 12.26%) = 0.27USD). Using Avg. cumulative growth for each week we can calculate that based on 0.19USD initial value we can expect 0.35USD as revenue on week 12. Provide a chart which calculates these numbers for all future weeks (up till week 12).\n\nSee example chart below which should show the following numbers for all weekly user cohorts:\n\n![https://i.imgur.com/t2ujd2P.png](https://i.imgur.com/t2ujd2P.png)\n\nThe chart above is our final result. You should calculate the average of cumulative revenue for 12th week for all users who have been on your website. This not only provide better estimate of CLV for all your users who have been on your website (including the ones who did not purchase anything) but also allows you to see trends for weekly cohorts. Have a look at the conditional formatting you added to all 3 charts and be prepared to answer follow up questions.\n\n## CLV task development\n\nThe main idea here is to calculate customer value (and customer lifetime value) using a stratification approach by means of cohorts. In this particular case, the users visiting for the first time the website in a given week will be assigned to one cohort, and followed in the next weeks. \n\nTo calculate the average order value, the total amount of sales for that week will be divided by the number of users registered in each cohort (i.e. we assume this remains constant across the following weeks).\n\nSince there is no specific ************registration************ event, the cohorts are defined by the users who first visit the website in a given week.\n\nNote that Marketing also requested to include *************all the users************* in the AOV (average order value), not only those who buy something.\n\n### Toggle query to calculate the cohort matrix (to be pivoted in Google Sheets)\n\n```sql\n/*\n  Starting from a dataset containing\n  - user id\n  - date of event\n  - amount spent\n  this query creates cohorts of users based on the earliest\n  event for each user.\n  \n  It also calculates the # of users in each week to derive the\n  metric of interest:\n\n  AOV (Average Order Value) = TotalDue / # of users\n  for each week in each cohort.\n\n  The final cohort matrix is calculated by aggregating (sum)\n  over cohort and week.\n\n  A column representing the elapsed weeks for each cohort is\n  also generated to facilitate the pivoting of the matrix\n  in Google Sheets\n*/\n\n-- Assign the cohort to each user based on the week\n-- in which the user's earliest event occurs\nwith\nuser_cohort as\n(\n  select \n    user_pseudo_id,\n    min(date_trunc(parse_date('%Y%m%d', event_date),WEEK)) as cohort\n  from\n    `turing_data_analytics.raw_events`\n  group by user_pseudo_id\n),\n\n-- Calculate how many users are there in the cohort \n-- This will be used to calculate TotalDue / # Users per week\nusers_per_cohort as\n(\n  select\n    cohort,\n    count(user_pseudo_id) n_users_cohort\n  from\n    user_cohort\n  group by cohort\n),\n\n-- Calculate the total spent for each user each week\nuser_totaldue_per_week as\n(\n  select\n    user_pseudo_id,\n    date_trunc(parse_date('%Y%m%d', event_date),WEEK) as week_date,\n    sum(purchase_revenue_in_usd) as totaldue\n  from\n    `turing_data_analytics.raw_events`\n  group by user_pseudo_id, week_date\n),\n\n-- Put everything together in one dataframe before\n-- aggregating across users in each week \ndf as\n(\n  select \n    user_cohort.user_pseudo_id,\n    user_cohort.cohort,\n    user_totaldue_per_week.week_date,\n    user_totaldue_per_week.totaldue,\n    n_users_cohort,\n  from user_cohort \n    join user_totaldue_per_week on user_cohort.user_pseudo_id = user_totaldue_per_week.user_pseudo_id\n    join users_per_cohort on user_cohort.cohort = users_per_cohort.cohort \n)\n\n-- Aggregate across users for each week by calculating\n-- TotalDue / # Users\n-- and create the cohort matrix (to be pivoted)\nselect\n  cohort,\n  week_date,\n  round(sum(totaldue) / n_users_cohort,2) as AOV,\n  rank() over (partition by cohort order by week_date) - 1 as elapsed_weeks -- useful for reordering the cohort matrix\nfrom  \n  df\ngroup by\n  cohort, week_date, n_users_cohort\norder by cohort\n```\n\nNow to generate the cohort table in Google Sheets is just a matter of pivoting the data using the elapsed weeks.\n\n[The Google Sheet with the results is here](https://docs.google.com/spreadsheets/d/1JdP590B1qXonoqOITdUlFjBfJH9_W51VECrGye5QTgU/edit#gid=1527413066).\n\n![Untitled](RFM%20and%20CLV%20graded%20task%2096a42877b0f84b4aa172900918cf5532/Untitled%203.png)\n\n![Untitled](RFM%20and%20CLV%20graded%20task%2096a42877b0f84b4aa172900918cf5532/Untitled%204.png)\n\n### Main observations\n\n- **Including the non-purchasing customers definitively changes the CLV estimate of about two orders of magnitude: from 246.00 US$ to 4.58 US$** - the latter being the average of the last column in the plot above multiplied by 3 years.\n\nAlthough this is understandable from a marketing perspective, **including also the non-purchasing customers dramatically underestimate the customer value for the purchasing customers** (e.g. if my website has a conversion-to-purchase rate of 1:1000). \n\nIf for instance a poor marketing campaign would bring 10 times more non-buying visitors to the website, the customer value of the loyal customers (who buy steadily) would automatically decrease ten times. \n\nOne should be careful about this when comparing CLV estimated in different period, and for instance also reporting the conversion rate at the same time, which allows to adjust the customer value. \n**This is particularly true for the cohort CLV estimate above: we don’t know whether the proportion between the buying and not-buying customers in different cohorts is the same**. Without any data, the situation could actually be that the customer value is actually the same for earlier and latter cohorts, but it appears to be less because of an increased number of non-paying customers. It could even be higher for the same reason.\n\n- **The Shopify blog advises to accept a CAC which is at most 1/3 of the CLV. In our data, the CAC is estimated in 2 US$, while the estimated CLV is ~ 4.6 US$ (with a lifetime of three *years*). The latter is therefore short of 1.4 US$ to meet this rule-of-thumb requirement.**\n    \n    Since the Customer Acquisition Cost (CAC) was estimated to be 2 US$, the margin at one year appears to be meager even for the best performing cohorts, and therefore the product appear not to be financially sustainable, as it would take in the best case one full year without profit only to break even with the CAC.\n    \n- At the same time, **the cohorts are very heterogeneous**, as seen e.g. by the cumulative growth across weeks for each cohort. Here the variability around the mean is very high (values not displayed) which suggests that the subsequent calculation of cumulative growth is *not* reliable.\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}