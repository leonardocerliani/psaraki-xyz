[
  {
    "objectID": "showcase.html",
    "href": "showcase.html",
    "title": "showcase",
    "section": "",
    "text": "Online Campaign Performance\n\n\n\nbusiness analytics\n\n\n\n\n\n\n\nLeonardo Cerliani\n\n\nAug 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFramingham CHD logistic regression\n\n\n\n\n\n\nLeonardo Cerliani\n\n\nJul 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of time-to-purchase (TTP)\n\n\n\n\n\n\nLeonardo Cerliani\n\n\nJul 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive data exploration in neuroimaging: a simple example\n\n\n\nshiny app\n\n\nneuroimaging\n\n\n\n\n\n\n\nLC\n\n\nDec 28, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LC",
    "section": "",
    "text": "leonardo cerliani\ndata analysis & brain imaging"
  },
  {
    "objectID": "secreto/monty-hall.html",
    "href": "secreto/monty-hall.html",
    "title": "LC",
    "section": "",
    "text": "Leonardo C.\n12 April 2023\nI finally got an understanding of the solution of the Monty Hall problem after watching this video from Lazy Programmer. Another explanation I found to be really clear is this Medium article from GreekDataGuy.\nI had tried a few times to solve the problem either by myself, or by reading other people’s solutions. I noticed that one of the reasons I was previously getting stuck with was by switching continously between my scribbles on the paper and the mental image of ****************************************where actually the car and the goats are****************************************. However in a real Monty Hall situation you do not get to see where the car is, and this is an important point that in my opinion should be kept as it is also when trying to solve the problem. After all, the rules of probability were devised exactly to reason in situations of uncertainty, but if we get to know in advance where the car actually is, the core uncertainty in the problem dissolves.\nSo what worked for me - and differently from all the other expositions of the Monty Hall problem I have seen so far - was not to start with “let’s assume that the car is behind door n”. Instead, let’s not assume anything about where the car actually is, and let’s focus only on the information that are actually available to us:\n\nthe door I choose, for instance door 1\nthe door that Monty Hall opens ************************after I have chosen door 1************************. For instance door 2.\n\nIndeed Monty **************************does not open one of the two remaining doors at random**************************. His choice of the door to be opened is conditioned - and in 2 out of 3 cases determined - by which door I choose, and by which door has the car behind it. In other words, he can only choose one door with a goat behind, among the two that I have not chosen.\nThis observation - the fact that the choice of which door Monty Hall opens is conditioned on which door I chose - is so natural and given for granted that I did not notice at first how crucial it is for calculating the probability of winning the car.\nSo, we know which door I initially chose, and which door Monty opens given my choice. The only ********information which is still uncertain is ****************where the car is****************, and we should focus our probability calculation on this, given what we know.\nSpecifically, we should focus our attention on calculating the probability that Monty Hall would choose to open the door he did open - door 2 in this case - in either one of the three cases in which the car is behind door 1, 2 or 3.\n\n\n\n\n\n\n\nProbability that Hall opens door 2 given that the car is behind door 1,2,3 and I chose door 1\nMotivation\n\n\n\n\n\\(p(H = 2 | C = 1) = 0.5\\)\nbecause goats are behind both doors 2 and 3\n\n\n\\(p(H = 2 | C = 2) = 0\\)\nbecause Monty Hall cannot open the door with the car\n\n\n\\(p(H = 2 | C = 3) = 1\\)\nbecause I chose door 1, and the car is behind door 2, so he can only open door 3\n\n\n\nOnce we have calculated these probabilities, we can get to the most important question: what is the probability that I will win the car in either of the two options: keep the initial choice of door 1, or switch to door 3 - as Monty Hall opened door 2.\n\n\n\n\n\n\n\n\\(p(C=1|H=2)\\)\nprobability of winning the car if I keep my original choice of door 1\n\n\n\n\n\\(p(C=3 | H = 2)\\)\nprobability of winning the car if I switch to door 3\n\n\n\nNote that the conditional probabilities I would like to calculate here are the opposite of those I previously estimated.\nTherefore to estimate \\(p(C|H)\\) I just need \\(p(H|C)\\) and Bayes’ rule:\n\\[\np(C|H) = \\frac{p(H|C) \\cdot p(C)}{p(H)}\n\\]\nWe know already our Prior, that is the probability of choosing the door with the car before knowing anything else: \\(p(C) = 1/3\\).\nWe also know the denominator of the equation, that is \\(p(H)\\) - or ***************************Marginal probability** of H* - since this is the weighted sum of all the initially calculated \\(p(H|C) = (1/2 + 0 + 1)/3 = 1/2\\).\nFinally, we know - again from the initially calculated conditional probabilities - what is the ******************************Likelihood********************* of both\n\n\\(p(H=2 | C=1) = p(Keep Door 1) = 0.5\\)\n\\(p(H=2 | C=3) = p(SwitchToDoor3) = 1\\)\n\nNow we just have to plug in our numbers to have the answers:\nIf I keep my original choice of door 1, the probability of having picked the door with the car is 1/3.\n\\[\np(C=1|H=2) = \\frac{p(H=2|C=1) \\cdot p(C=1)}{p(H=2)} = \\frac{1/2 \\cdot 1/3}{1/2} = 1/3\n\\]\nOn the other hand, if I switch to door 3, my probability of winning the car increases to 2/3.\n\\[\np(C=3|H=2) = \\frac{p(H=2|C=3) \\cdot p(C=3)}{p(H=2)} = \\frac{1 \\cdot 1/3}{1/2} = 2/3\n\\]"
  },
  {
    "objectID": "secreto/monty-hall.html#focus-on-the-right-question",
    "href": "secreto/monty-hall.html#focus-on-the-right-question",
    "title": "LC",
    "section": "Focus on the right question",
    "text": "Focus on the right question\nWhen we hear all the details of the problem, we realize there are ****many****, and we immediately try to form a mental movie with images - showing us where the car actually is - and sequences of event - first the guest’s choice of the door, then the choice of Monty Hall about which door to open. With this mental construct in mind, we start to work on the problem.\nThe main question is “what is the probability of me getting the car if I keep my choice or if I switch”. This is correct, but not complete. Probability is *******always******* conditional to something happening. When we formulate this question, we forget that the probability of getting the car is conditional to both:\n\nthe door that we just choose\nthe door behind which there is a car\nthe door that Monty Hall will open\n\nprecisely in ****this**** order. In fact, the decision of Monty Hall to open a certain door is conditional to both which door I chose and which door hides the car"
  },
  {
    "objectID": "blog/purrr_RManova/purrr_RManova.html",
    "href": "blog/purrr_RManova/purrr_RManova.html",
    "title": "How to replace for loops using purrr::map",
    "section": "",
    "text": "In many cases you need to repeat the same action across multiple objects, for instance loading many files, or computing summary statistics across many vectors of observations. Instead of repeating the same operation manually for every object - which is not only time consuming, but especially prone to mistakes - you can use for loops.\nHowever for can be quite verbose, and especially in case you need to nest them - i.e. running a loop inside a loop - it can be difficult to inspect the code for errors during the analysis and especially in the future.\nBase R already provides some functions to avoid the creation of for loops, with the family of apply functions. However sometimes the syntax can be different across functions, and still a bit verbose.\nThe tidyverse provides functions that help getting rid of for loops for good using the purrr package. Below there is just an example. More details can be found in the iteration chapter of R for Data Science and in the functionals chapter of Advanced R\n\n\nCode\nlibrary(tidyverse)\nlibrary(reactable)\noptions(digits=2)\n\n\nLet’s say you collected data in 8 different runs of an experiment. For instance the time, in seconds, spent freezing, running or grooming in 10 participants after a given stimulus in each subsequent run.\nFor our example we will create some random data. The code below creates 8 dataframes with 10 observations for three distinct variables. It already uses the map function that we are going to explain later, so for now you can just disregard it, and come back later to understand what it does as an excercise.\n\n\nCode\n1:8 %>% map( function(x) {\n  tibble(\n    SUBID = map(1:10, ~ paste0(\"sub_\",.x) ) %>% unlist(),\n    freezing = runif(10)*10 * log(x+1),\n    running = runif(10)*10,\n    grooming = runif(10)*10\n  ) %>%\n    write_csv(paste0(\"run_\",x,\".csv\"))\n})\n\n\nWe obtain 8 csv files with our data.\n\n\nCode\nmyfiles <- list.files(pattern = \".csv\", full.names = T)\nmyfiles\n\n\n[1] \"./run_1.csv\" \"./run_2.csv\" \"./run_3.csv\" \"./run_4.csv\" \"./run_5.csv\"\n[6] \"./run_6.csv\" \"./run_7.csv\" \"./run_8.csv\"\n\n\nCode\nread.csv(\"run_1.csv\")\n\n\n    SUBID freezing running grooming\n1   sub_1      2.7     9.7     1.24\n2   sub_2      6.9     5.1     6.35\n3   sub_3      1.1     5.6     0.32\n4   sub_4      1.7     8.6     9.19\n5   sub_5      2.6     7.6     3.53\n6   sub_6      5.4     8.5     0.74\n7   sub_7      4.1     5.9     0.29\n8   sub_8      2.3     4.2     1.02\n9   sub_9      5.8    10.0     3.53\n10 sub_10      3.6     6.8     2.60"
  },
  {
    "objectID": "blog/purrr_RManova/purrr_RManova.html#purrrmap",
    "href": "blog/purrr_RManova/purrr_RManova.html#purrrmap",
    "title": "How to replace for loops using purrr::map",
    "section": "purrr::map",
    "text": "purrr::map\nNow you want to load everything in the same dataframe (i.e. table), for instance to carry out a RM-ANOVA. You could use a for loop to load all the files:\n\n\nCode\nallruns = vector(mode = \"list\", length = 8)\n\nfor (run in 1:length(allruns)) {\n  allruns[[run]] <- read.csv( myfiles[[run]] )\n}\n\n# allruns\n\n\nOr you could use the map function inside the purrr package\n\n\nCode\nallruns <- map(myfiles, read.csv)\n\n# allruns\n\n\nIn other words you passed to every element of the list myfiles the function read.csv\nNote the advantages:\n\nyou do not need to write extra code to initialize an empty list, since the result is automatically stored in a list\nyou don’t need to provide the total number of files,\nthe syntax is much more concise (and when you get used to it, also much more readable)."
  },
  {
    "objectID": "blog/purrr_RManova/purrr_RManova.html#purrrmap2",
    "href": "blog/purrr_RManova/purrr_RManova.html#purrrmap2",
    "title": "How to replace for loops using purrr::map",
    "section": "purrr::map2",
    "text": "purrr::map2\nTo carry out the RM-ANOVA, you need to combine all the tables into one singe dataframe, but also retain information about the different run.\nThe idea is the same as before: you have a function that creates a column with the run numba in each run’s data table. This means that you want to provide two lists: (1) the list containing the table of each run and (2) the list of filenames.\n\n\nCode\nalldata <- map2(\n allruns, myfiles, function(run, file) {\n  run %>% mutate(run = file)\n }\n) %>% bind_rows()\n\n\nor with a more concise syntax:\n\n\nCode\nalldata <- map2_dfr(allruns, myfiles, ~ .x %>% mutate(run = .y))\n\n\nYou might have noticed that here I used a specific flavor of map, that is map_dfr, which returns a dataframe (or a tibble in the tidyverse language) instead of the default list, so that I can drop the final bind_rows()."
  },
  {
    "objectID": "blog/purrr_RManova/purrr_RManova.html#purrrpmap",
    "href": "blog/purrr_RManova/purrr_RManova.html#purrrpmap",
    "title": "How to replace for loops using purrr::map",
    "section": "purrr::pmap",
    "text": "purrr::pmap\nAs you might expect, there is also a function pmap which allows you to pass an arbitrary number of tables. I personally prefer this syntax since it allows me to pipe the list into it:\n\n\nCode\nalldata <- list(allruns, myfiles) %>% pmap_df(~ .x %>% mutate(run = .y))\n\nalldata %>% reactable(\n defaultColDef = colDef(\n  format = colFormat(digits = 2), minWidth = 50\n ),\n style = list(fontFamily = \"Arial Narrow\")\n)"
  },
  {
    "objectID": "blog/purrr_RManova/purrr_RManova.html#map-is-similar-to-group_by-for-dataframes",
    "href": "blog/purrr_RManova/purrr_RManova.html#map-is-similar-to-group_by-for-dataframes",
    "title": "How to replace for loops using purrr::map",
    "section": "map is similar to group_by for dataframes",
    "text": "map is similar to group_by for dataframes\nFinally, note that the map function - and its variation, such as pmap, is a similar operator for list to the group_by operator inside dataframes.\nFor instance let’s say that you want to get the mean and standard deviation for every variable in each run:\n\n\nCode\ndescriptives <- alldata %>% \n  group_by(run) %>%\n  summarise(\n    across(where(is.numeric), list(mean = mean, sd = sd)),\n    .groups = \"drop\"\n  ) %>% ungroup() \n\n\ndescriptives %>% reactable(\n defaultColDef = colDef(\n  format = colFormat(digits = 2), minWidth = 50\n ),\n style = list(fontFamily = \"Arial Narrow\")\n)"
  },
  {
    "objectID": "blog/post_001/post_001.html",
    "href": "blog/post_001/post_001.html",
    "title": "It is time to make a blog…",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "showcase/dimred_app/dimred_app.html",
    "href": "showcase/dimred_app/dimred_app.html",
    "title": "Interactive data exploration in neuroimaging: a simple example",
    "section": "",
    "text": "Konrad Lorenz once said that “It is a good morning exercise for a research scientist to discard a pet hypothesis every day before breakfast. It keeps him young” (ref). Having spent quite some years doing scientific research, I can comfortably say that this is one of the mantra for the morning ritual of every researcher. Another useful mantra on the same line is a quote by Richard Feynman: “you must not fool yourself and you are the easiest person to fool” (ref).\nFrom a practical standpoint, these two quotes provide an interesting perspective on why I think visualization is so useful and actually indispensable for a researcher and for everyone who works with data: images are a great device to show us when our hypotheses are correct and when they are wrong (or when there is something wrong in the data).\nVisualization is key not just to communicate a complex result in an intuitive way, but also for inspecting every step of the analysis, in order to validate - or invalidate - previous assumptions, avoid wrong conclusions, get useful insights to re(de)fine our hypotheses, and design the next analytic step.\nAnother reason why data visualization is tremendously helpful in data analysis is that data is always multifaceted: when you look at it from different perspectives (e.g. different summary statistics, for different variables, observations, or groups of them), it shows one of its many aspect, and usually a meaningful result stands out only when there is consistency among most of the perspectives from which we can look at our data: only if it squeaks like a duck, looks like a duck, walks like a duck, then most likely it is… an interesting result. I find images great for this: being able to inspect at once different images showing different aspects of the data is very helpful to reveal consistencies and inconsistencies both in our data and in our hypotheses.\nInteractive visualization takes this ability one step forward. Now we can carry out some kind of transformation in one feature of our data - e.g. selecting different groups of variables - and see immediately the effect of this in other aspects of our data.\nIn neuroimaging - the field where I worked for most time - this can be particularly useful, since the variables associated with both data pre-processing and actual data analysis are so many, that it is of paramount importance to check as much as possible each step of the process. Even restricing our focus to the actual analysis, the complexity associated with our research questions can grow very quickly: are the data from my participants homogeneous? How do they compare between the different tasks presented in the experiment? Is the effect restricted to only specific regions? Or maybe is it more evident when looking at sets (and potentially networks) of brain regions?\nIn the following I will present a basic example of how different visualizations of the same data can help to inspect and gain insights on the results of a (fictitious) fMRI experiment. I hope to show that while exploring even a simple dataset, the complexity of questions that come to the mind grows very fast. This prompts us to go back to the code, modify it, create new code chunks in our notebook for intermediate checks, and so on. This process can become very complex and - frankly - very messy.\nAt this point, we might think about creating a device that allows us to look at all the different questions we have generated, and see how they affect the representation that we are creating in our mind about the data. Therefore, I will show an interactive version of it, where the different tables and plots we generated react when something is changed in one of them (e.g. selecting different brain regions).\nNot necessarily exploring data will prompt us to create an interactive app all the time, however its utility might become apparent for instance if we realize that such exploration steps might be useful for similar datasets in the future."
  },
  {
    "objectID": "showcase/dimred_app/dimred_app.html#a-simple-fmri-experiment",
    "href": "showcase/dimred_app/dimred_app.html#a-simple-fmri-experiment",
    "title": "Interactive data exploration in neuroimaging: a simple example",
    "section": "A simple fMRI experiment",
    "text": "A simple fMRI experiment\nSuppose you want to compare brain activity during the execution of three different tasks: AAA, BBB and CCC. These tasks are behaviourally different, and your hypothesis is that this is reflected in which brain regions are recruited to perform each task. To test this hypothesis you set up an fMRI experiment where 30 participants execute many times each task while being in the scanner (this is an extremely simplified description of an fMRI experiment). The brain activity is recorded in each brain region, but for simplicity here we will focus only on nine of them.\nFor the purpose of showing the utility of our visual exploration of the data, we will generate fictitious data which is in accordance with the hypothesis. Specifically, I will generate summary statistics (mean activity) from 30 participants, showing that some brain regions are more active in task AAA and some in task CCC, while all examined brain regions do not appear to be particularly activated by task BBB.\n\n\nCode\nN = 30\n\nA <- function(mu) rnorm(N, mean = mu)\n\ndf <- tibble(\n task = c(rep(\"AAA\",N), rep(\"BBB\",N), rep(\"CCC\",N)) %>% as.factor(),\n  BA44  = c(A(2), A(2), A(5)),\n  BA45 = c(A(2), A(2), A(5)),\n  BA46  = c(A(2), A(3), A(5)),\n  V1  = c(A(2), A(2), A(2)),\n  V4  = c(A(2), A(2), A(2)),\n  V5  = c(A(2), A(2), A(2)),\n  SI   = c(A(5), A(2), A(2)),\n  SPL  = c(A(5), A(2), A(2)),\n  IPL  = c(A(5), A(3), A(2))\n) %>% \n group_by(task) %>%    # assign participant numbers\n # mutate(sub = paste0(\"sub_\",row_number())) %>%\n mutate(sub = paste0(\"sub_\",row_number(),\"_\",task)) %>%\n relocate(sub) %>%\n ungroup()\n \n\n# # plant an outlier\n# df[c(30,60,90),] %<>%  mutate(across(BA44:IPL, ~ .x + 4))\n\n# define a reusable function to print the table\nshow_table <- function(df) {\n \n BuYlRd <- function(x) {\n   rgb(colorRamp(c(\"#7fb7d7\", \"#ffffbf\", \"#fc8d59\"))(x), maxColorValue = 255)\n }\n \n  reactable(\n    df,\n   resizable = T,\n   # selection = \"multiple\",\n   # onClick = \"select\",\n   defaultColDef = colDef(\n     style = function(value) {\n        vals <- df %>% select(where(is.numeric))\n        if (!is.numeric(value)) return()\n        normalized <- (value - min(vals)) / (max(vals) - min(vals))\n        color <- BuYlRd(normalized)\n        list(background = color)\n      },\n     format = colFormat(digits = 2), minWidth = 50\n   ),\n   style = list(fontFamily = \"Arial narrow\", fontSize = \"13px\")\n  )\n}\n\ndf %>% show_table()\n\n\n\n\n\n\n\nThis is our complete dataset. It is very simplified (and clean) with respect to a real fMRI dataset, but it shows the many levels at which an fMRI can be examined:\n\nacross participants\nacross tasks\nacross brain regions\n\nand of course all the combinations of these levels. For instance we might be interested at the effect of a given task with respect to the other two in a specific brain region or in a specific set of brain regions.\nHere we start to see how quickly the complexity increases for the questions that we might ask even in a simple experiment like this. (It’s here that the interactive exploration will reveal its potential as we will see later).\nAlthough in this table I already color-coded brain the mean activity in each region for each task and each subject, it is quite difficult to have a clear view of the results."
  },
  {
    "objectID": "showcase/dimred_app/dimred_app.html#effect-of-task-in-different-brain-regions-across-participants",
    "href": "showcase/dimred_app/dimred_app.html#effect-of-task-in-different-brain-regions-across-participants",
    "title": "Interactive data exploration in neuroimaging: a simple example",
    "section": "Effect of task in different brain regions across participants",
    "text": "Effect of task in different brain regions across participants\nAt this point we want to check whether the effect we hypothesize is consistent across participants. This is done to achieve a robust estimate of our effect of interest (and of its variability). One basic visualization of this mean result is in the form of a table:\n\n\nCode\ndf_mean <- df %>% \n group_by(task) %>%\n summarise(\n  across(where(is.numeric), ~ mean(.x, na.rm = T)),\n  .groups = \"drop\"\n )\n\n# df_mean %>% show_table()\n\n# pivot to group all the brain regions in a variable\nt_df_mean <- df_mean %>% \n pivot_longer(\n  cols = !task, names_to = \"JU\", values_to = \"Zmean\"\n ) %>% \n pivot_wider(\n  names_from = task, values_from = Zmean\n )\n\nt_df_mean %>% show_table()\n\n\n\n\n\n\n\n\n\nNow the situation appears much more clear than when we looked at the data at the participant level, and indeed reflects how we actually generate the data: tasks CCC and AAA activate mostly the three top and bottom brain regions, respectively, while task BBB does not appear to activate any of the examined brain regions above baseline."
  },
  {
    "objectID": "showcase/dimred_app/dimred_app.html#a-different-visualization",
    "href": "showcase/dimred_app/dimred_app.html#a-different-visualization",
    "title": "Interactive data exploration in neuroimaging: a simple example",
    "section": "A different visualization",
    "text": "A different visualization\nConsider however that here we are looking only at 9 brain regions, while in reality - in a whole-brain analysis - we look at least at 50+ regions (e.g. the 52 Brodmann areas). That is why I prefer to look at the same result with a graphical representation such as the following one.\nNB: in the following graph you can select specific tasks by clicking on the legend\n\n\nCode\ndraw_spiderplot <- function(t_df_mean) {\n  \n  p <- plot_ly(\n    type = 'scatterpolar', mode = 'lines+markers', fill = 'toself', opacity = 0.5\n  ) %>% config(displayModeBar = F)\n  \n  df_vals <- t_df_mean %>% select(-JU)\n  ticks <-  t_df_mean$JU %>% as.character()\n  groups <- df_vals %>% colnames()\n  \n  for (ith_col in 1:length(groups)) {\n    \n    onecol <- df_vals[,ith_col] %>% pull()\n    \n    p <- p %>%\n      add_trace(\n        r = c(onecol, onecol[1]),\n        theta = c(ticks, ticks[1]),\n        name = groups[ith_col],\n        line = list(\n          dash = \"solid\",\n          shape = \"spline\",\n          smoothing = 1,\n          width = 2\n        )\n      ) %>% layout(font = list(family = \"arial narrow\"))\n  }\n  \n  return(p)\n}\n\ndraw_spiderplot(t_df_mean)"
  },
  {
    "objectID": "showcase/dimred_app/dimred_app.html#reducing-the-dimensionality-for-a-sharper-view",
    "href": "showcase/dimred_app/dimred_app.html#reducing-the-dimensionality-for-a-sharper-view",
    "title": "Interactive data exploration in neuroimaging: a simple example",
    "section": "Reducing the dimensionality for a sharper view",
    "text": "Reducing the dimensionality for a sharper view\nI find this visualization much easier to inspect than the table above. However, you might have noticed that we don’t know anymore what is going on at the level of the single participants. Of course are insights on the results so far are purely based on descriptive statistics, and appropriate inferential methods (e.g. ANOVA) will tell us whether the variability across tasks is substantially higher than that between participants. However, it can be useful to have a first look into this by looking at the main trajectories of variability across the entire initial dataset.\nLet’s see for instance what happens when we feed our initial participants-by-brain region table into UMAP.\n\n\nCode\ndo_dimred <- function(df, method, compX, compY, maxcomp=3) {\n  \n  # nsub <- length(df$task)\n  vals <- df %>% select(!task) %>% as.matrix()\n  \n  switch(method,\n         mds = {\n           mds <- cmdscale(dist(vals), eig = T, k = maxcomp)\n           pcs <- list(mds$points[,compX], mds$points[,compY])\n         },\n         umap = {\n           u <- umap(dist(vals), n_components = maxcomp)\n           pcs <- list(u[,compX], u[,compY])\n         },\n         tsne = {\n           tsne <- Rtsne(dist(vals), dims = 3, perplexity = 10)\n           pcs <- list(tsne$Y[,compX], tsne$Y[,compY])\n         }\n  )    \n  \n  df_lowdim <- tibble(\n    task = df$task,\n    sub = df$sub,\n    pc1 = pcs[[1]],\n    pc2 = pcs[[2]],\n  )\n  \n  plot_ly(type = 'scatter', mode = 'markers', source = \"A\") %>%\n    add_trace(\n      data = df_lowdim,\n      x = ~pc1,\n      y = ~pc2,\n      customdata = ~sub,\n      color = ~task,\n      text = ~sub,\n      hoverinfo = 'text'\n      # hovertemplate = paste('%{text}')\n    ) %>% \n    layout(dragmode = \"lasso\") %>% \n    config(displayModeBar = F)\n}\n\ndo_dimred(df,\"umap\", compX = 1, compY = 2)\n\n\n\n\n\n\n\n\nHere each point represents one participant performing one task. The data is grouped in three clusters reflecting - not surprisingly - the difference in brain activity across all particiants and between tasks.\nInterestingly, we see that the brain activity for task BBB in one participant is grouped with a cluster which is mostly populated with activity for task AAA (hover on the orange dot in the green cloud). To inspect this potential anomaly, we can look into the initial data table and inspect the original values. Eventually we can go back to the original fMRI data and - through appropriate examination - decide if something went wrong with the data of this participant, e.g. during data acquisition or pre-processing.\nThis situation is made more clear if we artificially “plant” an anomaly in our data. I will not run this here, but you can download this RMarkdown notebook and run it on your computer. Check out how the UMAP is able to spot the “outliers” which were planted in the data.\n\n\nCode\n# plant an outlier\ngf <- df\ngf[c(29,30,59,60,89,90),] %<>%  mutate(across(BA44:IPL, ~ .x + 4))\n\ndo_dimred(gf,\"umap\", compX = 1, compY = 2)\n\ngf %>% show_table()"
  },
  {
    "objectID": "showcase/dimred_app/dimred_app.html#bringing-all-together-in-an-interactive-app",
    "href": "showcase/dimred_app/dimred_app.html#bringing-all-together-in-an-interactive-app",
    "title": "Interactive data exploration in neuroimaging: a simple example",
    "section": "Bringing all together in an interactive app",
    "text": "Bringing all together in an interactive app\nWe have seen so far how many questions naturally arise even when looking at a intentionally very simplified dataset like the one we generated above.\nIf you read so far you have probably come up with a few other features that you would like to explore in one of the proposed visualization, and see how they affect the other perspectives from which we can look at the data.\nIt is probably clear by now that I am very intrigued by the possibility of exploration offered by dimensionality reduction methods such as UMAP. In the app which is displayed below, that you can open in full screen here, I implemented a few other possibilities that I encourage you to explore:\n\nFor instance, we might want to assess - for data exploration or based on a specific hypothesis - whether the expected effect would hold if you consider only a subset of the brain regions we looked at. This can be done by selecting the regions in the table, and observing the effect in the low-dimensional embedding.\nWe might also want to try to use other dimensionality reduction methoods besides UMAP (check for instance here). In the app, there is a choice of three different methods: Multi-dimensional Scaling (from the stats package), tSNE (from the Rtsne package) and UMAP (from the uwot package).\nAlso, to inspect the original values for each participant in each task, you just have to draw a lasso around the points you are interested in the low-dimensional embedding, and a heatmap of those values will appear in the lower-right corner of the app\n\nHopefully the potential usage of the app are self-explanatory, but if in doubt, check the animated gif at the beginning of this post."
  },
  {
    "objectID": "showcase/dimred_app/dimred_app.html#conclusion",
    "href": "showcase/dimred_app/dimred_app.html#conclusion",
    "title": "Interactive data exploration in neuroimaging: a simple example",
    "section": "Conclusion",
    "text": "Conclusion\nThis post has already become much bigger than what I expected. I hope you found it useful or at least interesting, and I would be glad to know what you think about it by dropping me an email.\nI just want to offer two small considerations to conclude.\n\nBuilding an interactive app for data exploration/analysis is something that I dreamt about ever since I started to learn Matlab many years ago. Nowadays with the introduction of libraries such as Shiny for R (and recently also for Python), it has become a task that anyone with an interest in coding can get started with in a relatively short time, and that I personally find satisfying on many different levels. When you get a bit of experience, prototyping a basic app can take as short as one or a few days (as a matter of fact it took me way more time to write this post than the app)\nIn this example I looked only at the participant-level results. Instead in neuroimaging a large amount of time and code is spent on pre-processing the data and checking the results of the many steps involved therein - e.g. checking the outcome of registering one brain to a template, or the whole-brain temporal profile of an fMRI volume after bandpass filtering or de-noising. Although many neuroimaging suite already did a great job in implementing GUIs, you might want to look into other features which are not currently offered, or you might just want to implement an interface for the pre-processing pipelines you wrote. Having this in mind, I believe that libraries like Shiny bring a great potential for developing such personalized tools for researcher in neuroimaging."
  },
  {
    "objectID": "showcase/Product_Analysis_psaraki/Product_Analysis.html",
    "href": "showcase/Product_Analysis_psaraki/Product_Analysis.html",
    "title": "Analysis of time-to-purchase (TTP)",
    "section": "",
    "text": "Code\nlibrary(bigrquery)\nlibrary(tidyverse)\nlibrary(highcharter)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(echarts4r)\nlibrary(broom)\nlibrary(flextable)\nlibrary(reactable)\nlibrary(GGally)\nlibrary(jtools)\nlibrary(ggstatsplot)\nlibrary(viridis)\n\n\n# avoid the conflict with MASS::select\nselect <- dplyr::select"
  },
  {
    "objectID": "showcase/Product_Analysis_psaraki/Product_Analysis.html#load-data-and-define-factors",
    "href": "showcase/Product_Analysis_psaraki/Product_Analysis.html#load-data-and-define-factors",
    "title": "Analysis of time-to-purchase (TTP)",
    "section": "Load data and define factors",
    "text": "Load data and define factors\nIn addition, we remove some non-informative details about the campaign.\n\n\nCode\ndf <- read_csv(\"Graded_Task_Data.csv\") %>% \n mutate_at(\n  c(\"user_pseudo_id\",\"country\",\"device\",\"OS\",\"browser\"), \n  factor\n ) %>% \n filter(country != \"(not set)\") %>% \n mutate(campaign = ifelse(campaign %in% c(\"<Other>\",\"(data deleted)\"), NA, campaign) ) %>% \n mutate(campaign = factor(campaign)) %>% \n filter(total_due > 0) %>%\n relocate(TTP)\n\n# summary(df)"
  },
  {
    "objectID": "showcase/Product_Analysis_psaraki/Product_Analysis.html#explore-overall-ttp-and-datapoints-per-country",
    "href": "showcase/Product_Analysis_psaraki/Product_Analysis.html#explore-overall-ttp-and-datapoints-per-country",
    "title": "Analysis of time-to-purchase (TTP)",
    "section": "Explore overall TTP and datapoints per country",
    "text": "Explore overall TTP and datapoints per country\nWe first notice that the Time-to-purchase (TTP) is very skewed towards short values, but has a very long right tail (high TTP).\nThe highest values are in part due to countries with very high median TTP values and very few datapoints - see the adjacent barplot and table.\nWe will therefore start by removing the countries with a median \\(TTP < 1.5*IQR(TTP)\\) of the distribution of the median TTP, where IQR(TTP) is the interquartile range of the TTP distribution. About 1% of all the datapoints are removed.\n\nTTP Distribution across countriesBarplot median TTPTable median TTPRemove countries with extreme medianTTP\n\n\nCountries above \\(1.5 * IQR(medianTTP)\\) will not be considered\n\n\nCode\n# Create a ggplot histogram\nggplot(df, aes(x = TTP)) +\n  geom_histogram(binwidth = 1200, color = \"black\", fill = \"lightblue\") +\n  labs(title = \"Distribution of TTP in seconds across all countries\", \n       x = \"Time to Purchase (TTP)\") +\n  # scale_x_continuous(labels = function(x) seconds_to_period(x)) +\n  theme_minimal() -> gg\n\nggplotly(gg)\n\n\n\n\n\n\n\n\n\n\nCode\ndf %>% \n select(country, TTP) %>% \n group_by(country) %>% \n summarise(\n  median_TTP = median(TTP),\n  percent_data = round(n()/nrow(df),4)*100\n ) %>% \n arrange(median_TTP) %>%\n mutate(name = fct_reorder(country, desc(median_TTP))) %>% \n ggplot(aes(x = median_TTP, y = name, \n            text = paste0(country,\"\\n percent data: \",percent_data) )) +\n theme_minimal() +\n theme(panel.grid = element_blank()) +\n geom_bar(stat = \"identity\", fill = \"lightblue\") +\n labs(\n  title = \"Median TTP per country\",\n  subtitle = \"Hover to inspect the % of datapoints in that country\",\n  x = \"median TTP\", \n  y = \"Country\"\n ) -> gg\n\nggplotly(gg)\n\n\n\n\n\n\n\n\nIt is possible to display the values in ascending/descending by clicking on the column header.\n\n\nCode\ndf %>% \n select(country, TTP) %>% \n group_by(country) %>% \n summarise(\n  country = unique(country),\n  median_TTP = median(TTP),\n  `% data` = round(n()/nrow(df),4)*100 \n ) %>% \n arrange(desc(`% data`)) %>% \n reactable(\n  searchable = T,\n  style = list(fontFamily = \"Calibri\")\n )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate the median TTP for each country\nmedianTTP_tbl <- df %>% \n select(country,TTP) %>% \n group_by(country) %>% \n summarise(median_TTP = median(TTP)) %>% \n arrange(desc(median_TTP))\n\nggplot(medianTTP_tbl %>% filter(median_TTP < 1e4), aes(y = median_TTP)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Median Time to Purchase\",\n       y = \"Median Time to Purchase (seconds)\") +\n  theme_minimal() -> gg\n\nggplotly(gg)\n\n\n\n\n\n\nCode\n# Remove countries with median TTP above 1.5 * IQR(medianTTP)\nthr_median_TTP <- medianTTP_tbl$median_TTP %>% median + 1.5 * IQR(medianTTP_tbl$median_TTP)\n\ndf <- df %>% \n group_by(country) %>% \n mutate(medianTTP = median(TTP)) %>% \n relocate(medianTTP) %>% \n filter(medianTTP <= thr_median_TTP) %>% \n ungroup()"
  },
  {
    "objectID": "showcase/Product_Analysis_psaraki/Product_Analysis.html#choice-of-the-reference-ttp",
    "href": "showcase/Product_Analysis_psaraki/Product_Analysis.html#choice-of-the-reference-ttp",
    "title": "Analysis of time-to-purchase (TTP)",
    "section": "Choice of the reference TTP",
    "text": "Choice of the reference TTP\nEven in countries where the median TTP is within acceptable range, we still have very long sessions. Very long sessions do not necessarily signal “outliers”, however before carrying out any analysis, we need to ask whether it makes sense to consider sessions of any length.\nSince eventually we are interested in assessing the effect of session length on revenues, we will operate a choice that takes into consideration both metrics - i.e. TTP and total_due.\nWe observe that:\n\nWhile plotting the raw TTP apparently shows that order amount decreases with increasing TTP, plotting order amount vs. log(TTP) reveals that order amount increase - at least in some orders - for sessions up to 40-60 minutes.\n80% of sessions last up to an hour, while 90% of sessions up to three hours\n\nWe will consider sessions up to an hour - and later assess the impact of shorter or longer sessions\n\nSession length decreases exponentiallyOrder value increases up to ~ 1 hourAverage Order ValueTest the increase up to ~ 1 hourRatio of high/low revenues increases up to ~ 1 hourTheshold TTP to 1 hour\n\n\nThe TTP appear to follow a power-law: the length of the sessions decreases exponentially, as shown by the log-transformed TTP and by the red lines indicating the 80th, 90th and 95th quantiles of the distribution of TTP.\nIn this plot, the order value appear to be decreasing with session time, however this is just due to the concentration of TTP in short sessions.\n\n\nCode\npar(mfrow = c(1,2))\n\npdf_TTP <- hist(df$TTP, main = \"Distribution of TTP\", xlab = \"TTP\", col = \"lightblue\")\npdf_logTTP <- hist(log(df$TTP), main = \"Distribution of log(TTP)\", xlab = \"log(TTP)\",\n                   col=\"lightblue\")\n\n\n\n\n\nCode\npar(mfrow = c(1,1))\n\npTTP <- plot(df$TTP, df$total_due, pch = 21, bg = \"lightblue\",\n     xlab = \"Time to Purchase (seconds)\",\n     ylab = \"Total Due in US$\",\n     main = \"TTP vs Total Due\")\n\n\nq <- quantile(df$TTP, probs = c(0.80, 0.90, 0.95)) %>% round\n\n# construct a plot function for the quantiles lines\nplot_quantiles <- function(q,n) {\n abline(v = q[[n]], col = \"red\", lwd = 2, lty = 2)\n text(q[[n]] -2000, 1500, names(q[n]), col = \"red\")\n}\n\n# plot the quantile lines\n1:length(q) %>% walk(~ plot_quantiles(q,.x))\n\n\n\n\n\n\n\nBy log-transforming TTP we observe that the length of the session has at least some effect on the amount of the order, up to 40-60 minutes. We will choose 1 hour as our threshold for the maximum session length, which gives us about 80% of the whole data.\nImportantly we will carry out analyses on log-transformed data to decrease the deviation of the data from normality.\n\n\nCode\nbreaks_log <- quantile(log(df$TTP), probs = seq(0.1,1,0.1))\n\nbreaks_hms <- quantile(log(df$TTP), probs = seq(0.1,1,0.1)) %>%\n exp() %>% \n round() %>% \n seconds_to_period %>% \n map_chr(~ sprintf(\"%02d:%02d:%02d\",hour(.x),minute(.x),second(.x)))\n\nxlabels <- paste0(seq(10,100,10),\"% - \", breaks_hms)\n\n# plot usd ~ logTTP\ndf %>% \n  ggplot(aes(x = log(TTP), y = total_due)) +\n  geom_point(fill = \"lightblue\", shape = 21, size = 2) +\n  geom_vline(xintercept = breaks_log, linetype = \"solid\", color = \"red\", linewidth = 0.3) +\n  geom_smooth(\n    method = \"loess\", formula = \"y ~ x\", color = \"lightgreen\", linetype = \"solid\", size = 1\n   ) +\n  theme_minimal() +\n  labs(title = \"TTP vs Total Due\",\n       x = \"Time to Purchase (seconds)\",\n       y = \"Total Due in US$\") +\n scale_x_continuous(\n  breaks = breaks_log, \n  labels = xlabels\n ) +\n theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nCode\n# # plot log(usd) ~ logTTP\ndf %>% \n # filter(TTP <= 3600) %>%\n filter(total_due >= 1) %>%\n ggplot(aes(x = log(TTP), y = total_due)) +\n  geom_point(fill = \"lightblue\", shape = 21, size = 2) +\n  geom_vline(xintercept = breaks_log, linetype = \"solid\", color = \"red\", linewidth = 0.3) +\n  geom_smooth(\n   method = \"loess\", formula = \"y ~ x\", \n   color = \"lightgreen\", linetype = \"solid\", linewidth = 1\n  ) +\n  theme_minimal() +\n  labs(title = \"TTP vs Total Due\",\n       x = \"Time to Purchase (seconds)\",\n       y = \"Log(Total Due) in US$\") +\n scale_x_continuous(\n  breaks = breaks_log, \n  labels = xlabels\n ) +\n scale_y_log10() +\n # coord_cartesian(ylim = c(0, 750)) +  # Set the y-axis limits\n theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(2,2))\n\nboxplot(df$total_due[df$total_due < 300], \n        horizontal = T, col = \"lightgreen\", \n        outpch = 3, outcol = \"lightgreen\", outcex = 1.5,\n        main = \"Total due (limited to < 200 US$)\")\n\nAOV <- round(sum(df$total_due) / nrow(df), 2)\n\nmedian_order_value <- median(df$total_due)\n\nhist(df$total_due, col = \"lightgreen\", main = \"Distribution of total_due\", xlab = \"total_due\")\n\nhist(df$total_due %>% log, col = \"lightgreen\", main = \"Distribution of log(total_due)\", xlab = \"log(total_due)\")\n\nqqnorm(log(df$total_due), col = \"lightgreen\")\nqqline(log(df$total_due),col = \"red\")\n\n\n\n\n\nAs a side note, the median order value is 48, very different from the average order value of 70.42 because there are orders extending up to 1500 USD. The order value follows a log-linear distribution\n\n\n\n\nCode\n# TTP between seconds_to_period(logTTP_range)\nlogTTP_range <- 6:9\n\n# Limit to 500 to have a better view of the distribution. \n# Stats with log are virtually identical \nthr_total_due <- 500\n\n# function to use hms of logTTP as grouping variable for the plot\nlogTTP_to_hms <- function(logTTP) {\n hms <- round(logTTP) %>% exp() %>% round %>% seconds_to_period() %>% \n  map_chr(~ sprintf(\"%02d:%02d:%02d\",hour(.x),minute(.x),second(.x)))\n}\n\n\nddf <- df %>% \n select(TTP, total_due) %>% \n mutate(logTTP = log(TTP)) %>% \n filter(logTTP >= min(logTTP_range) & logTTP <= max(logTTP_range)) %>% \n mutate(logTTP_hms = logTTP_to_hms(logTTP) ) %>% \n mutate(logUSD = log(total_due + 1)) %>% \n filter(total_due <= 500)\n\n\nggbetweenstats(\n data = ddf,\n x = logTTP_hms,\n y = total_due\n)\n\n\n\n\n\n\n\n\n\nCode\n# Proportion of high- vs. low-amount orders over time\n\nddf <- df %>% \n mutate(logTTP = log(TTP)) %>% \n select(logTTP, total_due) %>% \n mutate(ntile_logTTP = ntile(logTTP, 10)) %>% \n mutate(ntile_logUSD = ntile(log(total_due),10)) %>% \n group_by(ntile_logTTP, ntile_logUSD) %>% \n count() %>% \n ungroup(ntile_logUSD) %>% \n mutate(perctent_of_purchase = round(n/sum(n),4)*100 ) %>% \n select(-n) %>% \n arrange(ntile_logTTP)\n\n\n# Quantiles of TTP (for xaxis labels)\nq_TTP <- quantile(log(df$TTP), probs = seq(0.1, 1, 0.1) ) %>% \n exp() %>% seconds_to_period() %>% round() %>% \n map_chr(~ sprintf(\"%02d:%02d:%02d\",hour(.x), minute(.x),second(.x)))\n\n# Create a custom color palette using viridis colormap\ncustom_palette <- viridis(10)\n\n# Define the quantile values and labels\nquantile_probs <- seq(0.1, 1, 0.1)\nquantile_labels <- quantile(df$total_due %>% log(), probs = quantile_probs) %>% exp() %>% as.character()\n\n# Modify the levels and labels of ntile_logUSD\nddf$ntile_logUSD <- factor(ddf$ntile_logUSD, levels = 1:10, labels = quantile_labels)\n\nggplot(ddf, aes(x = factor(ntile_logTTP), y = perctent_of_purchase, fill = factor(ntile_logUSD))) + \n  geom_bar(stat = \"identity\", position = position_stack(reverse = TRUE), show.legend = TRUE) +\n  scale_fill_manual(values = custom_palette) +\n  xlab(\"ntile_logTTP\") +\n  ylab(\"Proportion of Purchase\") +\n  labs(fill = \"ntile_logUSD\") +\n  theme_minimal() +\n  scale_x_discrete(labels = as.character(q_TTP)) +\n  labs(\n    title = \"Proportion of low- and high-revenue orders in increasingly longer sessions\",\n    x = \"Session duration in hh:mm\",\n    y = \"Proportion of Purchases\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) -> gg\n\n# # If I use the following, the legend disappears in ggplotly\n# p <- p + guides(fill = guide_legend(reverse = TRUE), show.legend = TRUE)  # Reverse the order of the legend\n\nggplotly(gg, config = list(displayModeBar = FALSE))\n\n\n\n\n\n\n\n\nIf we wish to modify the range of TTP we consider, please enter the value of TTP in seconds below.\n\n\nCode\n# enter the desired value here to override the choice of 95% of TTP\nmaximum_TTP <- 3600\n\ndf_before_removing_extreme_TTP <- df\n\n# df <- df_before_removing_extreme_TTP\n\n# filter TTP < maximum_TTP and also create a column with its log\ndf <- df %>% \n filter(TTP <= maximum_TTP) %>% \n mutate(logTTP = log(TTP)) %>% \n relocate(TTP,logTTP)"
  },
  {
    "objectID": "showcase/Product_Analysis_psaraki/Product_Analysis.html#relationship-with-count-of-other-events",
    "href": "showcase/Product_Analysis_psaraki/Product_Analysis.html#relationship-with-count-of-other-events",
    "title": "Analysis of time-to-purchase (TTP)",
    "section": "Relationship with count of other events",
    "text": "Relationship with count of other events\nThere appear to be an association between TTP and some events, namely # of scrolls, page views, item views and user engagement. This is not surprising. However, such association is not straightforward: users with short sessions record few events, while users with long sessions can record either few or many events. Overall this is not an interesting finding.\nA better analysis of these associations should consider not only the number but also the duration of the events. This will be explored in the future.\n\nTTPlogTTP\n\n\n\n\nCode\ndf %>%\n select(TTP, total_due, n_scrolls, n_page_view, n_user_engagement, n_view_item) %>% \n GGally::ggpairs()\n\n\n\n\n\n\n\n\n\nCode\ndf %>%\n select(logTTP, total_due, n_scrolls, n_page_view, n_user_engagement, n_view_item) %>% \n GGally::ggpairs()"
  },
  {
    "objectID": "showcase/Product_Analysis_psaraki/Product_Analysis.html#user-who-buy-on-sale-products-have-longer-sessions",
    "href": "showcase/Product_Analysis_psaraki/Product_Analysis.html#user-who-buy-on-sale-products-have-longer-sessions",
    "title": "Analysis of time-to-purchase (TTP)",
    "section": "User who buy ON SALE products have longer sessions",
    "text": "User who buy ON SALE products have longer sessions\n\nAbout 1 in 8 users buy goods which are in the ON SALE category of the webstore. We hypothesized that people buying items on sale would have quicker sessions, speculating that the sale might not last long or other people might take advantage of that.\nInstead, exactly the opposite happens: people who buy items on sale have overall longer sessions that people choosing items from categories with regular prices.\nNB: The ‘Sale’ information is taken from the items added to the cart, not just from the page viewed.\n\nTTP for ON SALE vs. other categoriesNumber of events in ON SALE vs regular price purchasesRevenues for ON SALE vs regular priceOrder amount vs time for ON SALE and regular price\n\n\n\n\nCode\nddf <- df %>% \n select(TTP, is_on_sale, total_due) %>%\n mutate(logTTP = log(TTP)) %>% \n mutate(is_on_sale = ifelse(is_on_sale == 0, \"regular_price\",\"on_sale\")) %>% \n mutate(is_on_sale = factor(is_on_sale)) \n\n\nggbetweenstats(\n   data = ddf,\n   x = is_on_sale,\n   y = TTP,\n   type = \"nonparametric\"\n) + scale_y_continuous(trans = \"log\", labels = function(x) seconds_to_period(round(x)))\n\n\n\n\n\n\n\nThe longer session is not specifically linked to any kind of event, although a more accurate analysis would require to quantify the duration of different kind of events.\n\n\nCode\ndf %>% \n select(is_on_sale, n_scrolls, n_page_view, \n        n_view_item, n_user_engagement) %>%\n mutate(is_on_sale = ifelse(is_on_sale == 0, \"regular_price\",\"on_sale\")) %>% \n # mutate(is_on_sale = factor(is_on_sale)) %>% \n pivot_longer(cols = starts_with(\"n_\"), names_to = \"event_type\") %>% \n ggplot(aes(x = event_type, y = value, fill = is_on_sale)) +\n geom_boxplot() +\n theme_minimal() +\n labs(\n  title = \"Events for purchases on sale vs. with regular price\",\n  x = \"Type of event\", y = \"# of recorded events\"\n )\n\n\n\n\n\n\n\nInterestingly, people who buy items from the ON SALE section of the website spend on average 25% more than from all the other categories.\nNote: total_due values are limited to a max of 300 USD for clarity of visualization.\n\n\nCode\nddf %>% \n # filter(total_due < 300) %>%\nggbetweenstats(\n x = is_on_sale,\n y = total_due,\n ylab = \"total due in US $\",\n type = \"nonparametric\"\n) + scale_y_continuous(trans = \"log\", labels = function(x) round(x))\n\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\nThe fact that people who choose the ON SALE category spend more is true for sessions of almost all durations\n\n\nCode\nddf %>%\n mutate(qtile_logTTP = ntile(logTTP, 10)) %>%\n group_by(qtile_logTTP) %>% \n mutate(median_qtile_logTTP = median(logTTP)) %>%\n ungroup() %>% \n group_by(is_on_sale,median_qtile_logTTP) %>% \n reframe(\n  is_on_sale = unique(is_on_sale),\n  median_qtile_logTTP = unique(median_qtile_logTTP) %>% exp %>% round,\n  median_total_due = median(total_due)\n ) %>% \n ggplot(aes(x = median_qtile_logTTP, y = median_total_due, color = is_on_sale)) +\n  geom_line() +\n  geom_point(size = 1.5, shape = 16) +\n  # scale_x_continuous(breaks = x_breaks, labels = x_labels) +\n  labs(title = \"Total Due vs LogTTP Decile\",\n       x = \"TTP\",\n       y = \"Total Due\") +\n  theme_minimal() +\n  scale_x_continuous(trans = \"hms\") -> gg\n\nggplotly(gg)"
  },
  {
    "objectID": "showcase/Product_Analysis_psaraki/Product_Analysis.html#returning-customers-are-slightly-faster-to-purchase",
    "href": "showcase/Product_Analysis_psaraki/Product_Analysis.html#returning-customers-are-slightly-faster-to-purchase",
    "title": "Analysis of time-to-purchase (TTP)",
    "section": "Returning customers are (slightly) faster to purchase",
    "text": "Returning customers are (slightly) faster to purchase\nThe range of session length for returning customers is comparable to new customers, however on average they spend a few minutes less (~ 4 minutes) on the website.\n\nReturning customers TTPReturning customers eventsReturning customers order amount\n\n\n\n\nCode\nddf <- df %>% \n select(TTP, logTTP, user_pseudo_id, event_date, total_due) %>%\n mutate(log_total_due = log(total_due)) %>% \n group_by(user_pseudo_id) %>% \n mutate(is_returning_customer = ifelse( event_date == min(event_date), 0, 1 )) %>% \n ungroup()\n\nggbetweenstats(\n data = ddf,\n x = is_returning_customer,\n y = TTP,\n type = \"nonparametric\"\n) + scale_y_continuous(labels = function(x) round(seconds_to_period(x)))\n\n\n\n\n\n\n\nOverall returning customers spend less time on any event, however the difference is not substantial.\n\n\nCode\ndf %>% \n group_by(user_pseudo_id) %>% \n mutate(is_returning_customer = ifelse( event_date == min(event_date), \"new_customer\", \"returning_customer\" )) %>%\n mutate(is_returning_customer = factor(is_returning_customer)) %>% \n ungroup() %>% \n select(is_returning_customer, n_scrolls, n_page_view, \n        n_view_item, n_user_engagement) %>%\n pivot_longer(cols = starts_with(\"n_\"), names_to = \"event_type\") %>% \n ggplot(aes(x = event_type, y = value, fill = is_returning_customer)) +\n geom_boxplot() +\n theme_minimal() +\n labs(\n  title = \"Events for purchases on sale vs. with regular price\",\n  x = \"Type of event\", y = \"# of recorded events\"\n )\n\n\n\n\n\n\n\nAs an additional information, returning customers do NOT spend more than new customers.\n\n\nCode\nggbetweenstats(\n data = ddf,\n x = is_returning_customer,\n y = total_due,\n type = \"nonparametric\"\n) + scale_y_continuous(trans = \"log\", labels = function(x) round(x))"
  },
  {
    "objectID": "showcase/Product_Analysis_psaraki/Product_Analysis.html#mobiledestop-and-browser-type-have-no-effect-on-ttp",
    "href": "showcase/Product_Analysis_psaraki/Product_Analysis.html#mobiledestop-and-browser-type-have-no-effect-on-ttp",
    "title": "Analysis of time-to-purchase (TTP)",
    "section": "Mobile/Destop and browser type have no effect on TTP",
    "text": "Mobile/Destop and browser type have no effect on TTP\n\nNo effect of mobile/desktop deviceNo effect of browser typeReferrals TTPReferrals order amount\n\n\n\n\nCode\nddf <- df %>%\n select(TTP, device, total_due) %>% \n mutate(logTTP = log(TTP)) %>% \n filter(device != \"tablet\")\n\n\nggbetweenstats(\n data = ddf,\n x = device,\n y = TTP,\n type = \"nonparametric\"\n) + scale_y_continuous(labels = function(x) seconds_to_period(x))\n\n\n\n\n\nCode\nggbetweenstats(\n data = ddf,\n x = device,\n y = total_due,\n ylab = \"total due in US$\",\n type = \"nonparametric\"\n) + scale_y_continuous(\n trans = \"log\", labels = function(x) format(round(x))\n)\n\n\n\n\n\n\n\n\n\nCode\nddf <- df %>%\n select(TTP, browser, total_due) %>% \n mutate(logTTP = log(TTP)) %>% \n filter(browser %in% c(\"Chrome\",\"Safari\"))\n\n\nggbetweenstats(\n data = ddf,\n x = browser,\n y = TTP,\n type = \"nonparametric\"\n) + scale_y_continuous(labels = function(x) seconds_to_period(x))\n\n\n\n\n\nCode\nggbetweenstats(\n data = ddf,\n x = browser,\n y = total_due,\n ylab = \"total due in US$\",\n type = \"nonparametric\"\n) + scale_y_continuous(\n trans = \"log\", labels = function(x) format(round(x))\n)\n\n\n\n\n\n\n\nEffect of referrals\nPurchases from referral links are slightly faster, but the effect is not substantial (~ 1 minute faster).\n\n\n\n\n\nCode\nddf <- df %>%\n ungroup() %>% \n select(TTP,campaign,total_due) %>%\n mutate(logTTP = log(TTP)) %>% \n mutate(campaign = as.character(campaign)) %>% \n mutate(campaign = ifelse(is.na(campaign),\"(none)\",campaign)) %>% \n filter(campaign %in% c(\"(none)\",\"(referral)\"))\n # group_by(campaign) %>% \n # count()\n\nggbetweenstats(\n data = ddf,\n x = campaign,\n y = TTP,\n type = \"nonparametric\"\n) + scale_y_continuous(labels = function(x) seconds_to_period(x))\n\n\n\n\n\nCode\n# df$campaign %>% unique\n\n\n\n\nAs an additional information, returning customers do NOT spend more than new customers.\n\n\nCode\nggbetweenstats(\n data = ddf,\n x = campaign,\n y = total_due,\n ylab = \"total due in US$\",\n type = \"nonparametric\"\n) + scale_y_continuous(\n trans = \"log\", labels = function(x) format(round(x))\n)"
  },
  {
    "objectID": "showcase/Product_Analysis_psaraki/Product_Analysis.html#effect-of-referrals",
    "href": "showcase/Product_Analysis_psaraki/Product_Analysis.html#effect-of-referrals",
    "title": "Analysis of time-to-purchase (TTP)",
    "section": "Effect of referrals",
    "text": "Effect of referrals\nPurchases from referral links are slightly faster, but the effect is not substantial (~ 1 minute faster)."
  },
  {
    "objectID": "showcase/Product_Analysis_psaraki/Product_Analysis.html#seasonality",
    "href": "showcase/Product_Analysis_psaraki/Product_Analysis.html#seasonality",
    "title": "Analysis of time-to-purchase (TTP)",
    "section": "Seasonality",
    "text": "Seasonality\n\nDay levelWeek levelBy part of the dayBy hour\n\n\n\n\nCode\n# df %>%\n#  select(TTP,event_date) %>%\n#  group_by(event_date) %>% \n#  reframe(\n#   median_TTP = median(TTP),\n#   MAD_TTP = mad(TTP)\n#  ) %>% \n#  ggplot(aes(x = event_date, y = median_TTP)) +\n#  geom_line() +\n#   geom_ribbon(\n#    aes(ymin = median_TTP - MAD_TTP, ymax = median_TTP + MAD_TTP), \n#    fill = \"lightblue\", alpha = 0.5\n#   ) +\n#  theme_minimal() +\n#  theme(axis.text.x = element_text(angle = 90, hjust = 1)) -> gg\n# \n# gg\n# ggplotly(gg)\n\n\n\nddf <- df %>%\n select(TTP,event_date) %>%\n group_by(event_date) %>%\n reframe(\n  median_TTP = median(TTP),\n  MAD_TTP = mad(TTP),\n  MAD_low = floor(median(TTP) - mad(TTP)),\n  MAD_high = ceiling(median(TTP) + mad(TTP))\n )\n\nddf %>% \n  hchart(type = \"line\", hcaes(x = event_date, y = median_TTP), name = \"median TTP\") %>%\n  hc_add_series(\n   data = ddf, type = \"arearange\", name = \"MAD\", \n   hcaes(x = event_date, low = MAD_low, high = MAD_high),\n   color = \"lightblue\", opacity = 0.5 \n  ) %>%\n  hc_xAxis(type = \"datetime\") %>%\n  hc_chart(zoomType = \"x\") %>%\n  hc_tooltip(shared = TRUE) %>% \n  hc_title(text = \"TTP Range per day\") \n\n\n\n\n\n\n\n\n\n\n\nCode\nddf <- df %>% \n select(TTP,event_date) %>%\n mutate(year = year(event_date)) %>% \n mutate(week_number = paste0(year,\"-\",week(event_date)) ) %>%\n group_by(week_number) %>%\n reframe(\n  median_TTP = median(TTP),\n  MAD_TTP = mad(TTP),\n  MAD_low = floor(median(TTP) - mad(TTP)),\n  MAD_high = ceiling(median(TTP) + mad(TTP))\n ) %>% \n ungroup()\n \n\nddf %>% \n hchart(type = \"column\", hcaes(y = median_TTP, x = week_number), name = \"median TTP\") %>% \n hc_add_series(\n  data = ddf, type = \"errorbar\", hcaes(y = median_TTP, low = MAD_low, high = MAD_high),\n  name = \"MAD\"\n ) %>% \n hc_title(text = \"TTP Range per week\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nddf <- df %>% \n select(TTP, total_due, min_purchase_time) %>%\n rename(purchase_time = min_purchase_time) %>% \n mutate(\n    day_segment = case_when(\n      between(hour(purchase_time), 7, 12) ~ \"Morning\",\n      between(hour(purchase_time), 13, 17) ~ \"Afternoon\",\n      between(hour(purchase_time), 18, 24) ~ \"Evening\",\n      TRUE ~ \"Night\"\n    )\n  ) %>% \n group_by(day_segment) %>% \n reframe(\n  median_TTP = median(TTP),\n  MAD_TTP = mad(TTP),\n  MAD_low = floor(median(TTP) - mad(TTP)),\n  MAD_high = ceiling(median(TTP) + mad(TTP))\n ) %>% \n ungroup()\n\n# plot\nddf %>% \n arrange(match(day_segment, c(\"Morning\", \"Afternoon\", \"Evening\", \"Night\") )) %>% \n hchart(type = \"column\", hcaes(y = median_TTP, x = day_segment), name = \"day segment\") %>% \n hc_add_series(\n  data = ddf, type = \"errorbar\", hcaes(y = median_TTP, low = MAD_low, high = MAD_high),\n  name = \"MAD\"\n ) %>% \n hc_title(text = \"TTP Range per day segment\")\n\n\n\n\n\n\n\nCode\n# --- comparison below -----\nddf <- df %>% \n select(TTP, total_due, min_purchase_time) %>%\n rename(purchase_time = min_purchase_time) %>% \n mutate(\n    day_segment = case_when(\n      between(hour(purchase_time), 7, 12) ~ \"Morning\",\n      between(hour(purchase_time), 13, 17) ~ \"Afternoon\",\n      between(hour(purchase_time), 18, 24) ~ \"Evening\",\n      TRUE ~ \"Night\"\n    )\n  )\n\nggbetweenstats(\n data = ddf,\n x = day_segment,\n y = total_due,\n type = \"nonparametric\"\n) + scale_y_continuous(trans = \"log\", labels = function(x) round(x))\n\n\n\n\n\n\n\n\n\nCode\n# median TTP\nddf <- df %>% \n select(TTP, total_due, min_purchase_time) %>%\n mutate(hour_purchase = hour(min_purchase_time)) %>%  \n group_by(hour_purchase) %>% \n reframe(\n  median_TTP = median(TTP),\n  MAD_TTP = mad(TTP),\n  MAD_low = floor(median(TTP) - mad(TTP)),\n  MAD_high = ceiling(median(TTP) + mad(TTP))\n ) %>% \n ungroup()\n\n\n# plot\nddf %>% \n hchart(type = \"column\", hcaes(y = median_TTP, x = hour_purchase), name = \"hour\") %>% \n hc_add_series(\n  data = ddf, type = \"errorbar\", hcaes(y = median_TTP, low = MAD_low, high = MAD_high),\n  name = \"MAD\"\n ) %>% \n hc_title(text = \"TTP Range per hour segment\")\n\n\n\n\n\n\n\nCode\n# median total_due\nddf <- df %>% \n select(TTP, total_due, min_purchase_time) %>%\n mutate(hour_purchase = hour(min_purchase_time)) %>%  \n group_by(hour_purchase) %>% \n reframe(\n  median_USD = median(total_due),\n  MAD_USD = mad(total_due),\n  MAD_low = floor(median(total_due) - mad(total_due)),\n  MAD_high = ceiling(median(total_due) + mad(total_due))\n ) %>% \n ungroup()\n\n\n\n# plot\nddf %>% \n hchart(type = \"column\", hcaes(y = median_USD, x = hour_purchase), name = \"hour\") %>% \n hc_add_series(\n  data = ddf, type = \"errorbar\", hcaes(y = median_USD, low = MAD_low, high = MAD_high),\n  name = \"MAD\"\n ) %>% \n hc_title(text = \"US$ per hour segment\")"
  },
  {
    "objectID": "showcase/Marketing_Analysis_psaraki/Marketing_Analysis.html",
    "href": "showcase/Marketing_Analysis_psaraki/Marketing_Analysis.html",
    "title": "Online Campaign Performance",
    "section": "",
    "text": "Code\nlibrary(bigrquery)\nlibrary(tidyverse)\nlibrary(highcharter)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(echarts4r)\nlibrary(broom)\nlibrary(flextable)\nlibrary(reactable)\nlibrary(GGally)\nlibrary(jtools)\nlibrary(ggstatsplot)\nlibrary(viridis)\nlibrary(ggpubr)\nlibrary(htmltools)\nlibrary(ggpubr)\n\n# library(dlookr)\n\n\n# avoid the conflict with MASS::select\nselect <- dplyr::select\n\n# local folder\n# \"~/Dropbox/turing_college/Modules/Marketing_Analysis/\""
  },
  {
    "objectID": "showcase/Marketing_Analysis_psaraki/Marketing_Analysis.html#fail-to-increase-conversion-rate",
    "href": "showcase/Marketing_Analysis_psaraki/Marketing_Analysis.html#fail-to-increase-conversion-rate",
    "title": "Online Campaign Performance",
    "section": "Fail to increase conversion rate",
    "text": "Fail to increase conversion rate\nOverall, we don’t obseve a difference in the conversion rate - from browsing only to purchasing - due to campains\nWhen no campaigns are present on the website, 7% of all the visits result in a purchase. Comparably, when a campaign is running, 6% of the visitors end up purchasing.\nThe conversion rate for campaign sessions appears to be slightly higher on Thursday and Friday, but not more than what can be expected by chance.\n\nOverall conversion ratePer weekdayNo significant difference in proportions\n\n\n\n\nCode\ncontingency_table <- df %>% \n  select(campaign, purchase_session) %>% \n  group_by(campaign, purchase_session) %>% \n  count() %>% \n  pivot_wider(names_from = purchase_session, values_from = n, values_fill = 0)\n\n# --------------  ggbarstats --------------------------\n\n# Conversion rate across all days\nggbarstats(\n data = df,\n x = purchase_session,\n y = campaign,\n label = \"both\",\n results.subtitle = F\n) + \n scale_fill_manual(values = c(\"#F8766D\",\"#00BFC4\")) +\n labs(\n  title = \"Conversion rate in all sessions (campaign and no-campaign)\",\n  x = \"Campaign\"\n )\n\n\n\n\n\nCode\n# The following two are replaced by the line plot in the next tab\n\n# # Conversion rate for each day in campaign sessions\n# ggbarstats(\n#  data = df %>% filter(campaign == \"YES\"),\n#  x = purchase_session,\n#  y = weekday_name,\n#  label = \"both\",\n#  results.subtitle = F\n# ) +\n#  scale_fill_manual(values = c(\"#F8766D\",\"#00BFC4\")) +\n#  labs(\n#   title = \"Conversion rate in campaign sessions\",\n#   x = \"Day of the week\"\n#  )\n# \n# \n# \n# # Conversion rate for each day in no-campaign sessions\n# ggbarstats(\n#  data = df %>% filter(campaign == \"NO\"),\n#  x = purchase_session,\n#  y = weekday_name,\n#  label = \"both\",\n#  results.subtitle = F\n# ) +\n#  scale_fill_manual(values = c(\"#F8766D\",\"#00BFC4\")) +\n#  labs(\n#   title = \"Conversion rate in NO-campaign sessions\",\n#   x = \"Day of the week\"\n#  )\n\n\n\n\n\n\nCode\n# Barplot and line plot of the conversion rates\nddf <- df %>%\n group_by(weekday_name, campaign, purchase_session) %>% \n count() %>% \n group_by(weekday_name, campaign) %>% \n mutate(total = sum(n)) %>% \n filter(purchase_session == \"YES\") %>% \n mutate(conversion_rate = round(n/total*100,1))  \n # select(weekday_name, campaign, conversion_rate)\n\n\nddf %>%\n mutate(campaign = fct_rev(campaign)) %>% \n ggplot(\n  aes(x = weekday_name, y = conversion_rate, color = campaign, \n      group = campaign,\n      text = paste(\"Campaign:\", campaign, \"<br>Conversion Rate:\", conversion_rate, \"%\"))\n ) +\n # geom_bar(aes(fill = campaign), stat = \"identity\", position = \"dodge\") +\n geom_line() +\n geom_point() +\n theme_minimal() +\n labs(\n  title = \"Conversion rate for campaing and no-campaign sessions\",\n  x = \"Day of the week\",\n  y = \"Conversion Rate\"\n ) +\n scale_y_continuous(labels = scales::percent_format(scale = 1)) -> gg\n\nggplotly(gg, tooltip = \"text\", config = list(displayModeBar = FALSE))\n\n\n\n\n\n\n\n\n\n\nCode\n# Wednesday\nprop_Thu <- ddf %>% \n filter(weekday_name == \"Wed\") %>% \n rename(n_converted = n) %>% \n ungroup() %>% \n select(n_converted, total) %>% \n arrange(total)\n\ncat(\"Wednesday\")\n\n\nWednesday\n\n\nCode\nprop.test(x = prop_Thu$n_converted, n = prop_Thu$total, alternative = \"greater\")\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  prop_Thu$n_converted out of prop_Thu$total\nX-squared = 1.388e-28, df = 1, p-value = 0.5\nalternative hypothesis: greater\n95 percent confidence interval:\n -0.03071573  1.00000000\nsample estimates:\n    prop 1     prop 2 \n0.07179487 0.07021663 \n\n\nCode\n# Thursday\nprop_Thu <- ddf %>% \n filter(weekday_name == \"Thu\") %>% \n rename(n_converted = n) %>% \n ungroup() %>% \n select(n_converted, total) %>% \n arrange(total)\n\ncat(\"Thursday\")\n\n\nThursday\n\n\nCode\nprop.test(x = prop_Thu$n_converted, n = prop_Thu$total, alternative = \"greater\")\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  prop_Thu$n_converted out of prop_Thu$total\nX-squared = 1.2833e-28, df = 1, p-value = 0.5\nalternative hypothesis: greater\n95 percent confidence interval:\n -0.02947896  1.00000000\nsample estimates:\n    prop 1     prop 2 \n0.06349206 0.06127591 \n\n\nCode\n# Friday\nprop_Fri <- ddf %>% \n filter(weekday_name == \"Fri\") %>% \n rename(n_converted = n) %>% \n ungroup() %>% \n select(n_converted, total) %>% \n arrange(total)\n\n\ncat(\"Friday\")\n\n\nFriday\n\n\nCode\nprop.test(x = prop_Fri$n_converted, n = prop_Fri$total, alternative = \"greater\")\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  prop_Fri$n_converted out of prop_Fri$total\nX-squared = 2.6122e-29, df = 1, p-value = 0.5\nalternative hypothesis: greater\n95 percent confidence interval:\n -0.03484194  1.00000000\nsample estimates:\n    prop 1     prop 2 \n0.08187135 0.07921039"
  },
  {
    "objectID": "showcase/Marketing_Analysis_psaraki/Marketing_Analysis.html#session-length-differ-but-not-in-an-interesting-way",
    "href": "showcase/Marketing_Analysis_psaraki/Marketing_Analysis.html#session-length-differ-but-not-in-an-interesting-way",
    "title": "Online Campaign Performance",
    "section": "Session length differ, but not in an interesting way",
    "text": "Session length differ, but not in an interesting way\nCampaign sessions and purchase sessions (either on- or off-campaign) are significantly slighltly longer (up to 5 minutes) , however there is no interaction between campaign and session type (browse-only or purchasing). For instance, it is not the case that campaign sessions are longer for purchase and shorter for browse-only with respect to no-campaign sessions.\nAs expected from the previous anova, campaign sessions are slightly longer each day of the week, although the median value is associated with high variability.\nOn Saturday the purchasing campaign session is on average much shorter than the sessions not related to a campaign. However, a closer inspection shows that these are just 4 purchasing events, which makes a statistical comparison meaningless.\n\nSession time by conversion and campaignSession length during or outside a campaignSession length for Purchasing sessions (TTP)Browse-only sessions (no purchase)\n\n\nNB: Boxplots show session time in seconds, however anova2 was run on log(session_time) to approximate normally distributed values.\n\n\nCode\nddf <- df %>% \n select(TTP, campaign, purchase_session) %>%\n mutate(logTTP = log(TTP)) %>% \n mutate(campaign = fct_rev(campaign))\n\n# # Table\n# ddf %>%\n#  group_by(campaign, purchase_session) %>%\n#  reframe(mean_TTP = mean(TTP))\n\n\nddf %>% \n select(logTTP, campaign, purchase_session) %>% \n ggboxplot(\n  x = \"purchase_session\", y = \"logTTP\", color = \"campaign\"\n ) +\n scale_y_continuous(trans = \"exp\", labels = function(x)(round(exp(x)))) +\n labs(\n  title = \"Difference in session length\",\n  subtitle = \"for purchase/browse-only during and off campaign\",\n  y = \"Session length in seconds\",\n  x = \"Is purchase session\"\n )\n\n\n\n\n\nCode\nanova2 <- aov(formula = logTTP ~ campaign * purchase_session, data = ddf)\ncat(\"Session time is higher for campaign and purchase, but there is no interaction\")\n\n\nSession time is higher for campaign and purchase, but there is no interaction\n\n\nCode\nsummary(anova2)\n\n\n                             Df Sum Sq Mean Sq F value   Pr(>F)    \ncampaign                      1      5    4.99  13.181 0.000283 ***\npurchase_session              1    300  299.77 791.219  < 2e-16 ***\ncampaign:purchase_session     1      1    0.55   1.458 0.227199    \nResiduals                 55688  21099    0.38                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# plot(anova2)\n# car::Anova(anova2, type = \"III\")\n\n\n\n\n\n\nCode\n# define a fn to produce the three following plots\nplot_session_length <- function(df, title) {\n df %>% \n group_by(weekday_name, campaign) %>% \n reframe(\n  weekday_name = unique(weekday_name),\n  median_TTP = median(TTP),\n  MAD_low = floor(median(TTP) - mad(TTP)),\n  MAD_high = ceiling(median(TTP) + mad(TTP))\n ) %>%\n mutate(campaign = fct_rev(campaign)) %>%\n ggplot(aes(x = weekday_name, y = median_TTP, fill = campaign)) +\n geom_col(position = \"dodge\") +\n geom_errorbar(\n  aes(ymin = MAD_low, ymax = MAD_high),\n  position = position_dodge(width = 0.9),  # Adjust width for dodge position\n  width = 0.2, color = \"grey\"  # Width of error bars\n ) +\n labs(title = title, \n      x = \"Day of the week\",\n      y = \"Median TTP\") +\n theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    panel.grid.major.x = element_blank(),  # Remove vertical grid lines\n    panel.grid.minor = element_blank()     # Remove minor grid lines\n  ) -> gg\n\n return(ggplotly(gg) %>% config(displayModeBar = FALSE))\n}\n\n\n# overall session length (purchase and browse-only sessions)\nplot_session_length(\n df,\n title = \"Overall session length in- and outside campaign\"\n)\n\n\n\n\n\n\n\n\n\n\nCode\nplot_session_length(\n df %>% filter(purchase_session == \"YES\"),\n title = \"Purchase sessions length in- and outside campaign\"\n)\n\n\n\n\n\n\nCode\n# # The following compares TTP for campaign and no-campaign\n# # on Saturday, when the TTP appears shorter. However, a closer\n# # inspection reveals that these are just 4 observations, therefore\n# # a statistical test is meaningless\n\n# ddf <- df %>%\n#  filter(purchase_session == \"YES\", weekday_name == \"Sat\") %>%\n#  select(campaign, TTP)\n# \n# ggbetweenstats(\n#  data = ddf,\n#  x = campaign,\n#  y = TTP\n# ) + \n#  scale_y_continuous(trans = \"log\")\n#\n# ddf %>% filter(campaign == \"YES\")\n\n\n# # The following carries out an anova TTP ~ weekday * campaign\n# # There are no significant differences\n# ddf <- df %>% \n#  filter(purchase_session == \"YES\") %>% \n#  select(TTP, campaign, weekday_name) %>% \n#  mutate(logTTP = log(TTP))\n# \n# anova2 <- aov(TTP ~ campaign * weekday_name, data = ddf)\n# summary(anova2)\n# car::Anova(anova2, type = \"III\")\n\n\n\n\n\n\nCode\nplot_session_length(\n df %>% filter(purchase_session == \"NO\"),\n title = \"Browse-only sessions length in- and outside campaign\"\n)"
  },
  {
    "objectID": "showcase/Marketing_Analysis_psaraki/Marketing_Analysis.html#revenues-highest-at-mid-week",
    "href": "showcase/Marketing_Analysis_psaraki/Marketing_Analysis.html#revenues-highest-at-mid-week",
    "title": "Online Campaign Performance",
    "section": "Revenues highest at mid-week",
    "text": "Revenues highest at mid-week\nThe highest revenues for campaign-related orders were generated at mid-week, while they were much lower during the weekend and on Monday.\nThere are also differences in the median amount order between campaign and no-campaign sessions in some days, however not more than what could be expected by chance (assessed with an anova2).\n\nTotal revenue due to campaignsRange of order valuesTotal revenues for in- and out-campaign\n\n\n\n\nCode\ndf %>%\n  filter(purchase_session == \"YES\", campaign == \"YES\") %>%\n  select(weekday_name, revenue) %>% \n  group_by(weekday_name) %>% \n  reframe(\n    total_revenue = sum(revenue)\n  ) %>% \n  ggplot(aes(x = weekday_name, y = total_revenue, \n   text = paste(weekday_name, \"\\n\",\"Total Revenue: \", total_revenue, \"US$\"))\n  ) +\n  geom_bar(stat = \"identity\", fill = \"#F8766D\") +\n  labs(\n   title = \"Total revenue from campaign orders\",\n   x = \"Day of the week\",\n   y = \"Total revenue in USD\"\n  ) +\n  theme_minimal() -> gg\n\nggplotly(gg, tooltip = \"text\") %>% config(displayModeBar = FALSE)\n\n\n\n\n\n\n\n\n\n\nCode\ndf %>%\n filter(purchase_session == \"YES\") %>%\n select(weekday_name, revenue, campaign) %>% \n group_by(weekday_name, campaign) %>% \n mutate(campaign = fct_rev(campaign)) %>%\n ggplot(aes(x = weekday_name, y = revenue, fill = campaign)) +\n geom_boxplot() +\n # scale_y_continuous(limits = c(0,120)) +\n scale_y_continuous(breaks = seq(0, max(df$revenue), by = 100)) +\n labs(\n  title = \"Order value in- and outside-campaign\",\n  x = \"Day of the week\",\n  y = \"Revenue\"\n ) +\n theme_minimal() -> gg\n\n\nggplotly(gg) %>%\n layout(yaxis = list(range = c(0, 140))) %>% \n layout(boxmode = \"group\")\n\n\n\n\n\n\nCode\n# Anova 2 : revenue ~ weekday_name * campaign\n# No significant difference.\n\nddf <- df %>% \n filter(purchase_session == \"YES\") %>% \n select(revenue, weekday_name, campaign) %>% \n mutate(log_revenue = log(revenue))\n\ncat(\"No significant effect of of mean log_revenue or campaign\")\n\n\nNo significant effect of of mean log_revenue or campaign\n\n\nCode\nanova2 <- aov(log_revenue ~ weekday_name * campaign, data = ddf)\nsummary(anova2)\n\n\n                        Df Sum Sq Mean Sq F value Pr(>F)\nweekday_name             6    4.5  0.7583   0.994  0.427\ncampaign                 1    0.0  0.0211   0.028  0.868\nweekday_name:campaign    6    3.4  0.5662   0.742  0.615\nResiduals             3870 2951.9  0.7628               \n\n\nCode\n# Note that revenues are log-normally distributed\n# however this has no effect on the anova2\ndf$revenue[df$revenue > 0] %>% log() %>% hist(main = \"Log(revenue)\")\n\n\n\n\n\n\n\n\n\nCode\ndf %>%\n  filter(purchase_session == \"YES\") %>%\n  select(weekday_name, revenue, campaign) %>% \n  group_by(weekday_name, campaign) %>% \n  mutate(campaign = fct_rev(campaign)) %>%\n  reframe(\n    total_revenue = sum(revenue)\n  ) %>% \n  ggplot(aes(\n   x = weekday_name, y = total_revenue, \n   fill = campaign, \n   text = paste(weekday_name, \"\\n\",\"Total Revenue: \", total_revenue, \"\\n\", \"Campaign: \", campaign))\n  ) +\n  geom_bar(stat = \"identity\") +\n  # scale_y_continuous(\n  #   trans = \"log\",\n  #   labels = function(x) round(x)\n  # ) +\n theme_minimal() -> gg\n\nggplotly(gg, tooltip = \"text\")"
  },
  {
    "objectID": "showcase/Marketing_Analysis_psaraki/Marketing_Analysis.html#roi-91.17",
    "href": "showcase/Marketing_Analysis_psaraki/Marketing_Analysis.html#roi-91.17",
    "title": "Online Campaign Performance",
    "section": "ROI: 91.17%",
    "text": "ROI: 91.17%\n\n\nCode\n# NB: here we consider again the initial data, with no threshold for session duration\n\nnov <- read_csv(\"01_nov.csv\")\ndec <- read_csv(\"02_dec.csv\")\njan <- read_csv(\"03_jan.csv\")\n\ntotal_df <- rbind(nov, dec, jan)\n\nmarketing_cost <- read_csv(\"adsense_monthly.csv\")\ncost_of_campaigns <- sum(marketing_cost$Cost) %>% round(2)\n\n\ncampaigns_revenues <- total_df[total_df$campaign != \"none\", \"revenue\"] %>% na.omit() %>% sum\n\n# campaigns_revenues <- total_df %>% \n#  filter(campaign != \"none\", !is.na(revenue)) %>%\n#  select(revenue) %>% pull %>% sum\n\n\nROI <- campaigns_revenues / cost_of_campaigns\n\nROI_pct <- (round(ROI,4) - 1) * 100\n\nKPI_name <- \"Return on Investment\"\n\n\nReturn on Investment: The total cost for marketing campaigns was 2432.45 US$. The revenues generated by these campaigns were 4650 US$.\n\n\nCode\ndiv(\n  style = \"display: flex; flex-wrap: wrap; justify-content: space-around; align-items: center; font-family: sans-serif;\",\n  div(\n    style = \"background-color: #f39c12; color: white; padding: 10px; border-radius: 5px; margin: 10px; text-align: center;\",\n    div(style = \"font-size: 28px;\", KPI_name),\n    div(style = \"font-size: 36px;\", paste(ROI_pct, \"%\"))\n  )\n)\n\n\n\n\nReturn on Investment\n91.17 %\n\n\n\n\nCode\n# table with the cost of every marketing campaign\nmarketing_cost %>% \n group_by(Campaign) %>%\n reframe(\n  cost = round(sum(Cost),2)\n ) %>% flextable()\n\n\n\nCampaigncostBlackFriday_V1164.49BlackFriday_V2364.14Data Share Promo1,430.17Holiday_V1143.78Holiday_V2129.10NewYear_V144.30NewYear_V2156.47"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "blog",
    "section": "",
    "text": "How to replace for loops using purrr::map\n\n\n\n\n\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2022\n\n\nLC\n\n\n\n\n\n\n  \n\n\n\n\nIt is time to make a blog…\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nDec 23, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html",
    "title": "Framingham CHD logistic regression",
    "section": "",
    "text": "The Framingham dataset (available also on kaggle) is an ongoing study which has been running since 1948. It aims to identify demographic, lifestyle and medical factors which are associated with the 10-year risk of developing heart disease.\nHere we use logistic regression to model the 10-year risk of coronary heart disease (CHD). The dataset consists of:\n\n4238 individuals, 15% of which with assessed risk of CHD\n43% males, 57% females between 32 and 70 yrs (median age = 49)\n15 recorded variables, including education, smoking habits, blood pressure measurements and anomalies, blood sample indices (e.g. glucose), medicament assumptions."
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#two-independent-samples-t-tests",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#two-independent-samples-t-tests",
    "title": "Framingham CHD logistic regression",
    "section": "Two-independent-samples t-tests",
    "text": "Two-independent-samples t-tests\n(an excercise in fitting many models using purrr::map)\n\n\nCode\nlibrary(tidyr)\n\nttests <- train %>%\n   # select(\"ten_year_chd\", \"age\", \"glucose\") %>%\n   \n   select(-c(\"education\",\"male\",\"current_smoker\",\"bp_meds\",\n           \"prevalent_stroke\",\"prevalent_hyp\", \"diabetes\")) %>% \n   na.omit() %>%\n   \n   # pivot longer to prepare the nesting\n   pivot_longer(\n      cols = -ten_year_chd, \n      names_to = \"variable\", values_to = \"value\"\n   ) %>% \n   group_by(variable) %>% \n   group_nest() %>%\n   \n   # fit a model to each nested df\n   mutate(\n    ttmod = map(data, ~ lm(value ~ ten_year_chd, data = .x))\n   ) %>% \n\n   # get the stats\n   mutate(\n      tidy = map(ttmod, broom::tidy)\n   ) %>% \n   unnest(tidy) %>% \n   filter(term == \"ten_year_chd1\") %>% \n   \n   # correct for multiple comparisons\n   mutate(adj_pval = p.adjust(p.value, method = \"fdr\")) %>% \n   # filter(adj_pval <= 0.01) %>% \n   relocate(adj_pval, .after=variable) %>% \n   arrange(adj_pval)\n\n\nttests %>%\n  select(variable, statistic, adj_pval) %>%\n  rename(`t value` = statistic) %>%\n  flextable() %>%\n  set_formatter(adj_pval = function(x) sprintf(\"%.2e\", x))\n\n\n\nvariablet valueadj_pvalage12.08155631.01e-31sys_bp11.97328401.73e-31dia_bp8.22156028.85e-16glucose6.56539951.28e-10bmi5.63082603.22e-08tot_chol4.03263917.59e-05cigs_per_day2.11417713.96e-02heart_rate0.85121593.95e-01\n\n\n\n\n\nAge\n\n\nCode\nggbetweenstats(\n   data = train,\n   x = ten_year_chd,\n   y = age\n)\n\n\n\n\n\n\n\nSystolic blood pressure\n\n\nCode\nggbetweenstats(\n   data = train,\n   x = ten_year_chd,\n   y = sys_bp\n)\n\n\n\n\n\n\n\nDiastolic blood pressure\n\n\nCode\n# \nggbetweenstats(\n   data = train,\n   x = ten_year_chd,\n   y = dia_bp\n)\n\n\n\n\n\n\n\nGlucose\n\n\nCode\n# \nggbetweenstats(\n   data = train,\n   x = ten_year_chd,\n   y = glucose\n)\n\n\n\n\n\n\n\nBMI\n\n\nCode\n# \nggbetweenstats(\n   data = train,\n   x = ten_year_chd,\n   y = bmi\n)\n\n\n\n\n\n\n\nCholesterol level\n\n\nCode\n# \nggbetweenstats(\n   data = train,\n   x = ten_year_chd,\n   y = tot_chol\n)"
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#correlation-matrix",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#correlation-matrix",
    "title": "Framingham CHD logistic regression",
    "section": "Correlation matrix",
    "text": "Correlation matrix\nExploring the correlation matrix of the numerical variables to detect potential collinearity\n\n\nCode\ntrain %>% \n # select(-c(\"education\",\"male\",\"current_smoker\",\"bp_meds\",\n #           \"prevalent_stroke\",\"prevalent_hyp\", \"diabetes\")) %>%\n select(ten_year_chd, age, sys_bp, dia_bp, bmi, glucose, tot_chol) %>% \n na.omit() %>% \n # mutate_at(vars(-ten_year_chd), ~ log(. + 1)) %>% \n GGally::ggpairs(\n    aes(color = ten_year_chd),\n    upper = list(continuous = wrap(\"cor\", size = 3)),\n    lower = list(continuous = wrap(\"points\", alpha = 0.7, size = 1)),\n    diag = list(continuous = wrap(\"densityDiag\", alpha = 0.5))\n )"
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#observations---numerical-evs",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#observations---numerical-evs",
    "title": "Framingham CHD logistic regression",
    "section": "Observations - numerical EVs",
    "text": "Observations - numerical EVs\n\nage appears to be the most discriminant variable for ten year CHD prediction. Other EVs which are significantly different (after correction with FDR) in people with and without CHD risk are bmi, sys_bp, dia_bp, glucose, tot_chol.\nthe systolic sys_bp and diastolic dia_bp blood pressure are correlated with each other - as expected. Since CHD is especially related to hypertension, we will only use systolic blood pressure\nbmi and age are also correlated with blood pressure, however they appear to be important variables for CHD and their correlation with blood pressure is mild, so we will keep them\nNB: the number of cigarettes per day (cigs_per_day) will be explored as a categorical variable (see below) with levels for tens of cigarettes per day"
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#chi-square-tests",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#chi-square-tests",
    "title": "Framingham CHD logistic regression",
    "section": "Chi-square tests",
    "text": "Chi-square tests\n\nSex (male)\n\n\nCode\nlibrary(ggstatsplot)\n\nggbarstats(\n   data = train %>% select(ten_year_chd, male),\n   x = male,\n   y = ten_year_chd,\n   label = \"both\"\n)\n\n\n\n\n\n\n\nHypertension\n\n\nCode\nggbarstats(\n   data = train %>% select(ten_year_chd, prevalent_hyp),\n   x = prevalent_hyp,\n   y = ten_year_chd,\n   label = \"both\"\n)\n\n\n\n\n\n\n\nCigarettes per day\nThe values are too sparse to treat this as a continous variable. We will therefore group the # cigs per day in tens and treat this as a categorical variable.\nA first visual exploration shows that there appear to be mild differences in the proportion of people smoking different amount of cigarettes per day.\nWhen carrying out a chi-square test of independence, we are warned that the results might be incorrect. This appears to be due to the fact that there are too few observations in the category for 50 and 60 cigarettes per day. When these levels are removed, a significant (p = 0.0066) dependence is shown between risk group and amount of cigarettes per day.\n\n\nCode\n# Suppress summarise info\noptions(dplyr.summarise.inform = FALSE)\n\ntrain %>% \n   select(ten_year_chd, cigs_per_day) %>% \n   na.omit() %>% \n   mutate(cigs_factor = round(cigs_per_day/10,0)) %>% \n   group_by(ten_year_chd, cigs_factor) %>% \n   summarise(count = n(), .group = \"drop\") %>% \n   group_by(ten_year_chd) %>%\n   mutate(prop = count / sum(count)) %>% \n   select(ten_year_chd, cigs_factor, prop) %>%\n   ggplot(aes(x = factor(cigs_factor), y = prop, fill = ten_year_chd)) +\n   geom_bar(stat = \"identity\", position = \"dodge\") + \n   labs(x = \"Tens of Cigarettes per Day\", y = \"Proportion in each group\", fill = \"CHD Status\")\n\n\n\n\n\nCode\ntrain %>% \n   select(ten_year_chd, cigs_per_day) %>% \n   mutate(tens_cigarettes = round(cigs_per_day/10, 0) ) %>% \n   filter(tens_cigarettes < 5) %>% \n   select(-cigs_per_day) %>% \n   na.omit() %>% \n   xtabs(~ ten_year_chd + tens_cigarettes, data = .) %>% \n   chisq.test()  %>% tidy() %>% kable(format = \"html\") %>%\n   add_header_above(c(\"Chi-Square CHD Risk ~ Tens_of_cigarettes\" = 4)) %>%\n   kable_styling()\n\n\n\n\n \nChi-Square CHD Risk ~ Tens_of_cigarettes\n  \n    statistic \n    p.value \n    parameter \n    method \n  \n \n\n  \n    14.22453 \n    0.0066119 \n    4 \n    Pearson's Chi-squared test \n  \n\n\n\n\n\n\n\nCurrent smoker\n\n\nCode\nggbarstats(\n   data = train %>% select(ten_year_chd, current_smoker),\n   x = current_smoker,\n   y = ten_year_chd,\n   label = \"both\"\n)\n\n\n\n\n\n\n\nBlood pressure medication\n\n\nCode\nggbarstats(\n   data = train %>% select(ten_year_chd, bp_meds),\n   x = bp_meds,\n   y = ten_year_chd,\n   label = \"both\"\n)\n\n\n\n\n\n\n\nEducation\n\n\nCode\nggbarstats(\n   data = train %>% select(ten_year_chd, education),\n   x = education,\n   y = ten_year_chd,\n   label = \"both\"\n)"
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#association-matrix-categorical",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#association-matrix-categorical",
    "title": "Framingham CHD logistic regression",
    "section": "Association matrix (categorical)",
    "text": "Association matrix (categorical)\nGraphically display the association between binary categorical variables, including ten_year_chd to assess the presence of correlated predictors\n\n\nCode\nlibrary(vcd)\n\ntrain_cat <- train %>% select_if(is.factor) %>% select(-c(\"ten_year_chd\",\"education\"))\n\npairs(\n   table(train_cat), \n   diag_panel = pairs_diagonal_mosaic(offset_varnames=-2.5),    #move down variable names\n   upper_panel_args = list(shade = TRUE),                       #set shade colors\n   lower_panel_args = list(shade = TRUE)\n)"
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#observations---categorical-evs",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#observations---categorical-evs",
    "title": "Framingham CHD logistic regression",
    "section": "Observations - categorical EVs",
    "text": "Observations - categorical EVs\n\nThe 10 year risk of CHD is more prevalent in males than in females\nNot surprisingly, hypertension and assumption of blood pressure medications is also associated with risk. We will also examine the latter, although only ~ 3% of the sample takes these medications\nThe number of cigarettes smoked per day (in groups of 10) also increaes the risk of CHD. Interestingly, still, more than 50% of the people who are at risk do not smoke. It would be interesting to have data about the quality of the air of the place where they live, or whether their partner is a smoker.\nIt is also interesting that the relationship between amount of cigarettes per day and risk is not linear: there are more people at risk among those who smoke 20-30 or 40-50 cigs per day. while among those who smoke 10-20 or 30-40 cigs per day the proportion of people with and without risk is almost 50%. This strange behaviour can probably be - at least in part - explained by the fact that smokers tend to underestimate the amount of cigarettes smoked.\nInstead, the fact of being a current smoker does not appear to be a risk factor\nThere is an interaction between being make, having hypertension and being a current smoker. We will examine whether this interaction can increase the prediction, although the fact of currently smoking per se apparently does not."
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#age-1",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#age-1",
    "title": "Framingham CHD logistic regression",
    "section": "age",
    "text": "age\n\n\nCode\nlibrary(gridExtra)\n\nboxplot_tt <- function(response, predictor) {\n   tt <- t.test(train[[response]] ~ train[[predictor]], data =train)\n   \n   res <- paste0(\"t = \", round(tt$statistic,2), \" p = \", round(tt$p.value,4))\n   \n   p <- train %>% na.omit() %>% \n      ggplot(aes(x = .data[[predictor]], y = .data[[response]])) +\n      geom_boxplot() +\n      labs(title = paste0(predictor,\" vs \", response),\n           subtitle = res)\n   \n   return(p)\n   \n}\n\np1 <- boxplot_tt(\"age\", \"male\")\np2 <- boxplot_tt(\"age\", \"prevalent_hyp\")\np3 <- boxplot_tt(\"age\", \"bp_meds\")\n\ngrid.arrange(p1, p2, p3, nrow = 1)"
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#bmi-1",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#bmi-1",
    "title": "Framingham CHD logistic regression",
    "section": "bmi",
    "text": "bmi\n\n\nCode\np1 <- boxplot_tt(\"bmi\", \"male\")\np2 <- boxplot_tt(\"bmi\", \"prevalent_hyp\")\np3 <- boxplot_tt(\"bmi\", \"bp_meds\")\n\ngrid.arrange(p1, p2, p3, nrow = 1)"
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#hypertension-sys_bp",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#hypertension-sys_bp",
    "title": "Framingham CHD logistic regression",
    "section": "hypertension (sys_bp)",
    "text": "hypertension (sys_bp)\n\n\nCode\np1 <- boxplot_tt(\"sys_bp\", \"male\")\np2 <- boxplot_tt(\"sys_bp\", \"prevalent_hyp\")\np3 <- boxplot_tt(\"sys_bp\", \"bp_meds\")\n\ngrid.arrange(p1, p2, p3, nrow = 1)"
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#glucose-level",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#glucose-level",
    "title": "Framingham CHD logistic regression",
    "section": "glucose level",
    "text": "glucose level\n\n\nCode\np1 <- boxplot_tt(\"glucose\", \"male\")\np2 <- boxplot_tt(\"glucose\", \"prevalent_hyp\")\np3 <- boxplot_tt(\"glucose\", \"bp_meds\")\n\ngrid.arrange(p1, p2, p3, nrow = 1)"
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#cholesterol-level-1",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#cholesterol-level-1",
    "title": "Framingham CHD logistic regression",
    "section": "cholesterol level",
    "text": "cholesterol level\n\n\nCode\np1 <- boxplot_tt(\"tot_chol\", \"male\")\np2 <- boxplot_tt(\"tot_chol\", \"prevalent_hyp\")\np3 <- boxplot_tt(\"tot_chol\", \"bp_meds\")\n\ngrid.arrange(p1, p2, p3, nrow = 1)"
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#bmi-2",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#bmi-2",
    "title": "Framingham CHD logistic regression",
    "section": "bmi",
    "text": "bmi\n\n\nCode\n# bmi ~ male\nggbetweenstats(\n   data = train,\n   x = male,\n   y = bmi\n)"
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#glucose-1",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#glucose-1",
    "title": "Framingham CHD logistic regression",
    "section": "glucose",
    "text": "glucose\n\n\nCode\n# glucose ~ hypertension\nggbetweenstats(\n   data = train,\n   x = prevalent_hyp,\n   y = glucose\n)"
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#cholesterol",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#cholesterol",
    "title": "Framingham CHD logistic regression",
    "section": "cholesterol",
    "text": "cholesterol\n\n\nCode\n# cholesterol ~ hypertension\nggbetweenstats(\n   data = train,\n   x = prevalent_hyp,\n   y = tot_chol\n)"
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#bp_meds",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#bp_meds",
    "title": "Framingham CHD logistic regression",
    "section": "bp_meds",
    "text": "bp_meds\n\n\nCode\n# bp_meds ~ prevalent_hyp\ndf %>% \n  select(sys_bp, prevalent_hyp, bp_meds) %>%\n  group_by(bp_meds, prevalent_hyp) %>% \n  summarise(\n    median_sys_bp = median(sys_bp),\n    count_prevalent_hyp = n()\n  ) \n\n\n# A tibble: 5 × 4\n# Groups:   bp_meds [3]\n  bp_meds prevalent_hyp median_sys_bp count_prevalent_hyp\n  <fct>   <fct>                 <dbl>               <int>\n1 0       0                      122                 2891\n2 0       1                      150.                1170\n3 1       1                      165                  124\n4 <NA>    0                      121                   31\n5 <NA>    1                      154.                  22\n\n\nNA imputation on train and test set\n\n\nCode\n# ------------------------- train --------------------------------------------\ntrain <- train %>%\n   # impute the bmi as the median of males/females\n   group_by(ten_year_chd, male) %>%\n   mutate(bmi = ifelse(is.na(bmi), median(bmi, na.rm = T),bmi)) %>%\n   ungroup() %>%\n   \n   # impute glucose and cholesterol level as the median of prevalent_hyp\n   group_by(ten_year_chd, prevalent_hyp) %>%\n   mutate(glucose = ifelse(is.na(glucose), median(glucose, na.rm=T), glucose)) %>%\n   mutate(tot_chol = ifelse(is.na(tot_chol), median(tot_chol, na.rm=T), tot_chol)) %>%\n   ungroup() %>% \n   \n   # remove people where bp_meds is not unknown\n   filter(!is.na(bp_meds)) %>% \n   \n   # remove people with no information about education level\n   filter(!is.na(education)) %>% \n   \n   # # remove the mean from numerical variables\n   # mutate_if(is.numeric, ~ . - mean(., na.rm = TRUE)) %>% \n   \n   # remove residual NAs\n   na.omit()\n   \n   \n   \n\ntrain %>% \n   select(\n      ten_year_chd, age, sys_bp, bmi, glucose, tot_chol, \n      male, prevalent_hyp, bp_meds, education\n   ) %>%\n   diagnose %>%\n   flextable\n\n\n\nvariablestypesmissing_countmissing_percentunique_countunique_rateten_year_chdfactor0020.0008093889agenumeric00390.0157830838sys_bpnumeric002130.0861999191bminumeric001,0990.4447592068glucosenumeric001210.0489680291tot_cholnumeric002270.0918656414malefactor0020.0008093889prevalent_hypfactor0020.0008093889bp_medsfactor0020.0008093889educationfactor0040.0016187778\n\n\n\nCode\n# ------------------------- test --------------------------------------------\ntest <- test %>%\n   # impute the bmi as the median of males/females\n   group_by(ten_year_chd, male) %>%\n   mutate(bmi = ifelse(is.na(bmi), median(bmi, na.rm = T),bmi)) %>%\n   ungroup() %>%\n   \n   # impute glucose and cholesterol level as the median of prevalent_hyp\n   group_by(ten_year_chd, prevalent_hyp) %>%\n   mutate(glucose = ifelse(is.na(glucose), median(glucose, na.rm=T), glucose)) %>%\n   mutate(tot_chol = ifelse(is.na(tot_chol), median(tot_chol, na.rm=T), tot_chol)) %>%\n   ungroup() %>% \n   \n   # remove people where bp_meds is not unknown\n   filter(!is.na(bp_meds)) %>% \n   \n   # remove people with no information about education level\n   filter(!is.na(education)) %>% \n   \n   # # remove the mean from numerical variables\n   # mutate_if(is.numeric, ~ . - mean(., na.rm = TRUE)) %>% \n   \n   # remove residual NAs\n   na.omit()\n\n\n\n\ntest %>% \n   select(\n      ten_year_chd, age, sys_bp, bmi, glucose, tot_chol, \n      male, prevalent_hyp, bp_meds, education\n   ) %>%\n   diagnose %>%\n   flextable\n\n\n\nvariablestypesmissing_countmissing_percentunique_countunique_rateten_year_chdfactor0020.001265823agenumeric00370.023417722sys_bpnumeric002010.127215190bminumeric008590.543670886glucosenumeric001020.064556962tot_cholnumeric002130.134810127malefactor0020.001265823prevalent_hypfactor0020.001265823bp_medsfactor0020.001265823educationfactor0040.002531646"
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#procedure-for-stepwise-modelling",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#procedure-for-stepwise-modelling",
    "title": "Framingham CHD logistic regression",
    "section": "Procedure for stepwise modelling",
    "text": "Procedure for stepwise modelling\nWe start by fitting the “maximal” model, which includes all the provided EVs, to have a baseline estimate for the AUC of this model and of the impact of the correlation between variables we previously detected with EDA - for the latter, we will check the VIF.\nThen we fit the full model, which includes all the variables we previously selected based on the EDA. This will show how well the hints gathered in the EDA are actually reflected in the significance of the coefficients. Specifically, compared to the “maximal” model.\nWe will then proceed to eliminate EVs based on whether the coefficients are not sigificant or have a very small effect (in terms of odds ratio). At each step we evaluate the AIC and AUC to detect whether reducing the EVs led to an increase or decrease of the performance.\nFinally we will examine the confusion matrix and adjust the threshold for binary prediction in order to have maximum Sensitivity (more on this later)."
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#format-of-the-summary-tables",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#format-of-the-summary-tables",
    "title": "Framingham CHD logistic regression",
    "section": "Format of the summary tables",
    "text": "Format of the summary tables\nThe coefficients are reported in two forms (at the expense of verbosity):\n\noriginal, non-standardized and non-exponentiated (using summary()): to allow calculation of the odds ratio (via exp()) of 10 year CHD given the actual values of the EVs for a specific person.\nstandardized and exponentiated (using jtools::summ()): to allow comparing the magnitude of the coefficients, to have the confidence intervals and to calculate the VIF."
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#results",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#results",
    "title": "Framingham CHD logistic regression",
    "section": "Results",
    "text": "Results\nThe AIC was similar across all examined models (between 1887 and 1890), therefore this metric had a small weight in the choice of the final model. Similarly, all the models explained at most 17% of the total variance in the data, which is not a substantial amount.\nThe VIF was not critical, even in the maximal model, therefore the impact of correlated predictors was minimal or negligible.\nThe maximal model highlighted the contribution of gender, age, systolic blood pressure, and to a lesser extent the prevalence of hypertension and education. Importantly, the p values for some of these variables was higher than in simpler models, most likely due to collinearity with other variables. The AUC of this model was 0.703.\nOur full model highlighted the same variables. In addition reducing the number of EVs increased the significance of most of the selected EVs (especially age and systolic blood pressure). The AUC of this model was 0.705.\nWe then removed EVs with small or no effect from the full model, specifically blood pressure medications, bmi, cholesterol and education. This led to an AUC of 0.711, and all variables significant.\nFurther removing the EVs specifying whether the patient had hypertension led to a simpler model and did not substantially change the AUC (0.713).\nRemarkably, this model contained only gender, age, systolic blood pressure and glucose blood level, in addition to number of cigarettes smoked per day. That is, two common demographic variables, and two physiologic measurements which can be easily retrieved with blood pressure measurement and a simple blood exam\nThe increase in odds to develop a CHD in 10 years were as follows for each EV:\n\nsex: males have \\(exp(0.449) \\approx 57\\%\\) greater odds of CHD risk than females\ncigarettes per day : \\(exp(0.1766) \\approx 19\\%\\) greater odds for any additional 10 cigarettes smoked every day\nage : the odds of CHD increase \\(exp(0.0658) \\approx 6.8\\%\\) every year\nsystolic blood pressure : \\(exp(0.0186) \\approx 1.8\\%\\) greater odds for each mmHg\nglucose : \\(exp(0.0075) \\approx 0.7\\%\\) greater odds for each mg\n\nThe intercept can be interpreted as the probability of a female being at risk when all predictors are set to 0. In our case we have predictors for age, glucose and blood pressure, which are obviously not zero, therefore this interpretation of the intercept is not really helpful in our case. For the sake of calculation: \\(p = \\frac{e^{\\beta_0}}{1 + e^{\\beta_0}} = \\frac{exp(-8.643)}{1+exp(-8.643)} = 0.000176\\)\nIt is at this point very interesting to notice that:\n\nAssumption of blood pressure medications does not reduce the 10 years risk of CHD\nHaving hypertension has only a small effect on the prediction of developing a risk of CHD in 10 years\n\nThis is a very interesting result from a practical perspective, since it means that generic demographic and physiological variables are much more predictive of the risk of CHD in 10 years with respect to a diagnosis of hypertension, and that the assumption of blood pressure medicaments - presumably to normalize the blood pressure level in people with hypertension - does not reduce the risk of CHD\nOur final model therefore includes:\n\nsex\nage\nsystolic blood pressure\nglucose\ncigarettes per day\n\nAfterwards, we evaluated the performance of our model on the test data. Using a default probability of 0.5 for binary prediction leads to a poor Sensitivity, in that only ~ 7% of the people with a real 10 year CHD risk (\\(\\frac{16}{16+219}\\)) are predicted as being at risk.\nSince in our case the aim is to maximise the detection of people who are truly at risk - even at expense of inflating the number of false negatives - we lowered the threshold of binary prediction to 0.1. This allowed us to reach a sensitivity of 84%.\nThe automatic stepwise regression (stepAIC) identifies the same variables as our final model, plus a small (uncorrected significant) contribution of education level and prevalence of hypertension. However, this model has a smaller AUC (0.703) than our final model, and is also more complex. Therefore we retained our final model.\nFinally, we hypothesized that the imbalance between Sensitivity and Specificity could be due to the high proportion of people not at risk in our dataset. Therefore, as a last test, we sampled an equal number of people with and without risk.\nIn this balanced samples model the AUC increased to 0.738. By using a threshold for binary decision of 0.3, this model correctly predicts 91.4% of the people with actual CHD risk, and a false negative rate of 70%. By adopting a more liberal threshold of 0.4, the model predicts 81.3% of the people with actual CHD, and about 50% of the people with no risk of CHD.\nA final remark This model captures only a fraction of the total variance in the data (~ 15%). It is odd that other variables widely thought to be associated with cardiovascular diseases were not recorded, such as stress level (e.g. # hours of work per day/week), alcohol comsumption, quality of the air.\n\ndo_logistic_regression : a function to carry out the logistic regression and print out the summary of the model, the ROC curve - and the AUC - as well as the confusion matrix.\nplot_ROC : a function that generates an interactive ROC that allows to inspect Sensitivity and Specificity for different levels of threshold for binary decision\n\n\nCode\ndo_logistic_regression <- function(EVs, train_data = train, test_data = test, thresh = 0.5) {\n   \n   # Assemble the formula\n   formula_string <- paste(EVs, collapse = \" + \")\n   formula_obj <- as.formula(paste(\"ten_year_chd ~\", formula_string))\n   \n   fit <- glm(formula_obj, family = \"binomial\", data = train_data)\n\n   \n   # # Standard R summary\n   summary(fit) %>% print()\n   \n   \n   # Print a summary\n   # summary(fit) %>% print()\n   if (length(EVs) > 1) {\n      jtools::summ(fit, confint = T, vifs = T, scale = F, exp = T, digits = 2) %>% print()\n   } else {\n      jtools::summ(fit, confint = T, vifs = T, scale = F, exp = T, digits = 2) %>% print()\n   }\n\n   # Estimated probability values from the logistic model\n   phat = predict(fit, newdata = test_data, type = \"response\")\n\n   # Confusion matrix and derived quantities - using caret::confusionmatrix\n   sprintf(\"\\n Using a threshold for prediction = %.2f \\n\\n\", thresh) %>% cat()\n   \n   thresh <- thresh\n   predicted <- ifelse(phat > thresh, 1, 0)\n   \n   actual_predicted <- tibble(\n      actual = test_data$ten_year_chd, \n      predicted = ifelse(phat > thresh, 1, 0) \n   )\n   \n   caret::confusionMatrix(\n      as.factor(predicted), # predicted\n      as.factor(test_data$ten_year_chd), # actual \n      positive = \"1\"  # the positive class in our case is \"1\"\n   ) %>% print()\n   \n   return(phat)\n}\n\n\nplot_ROC <- function(test_data = test, phat = phat) {\n # Define the probability thresholds you want to use\n   pthresh <- seq(0, 1, 0.1)\n   \n   # Create the ROC curve\n   roc_curve <- roc(test_data$ten_year_chd, phat, plot = F, print.auc=T)\n   \n   # Get the coordinates (sensitivity and specificity) for the specified thresholds\n   coordinates <- pROC::coords(roc_curve, pthresh)\n\n    # Using highcharter\n    h <- coordinates %>% \n    hchart(\n        type = \"line\",\n        hcaes(x = specificity, y = sensitivity, threshold = threshold)\n    ) %>% \n    hc_xAxis(title = list(text = \"Specificity\"), reversed = TRUE, min = 0, max = 1) %>%\n    hc_yAxis(title = list(text = \"Sensitivity\"), min = 0, max = 1) %>% \n    hc_add_series(\n        data = coordinates,\n        type = \"scatter\",\n        hcaes(x = specificity, y = sensitivity, threshold = threshold),\n        marker = list(radius = 6, fillColor = \"lightblue\", lineWidth = 2),\n        tooltip = list(\n            headerFormat = \"\",\n            pointFormat = \"<b>Threshold: {point.threshold:.2f}</b><br>\n                           Sensitivity: {point.y:.2f}<br>\n                           Specificity: {point.x:.2f}\")\n    ) %>% \n     hc_chart(aspectRatio = 1) %>%   # Set the equal aspect ratio\n     hc_title(text = paste(\"ROC curve - AUC =\", round(roc_curve$auc,3)))\n    \n    return(h)\n}"
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#manual-stepwise",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#manual-stepwise",
    "title": "Framingham CHD logistic regression",
    "section": "Manual stepwise",
    "text": "Manual stepwise\n\nMaximal model\nMaximal model, including all the variables in the dataset. We use this only as a benchmark for the models instructed by EDA.\nWe observe high VIF values in sys_bp and dia_bp - as expected.\n\n\nCode\nEVs <- train %>% select(-ten_year_chd) %>% colnames()\nphat <- do_logistic_regression(EVs, thresh = 0.5)\n\n\n\nCall:\nglm(formula = formula_obj, family = \"binomial\", data = train_data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.8415  -0.5828  -0.4175  -0.2897   2.8226  \n\nCoefficients:\n                    Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       -7.4497344  0.8561698  -8.701  < 2e-16 ***\nmale1              0.3800302  0.1329243   2.859  0.00425 ** \nage                0.0608765  0.0083022   7.333 2.26e-13 ***\neducation2        -0.3243521  0.1517706  -2.137  0.03259 *  \neducation3        -0.3327557  0.1860997  -1.788  0.07377 .  \neducation4        -0.0502180  0.1969542  -0.255  0.79874    \ncurrent_smoker1    0.1717930  0.1922979   0.893  0.37166    \ncigs_per_day       0.0149836  0.0078998   1.897  0.05787 .  \nbp_meds1           0.3755894  0.2932894   1.281  0.20033    \nprevalent_stroke1  0.8294602  0.6158164   1.347  0.17800    \nprevalent_hyp1     0.3669952  0.1660561   2.210  0.02710 *  \ndiabetes1          0.3126526  0.3897172   0.802  0.42241    \ntot_chol           0.0004310  0.0014095   0.306  0.75980    \nsys_bp             0.0121563  0.0046474   2.616  0.00890 ** \ndia_bp             0.0004736  0.0078561   0.060  0.95193    \nbmi                0.0141265  0.0151335   0.933  0.35058    \nheart_rate        -0.0068913  0.0050928  -1.353  0.17601    \nglucose            0.0063839  0.0028683   2.226  0.02604 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2114.4  on 2470  degrees of freedom\nResidual deviance: 1854.4  on 2453  degrees of freedom\nAIC: 1890.4\n\nNumber of Fisher Scoring iterations: 5\n\nMODEL INFO:\nObservations: 2471\nDependent Variable: ten_year_chd\nType: Generalized linear model\n  Family: binomial \n  Link function: logit \n\nMODEL FIT:\nχ²(17) = 259.91, p = 0.00\nPseudo-R² (Cragg-Uhler) = 0.17\nPseudo-R² (McFadden) = 0.12\nAIC = 1890.45, BIC = 1995.07 \n\nStandard errors: MLE\n-------------------------------------------------------------------------\n                          exp(Est.)   2.5%   97.5%   z val.      p    VIF\n----------------------- ----------- ------ ------- -------- ------ ------\n(Intercept)                    0.00   0.00    0.00    -8.70   0.00       \nmale1                          1.46   1.13    1.90     2.86   0.00   1.25\nage                            1.06   1.05    1.08     7.33   0.00   1.32\neducation2                     0.72   0.54    0.97    -2.14   0.03   1.13\neducation3                     0.72   0.50    1.03    -1.79   0.07   1.13\neducation4                     0.95   0.65    1.40    -0.25   0.80   1.13\ncurrent_smoker1                1.19   0.81    1.73     0.89   0.37   2.61\ncigs_per_day                   1.02   1.00    1.03     1.90   0.06   2.74\nbp_meds1                       1.46   0.82    2.59     1.28   0.20   1.11\nprevalent_stroke1              2.29   0.69    7.66     1.35   0.18   1.04\nprevalent_hyp1                 1.44   1.04    2.00     2.21   0.03   1.94\ndiabetes1                      1.37   0.64    2.93     0.80   0.42   1.88\ntot_chol                       1.00   1.00    1.00     0.31   0.76   1.07\nsys_bp                         1.01   1.00    1.02     2.62   0.01   3.64\ndia_bp                         1.00   0.99    1.02     0.06   0.95   2.92\nbmi                            1.01   0.98    1.04     0.93   0.35   1.23\nheart_rate                     0.99   0.98    1.00    -1.35   0.18   1.10\nglucose                        1.01   1.00    1.01     2.23   0.03   1.89\n-------------------------------------------------------------------------\n\n Using a threshold for prediction = 0.50 \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1329  213\n         1   16   22\n                                          \n               Accuracy : 0.8551          \n                 95% CI : (0.8367, 0.8721)\n    No Information Rate : 0.8513          \n    P-Value [Acc > NIR] : 0.3513          \n                                          \n                  Kappa : 0.1249          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.09362         \n            Specificity : 0.98810         \n         Pos Pred Value : 0.57895         \n         Neg Pred Value : 0.86187         \n             Prevalence : 0.14873         \n         Detection Rate : 0.01392         \n   Detection Prevalence : 0.02405         \n      Balanced Accuracy : 0.54086         \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nCode\nplot_ROC(test, phat)\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls < cases\n\n\n\n\n\n\n\n\n\nFull with selected variables\nThis is the “full” model with all the variables chosen based on the EDA.\nNo problems with collinearity.\n\n\nCode\nEVs  <- c(\n   \"male\", \"prevalent_hyp\", \"bp_meds\", \"education\",\n   \"age\", \"sys_bp\", \"bmi\", \"glucose\", \"tot_chol\", \"cigs_per_day\"\n)\n\nphat <- do_logistic_regression(EVs, thresh = 0.5)\n\n\n\nCall:\nglm(formula = formula_obj, family = \"binomial\", data = train_data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.9398  -0.5882  -0.4186  -0.2876   2.8221  \n\nCoefficients:\n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    -7.8924235  0.7224553 -10.924  < 2e-16 ***\nmale1           0.3996399  0.1306947   3.058 0.002230 ** \nprevalent_hyp1  0.3564960  0.1628251   2.189 0.028565 *  \nbp_meds1        0.4381388  0.2883448   1.519 0.128638    \neducation2     -0.3319065  0.1512202  -2.195 0.028174 *  \neducation3     -0.3409724  0.1858949  -1.834 0.066621 .  \neducation4     -0.0373354  0.1959238  -0.191 0.848870    \nage             0.0613141  0.0080512   7.616 2.63e-14 ***\nsys_bp          0.0118211  0.0034909   3.386 0.000709 ***\nbmi             0.0137041  0.0146029   0.938 0.348011    \nglucose         0.0075754  0.0021169   3.579 0.000345 ***\ntot_chol        0.0002287  0.0014069   0.163 0.870891    \ncigs_per_day    0.0191629  0.0052321   3.663 0.000250 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2114.4  on 2470  degrees of freedom\nResidual deviance: 1859.5  on 2458  degrees of freedom\nAIC: 1885.5\n\nNumber of Fisher Scoring iterations: 5\n\nMODEL INFO:\nObservations: 2471\nDependent Variable: ten_year_chd\nType: Generalized linear model\n  Family: binomial \n  Link function: logit \n\nMODEL FIT:\nχ²(12) = 254.88, p = 0.00\nPseudo-R² (Cragg-Uhler) = 0.17\nPseudo-R² (McFadden) = 0.12\nAIC = 1885.48, BIC = 1961.04 \n\nStandard errors: MLE\n----------------------------------------------------------------------\n                       exp(Est.)   2.5%   97.5%   z val.      p    VIF\n-------------------- ----------- ------ ------- -------- ------ ------\n(Intercept)                 0.00   0.00    0.00   -10.92   0.00       \nmale1                       1.49   1.15    1.93     3.06   0.00   1.21\nprevalent_hyp1              1.43   1.04    1.97     2.19   0.03   1.87\nbp_meds1                    1.55   0.88    2.73     1.52   0.13   1.09\neducation2                  0.72   0.53    0.97    -2.19   0.03   1.11\neducation3                  0.71   0.49    1.02    -1.83   0.07   1.11\neducation4                  0.96   0.66    1.41    -0.19   0.85   1.11\nage                         1.06   1.05    1.08     7.62   0.00   1.25\nsys_bp                      1.01   1.00    1.02     3.39   0.00   2.06\nbmi                         1.01   0.99    1.04     0.94   0.35   1.16\nglucose                     1.01   1.00    1.01     3.58   0.00   1.02\ntot_chol                    1.00   1.00    1.00     0.16   0.87   1.06\ncigs_per_day                1.02   1.01    1.03     3.66   0.00   1.22\n----------------------------------------------------------------------\n\n Using a threshold for prediction = 0.50 \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1332  219\n         1   13   16\n                                          \n               Accuracy : 0.8532          \n                 95% CI : (0.8347, 0.8703)\n    No Information Rate : 0.8513          \n    P-Value [Acc > NIR] : 0.433           \n                                          \n                  Kappa : 0.0915          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.06809         \n            Specificity : 0.99033         \n         Pos Pred Value : 0.55172         \n         Neg Pred Value : 0.85880         \n             Prevalence : 0.14873         \n         Detection Rate : 0.01013         \n   Detection Prevalence : 0.01835         \n      Balanced Accuracy : 0.52921         \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nCode\nplot_ROC(test, phat)\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls < cases\n\n\n\n\n\n\n\n\n\nRemove no-effect EVs\nRemove education, bp_meds, bmi, tot_chol since they have no effect on the model.\nThe AIC slightly decreases with respect to the full model.\n\n\nCode\n# Remove education, bp_meds, bmi, tot_chol since they have\n# no effect on the model\nEVs  <- c(\n   \"male\", \"prevalent_hyp\",\n   \"age\", \"sys_bp\", \"glucose\", \"cigs_per_day\"\n)\n\nphat <- do_logistic_regression(EVs, thresh = 0.5)\n\n\n\nCall:\nglm(formula = formula_obj, family = \"binomial\", data = train_data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1088  -0.5889  -0.4241  -0.2951   2.7635  \n\nCoefficients:\n                Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    -8.027822   0.562910 -14.261  < 2e-16 ***\nmale1           0.434934   0.127776   3.404 0.000664 ***\nprevalent_hyp1  0.387263   0.160552   2.412 0.015862 *  \nage             0.064891   0.007775   8.346  < 2e-16 ***\nsys_bp          0.013408   0.003393   3.952 7.76e-05 ***\nglucose         0.007572   0.002106   3.595 0.000324 ***\ncigs_per_day    0.018188   0.005197   3.500 0.000465 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2114.4  on 2470  degrees of freedom\nResidual deviance: 1870.0  on 2464  degrees of freedom\nAIC: 1884\n\nNumber of Fisher Scoring iterations: 5\n\nMODEL INFO:\nObservations: 2471\nDependent Variable: ten_year_chd\nType: Generalized linear model\n  Family: binomial \n  Link function: logit \n\nMODEL FIT:\nχ²(6) = 244.31, p = 0.00\nPseudo-R² (Cragg-Uhler) = 0.16\nPseudo-R² (McFadden) = 0.12\nAIC = 1884.05, BIC = 1924.73 \n\nStandard errors: MLE\n----------------------------------------------------------------------\n                       exp(Est.)   2.5%   97.5%   z val.      p    VIF\n-------------------- ----------- ------ ------- -------- ------ ------\n(Intercept)                 0.00   0.00    0.00   -14.26   0.00       \nmale1                       1.54   1.20    1.98     3.40   0.00   1.16\nprevalent_hyp1              1.47   1.08    2.02     2.41   0.02   1.83\nage                         1.07   1.05    1.08     8.35   0.00   1.17\nsys_bp                      1.01   1.01    1.02     3.95   0.00   1.95\nglucose                     1.01   1.00    1.01     3.60   0.00   1.01\ncigs_per_day                1.02   1.01    1.03     3.50   0.00   1.21\n----------------------------------------------------------------------\n\n Using a threshold for prediction = 0.50 \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1335  219\n         1   10   16\n                                          \n               Accuracy : 0.8551          \n                 95% CI : (0.8367, 0.8721)\n    No Information Rate : 0.8513          \n    P-Value [Acc > NIR] : 0.3513          \n                                          \n                  Kappa : 0.0958          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.06809         \n            Specificity : 0.99257         \n         Pos Pred Value : 0.61538         \n         Neg Pred Value : 0.85907         \n             Prevalence : 0.14873         \n         Detection Rate : 0.01013         \n   Detection Prevalence : 0.01646         \n      Balanced Accuracy : 0.53033         \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nCode\nplot_ROC(test, phat)\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls < cases\n\n\n\n\n\n\n\n\n\nRemove prevalent_hyp\nRemove prevalent_hyp since it is only marginally significant\nNote that the AIC only marginally increases (less than 0.2%) and this model is much simpler and requires only 4 variables, two of which are common demographic variables (age and sex), while the other two can be easily gathered through a pressure measurement and a blood test.\n\n\nCode\n# Remove prevalent_hyp since it is only marginally significant\nEVs  <- c(\n   \"male\", \"age\", \"sys_bp\", \"glucose\", \"cigs_per_day\"\n)\n\nphat <- do_logistic_regression(EVs, thresh = 0.5)\n\n\n\nCall:\nglm(formula = formula_obj, family = \"binomial\", data = train_data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1053  -0.5899  -0.4330  -0.2982   2.8162  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -8.643579   0.504565 -17.131  < 2e-16 ***\nmale1         0.449590   0.127385   3.529 0.000417 ***\nage           0.065845   0.007745   8.502  < 2e-16 ***\nsys_bp        0.018684   0.002597   7.194 6.29e-13 ***\nglucose       0.007547   0.002112   3.573 0.000353 ***\ncigs_per_day  0.017858   0.005184   3.445 0.000571 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2114.4  on 2470  degrees of freedom\nResidual deviance: 1875.8  on 2465  degrees of freedom\nAIC: 1887.8\n\nNumber of Fisher Scoring iterations: 5\n\nMODEL INFO:\nObservations: 2471\nDependent Variable: ten_year_chd\nType: Generalized linear model\n  Family: binomial \n  Link function: logit \n\nMODEL FIT:\nχ²(5) = 238.54, p = 0.00\nPseudo-R² (Cragg-Uhler) = 0.16\nPseudo-R² (McFadden) = 0.11\nAIC = 1887.82, BIC = 1922.69 \n\nStandard errors: MLE\n--------------------------------------------------------------------\n                     exp(Est.)   2.5%   97.5%   z val.      p    VIF\n------------------ ----------- ------ ------- -------- ------ ------\n(Intercept)               0.00   0.00    0.00   -17.13   0.00       \nmale1                     1.57   1.22    2.01     3.53   0.00   1.16\nage                       1.07   1.05    1.08     8.50   0.00   1.17\nsys_bp                    1.02   1.01    1.02     7.19   0.00   1.14\nglucose                   1.01   1.00    1.01     3.57   0.00   1.01\ncigs_per_day              1.02   1.01    1.03     3.44   0.00   1.20\n--------------------------------------------------------------------\n\n Using a threshold for prediction = 0.50 \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1337  219\n         1    8   16\n                                          \n               Accuracy : 0.8563          \n                 95% CI : (0.8381, 0.8733)\n    No Information Rate : 0.8513          \n    P-Value [Acc > NIR] : 0.3             \n                                          \n                  Kappa : 0.0987          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.06809         \n            Specificity : 0.99405         \n         Pos Pred Value : 0.66667         \n         Neg Pred Value : 0.85925         \n             Prevalence : 0.14873         \n         Detection Rate : 0.01013         \n   Detection Prevalence : 0.01519         \n      Balanced Accuracy : 0.53107         \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nCode\nplot_ROC(test, phat)\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls < cases\n\n\n\n\n\n\n\n\n\nAssess categorical interactions\nThe number of cigarettes per day is a relevant factor, although being a current smoker is not. Since there is an interaction between being male, current smoker and having hypertension, it is worth exploring whether including these interactions would improve the performance of the model.\nHowever, the results show that the coefficients associated with male:current_smoker and current_smoker:prevalent_hyp are not significant - especially after correction. Also, they are highly collinear with cigs_per_day - which becomes not significant - and do not improve the model performance.\nFor all these resasons, these interactions will not be included in the final model.\n\n\nCode\n# Try to include `male:current_smoker` and `current_smoker:prevalent_hyp`\nEVs  <- c(\n   \"male\", \"age\", \"sys_bp\", \"glucose\", \"male:current_smoker\", \"current_smoker:prevalent_hyp\"\n)\n\nphat <- do_logistic_regression(EVs, thresh = 0.5)\n\n\n\nCall:\nglm(formula = formula_obj, family = \"binomial\", data = train_data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.0959  -0.5871  -0.4262  -0.2940   2.7761  \n\nCoefficients:\n                                Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                    -8.071834   0.575043 -14.037  < 2e-16 ***\nmale1                           0.411137   0.177391   2.318 0.020466 *  \nage                             0.064531   0.007790   8.284  < 2e-16 ***\nsys_bp                          0.013450   0.003398   3.959 7.54e-05 ***\nglucose                         0.007486   0.002101   3.563 0.000367 ***\nmale0:current_smoker1           0.386439   0.213094   1.813 0.069760 .  \nmale1:current_smoker1           0.560270   0.206397   2.715 0.006637 ** \ncurrent_smoker0:prevalent_hyp1  0.472103   0.200758   2.352 0.018693 *  \ncurrent_smoker1:prevalent_hyp1  0.287916   0.201998   1.425 0.154058    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2114.4  on 2470  degrees of freedom\nResidual deviance: 1871.3  on 2462  degrees of freedom\nAIC: 1889.3\n\nNumber of Fisher Scoring iterations: 5\n\nMODEL INFO:\nObservations: 2471\nDependent Variable: ten_year_chd\nType: Generalized linear model\n  Family: binomial \n  Link function: logit \n\nMODEL FIT:\nχ²(8) = 243.03, p = 0.00\nPseudo-R² (Cragg-Uhler) = 0.16\nPseudo-R² (McFadden) = 0.11\nAIC = 1889.32, BIC = 1941.64 \n\nStandard errors: MLE\n-------------------------------------------------------------------------------\n                                       exp(Est.)   2.5%   97.5%   z val.      p\n------------------------------------ ----------- ------ ------- -------- ------\n(Intercept)                                 0.00   0.00    0.00   -14.04   0.00\nmale1                                       1.51   1.07    2.14     2.32   0.02\nage                                         1.07   1.05    1.08     8.28   0.00\nsys_bp                                      1.01   1.01    1.02     3.96   0.00\nglucose                                     1.01   1.00    1.01     3.56   0.00\nmale0:current_smoker1                       1.47   0.97    2.23     1.81   0.07\nmale1:current_smoker1                       1.75   1.17    2.62     2.71   0.01\ncurrent_smoker0:prevalent_hyp1              1.60   1.08    2.38     2.35   0.02\ncurrent_smoker1:prevalent_hyp1              1.33   0.90    1.98     1.43   0.15\n-------------------------------------------------------------------------------\n \n-------------------------------------------\n                                        VIF\n------------------------------------ ------\n(Intercept)                                \nmale1                                  2.24\nage                                    1.18\nsys_bp                                 1.96\nglucose                                1.01\nmale0:current_smoker1                  4.18\nmale1:current_smoker1                  4.18\ncurrent_smoker0:prevalent_hyp1         3.57\ncurrent_smoker1:prevalent_hyp1         3.57\n-------------------------------------------\n\n Using a threshold for prediction = 0.50 \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1332  221\n         1   13   14\n                                          \n               Accuracy : 0.8519          \n                 95% CI : (0.8334, 0.8691)\n    No Information Rate : 0.8513          \n    P-Value [Acc > NIR] : 0.4892          \n                                          \n                  Kappa : 0.0786          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.059574        \n            Specificity : 0.990335        \n         Pos Pred Value : 0.518519        \n         Neg Pred Value : 0.857695        \n             Prevalence : 0.148734        \n         Detection Rate : 0.008861        \n   Detection Prevalence : 0.017089        \n      Balanced Accuracy : 0.524955        \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nCode\nplot_ROC(test, phat)\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls < cases\n\n\n\n\n\n\n\n\n\nSigmoid for age\nWe also decided to try to model age as a sigmoid, under the assumption that the odds of a CHD risk in ten years were overall smaller for people < 40 and would increase at a steeper rate afterwards, however the perfomance of the model did not change.\n\n\nCode\nsigmoid_function <- function(x, a, b, c, d) {\n  # Sigmoid function formula: a + (b - a) / (1 + exp(-c * (x - d)))\n  return(a + (b - a) / (1 + exp(-c * (x - d))))\n}\n\n# Define the parameters for the sigmoid function to control the curve\na_linear <- 0      # Start value (linear increase starts from 0)\nb_linear <- 1      # End value of linear increase (1 represents 100%)\nc_steepness <- 0.15 # Steepness factor for the sigmoid curve (adjust as needed)\nd_transition <- 50 # Age at which the transition occurs (50 in your case)\n\n\ntrain_sigmoid_age <- train %>% \n mutate(age_sigmoid = sigmoid_function(age, a_linear, b_linear, c_steepness, d_transition))\n\ntest_sigmoid_age <- test %>% \n mutate(age_sigmoid = sigmoid_function(age, a_linear, b_linear, c_steepness, d_transition))\n\n\nplot(train_sigmoid_age$age, train_sigmoid_age$age_sigmoid)\n\n\n\n\n\nCode\n# The proxy_age variable now has a smooth transition, increasing linearly until 50 and more steeply after that.\n\n\n\nEVs  <- c(\n   \"male\", \"age_sigmoid\", \"sys_bp\", \"glucose\", \"cigs_per_day\"\n)\n\nphat <- do_logistic_regression(\n train_data = train_sigmoid_age,\n test_data = test_sigmoid_age,\n EVs, thresh = 0.1\n)\n\n\n\nCall:\nglm(formula = formula_obj, family = \"binomial\", data = train_data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.0739  -0.5929  -0.4279  -0.2942   2.7784  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -6.433353   0.401903 -16.007  < 2e-16 ***\nmale1         0.449367   0.127351   3.529 0.000418 ***\nage_sigmoid   2.197337   0.258184   8.511  < 2e-16 ***\nsys_bp        0.018622   0.002598   7.167 7.64e-13 ***\nglucose       0.007502   0.002109   3.556 0.000376 ***\ncigs_per_day  0.017897   0.005184   3.452 0.000556 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2114.4  on 2470  degrees of freedom\nResidual deviance: 1874.9  on 2465  degrees of freedom\nAIC: 1886.9\n\nNumber of Fisher Scoring iterations: 5\n\nMODEL INFO:\nObservations: 2471\nDependent Variable: ten_year_chd\nType: Generalized linear model\n  Family: binomial \n  Link function: logit \n\nMODEL FIT:\nχ²(5) = 239.49, p = 0.00\nPseudo-R² (Cragg-Uhler) = 0.16\nPseudo-R² (McFadden) = 0.11\nAIC = 1886.87, BIC = 1921.74 \n\nStandard errors: MLE\n--------------------------------------------------------------------\n                     exp(Est.)   2.5%   97.5%   z val.      p    VIF\n------------------ ----------- ------ ------- -------- ------ ------\n(Intercept)               0.00   0.00    0.00   -16.01   0.00       \nmale1                     1.57   1.22    2.01     3.53   0.00   1.16\nage_sigmoid               9.00   5.43   14.93     8.51   0.00   1.17\nsys_bp                    1.02   1.01    1.02     7.17   0.00   1.14\nglucose                   1.01   1.00    1.01     3.56   0.00   1.01\ncigs_per_day              1.02   1.01    1.03     3.45   0.00   1.20\n--------------------------------------------------------------------\n\n Using a threshold for prediction = 0.10 \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 584  39\n         1 761 196\n                                          \n               Accuracy : 0.4937          \n                 95% CI : (0.4687, 0.5186)\n    No Information Rate : 0.8513          \n    P-Value [Acc > NIR] : 1               \n                                          \n                  Kappa : 0.1183          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.8340          \n            Specificity : 0.4342          \n         Pos Pred Value : 0.2048          \n         Neg Pred Value : 0.9374          \n             Prevalence : 0.1487          \n         Detection Rate : 0.1241          \n   Detection Prevalence : 0.6057          \n      Balanced Accuracy : 0.6341          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nCode\nplot_ROC(test_sigmoid_age, phat)\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls < cases\n\n\n\n\n\n\n\n\n\nFinal model using thr = 0.5\nWe decided to include the following variables in the final model:\n\nmale\nage\nsys_bp\nglucose\ncigs_per_day\n\nAt this point, we evaluate the performance of the model in terms of sensitivity (TP/TP+FN) using a default threshold of 0.5 for binary decision on the test set.\nOur aim is to maximise the amount of people at risk which are predicted to be so by the model.\n\n\nCode\n# The main aim is to use the model to identify the highest amount of true positives\n# and minimize false negatives\n\nEVs  <- c(\n   \"male\", \"age\", \"sys_bp\", \"glucose\", \"cigs_per_day\"\n)\n\nphat <- do_logistic_regression(EVs, thresh = 0.5)\n\n\n\nCall:\nglm(formula = formula_obj, family = \"binomial\", data = train_data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1053  -0.5899  -0.4330  -0.2982   2.8162  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -8.643579   0.504565 -17.131  < 2e-16 ***\nmale1         0.449590   0.127385   3.529 0.000417 ***\nage           0.065845   0.007745   8.502  < 2e-16 ***\nsys_bp        0.018684   0.002597   7.194 6.29e-13 ***\nglucose       0.007547   0.002112   3.573 0.000353 ***\ncigs_per_day  0.017858   0.005184   3.445 0.000571 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2114.4  on 2470  degrees of freedom\nResidual deviance: 1875.8  on 2465  degrees of freedom\nAIC: 1887.8\n\nNumber of Fisher Scoring iterations: 5\n\nMODEL INFO:\nObservations: 2471\nDependent Variable: ten_year_chd\nType: Generalized linear model\n  Family: binomial \n  Link function: logit \n\nMODEL FIT:\nχ²(5) = 238.54, p = 0.00\nPseudo-R² (Cragg-Uhler) = 0.16\nPseudo-R² (McFadden) = 0.11\nAIC = 1887.82, BIC = 1922.69 \n\nStandard errors: MLE\n--------------------------------------------------------------------\n                     exp(Est.)   2.5%   97.5%   z val.      p    VIF\n------------------ ----------- ------ ------- -------- ------ ------\n(Intercept)               0.00   0.00    0.00   -17.13   0.00       \nmale1                     1.57   1.22    2.01     3.53   0.00   1.16\nage                       1.07   1.05    1.08     8.50   0.00   1.17\nsys_bp                    1.02   1.01    1.02     7.19   0.00   1.14\nglucose                   1.01   1.00    1.01     3.57   0.00   1.01\ncigs_per_day              1.02   1.01    1.03     3.44   0.00   1.20\n--------------------------------------------------------------------\n\n Using a threshold for prediction = 0.50 \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1337  219\n         1    8   16\n                                          \n               Accuracy : 0.8563          \n                 95% CI : (0.8381, 0.8733)\n    No Information Rate : 0.8513          \n    P-Value [Acc > NIR] : 0.3             \n                                          \n                  Kappa : 0.0987          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.06809         \n            Specificity : 0.99405         \n         Pos Pred Value : 0.66667         \n         Neg Pred Value : 0.85925         \n             Prevalence : 0.14873         \n         Detection Rate : 0.01013         \n   Detection Prevalence : 0.01519         \n      Balanced Accuracy : 0.53107         \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nCode\nplot_ROC(test, phat)\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls < cases\n\n\n\n\n\n\n\n\n\nFinal model using thr = 0.1\nLeaving the default threshold of 0.5 for binary prediction leads to identifying too few true positives. For this reason, we decided to maximise the Sensitivity (TP / (TP+FN)) even at the expense of assigning high risk of CHD to some people who are not.\nThis rationale is motivated by the fact that in this case the decision is not about doing or not doing an open-heart surgery, but rather to start to monitor people who might be at high risk. Plus, according the to the model, an accurate monitoring only requires inexpensive procedures: blood test and pressure measurements.\nFor this reason, it is of paramount important to include all the people who might be at risk even if they will turn out not to be so.\nThe threshold for risk detection is accordingly lowered to 0.1.\nThis leads to determine that almost 50% of the people who are not at risk will be deemed to be actually at risk. However, this choice will lead to a Sensitivity of about 80%.\nNB: We also replace the cigs_per_day with units of tens of cigarettes\n\n\nCode\n# The main aim is to use the model to identify the highest amount of true positives\n# and minimize false negatives\n\n# EVs  <- c(\n#    \"male\", \"age\", \"sys_bp\", \"glucose\", \"cigs_per_day\"\n# )\n# \n# phat <- do_logistic_regression(EVs, thresh = 0.1)\n\n\ntrain_cig_factor <- train %>% \n mutate(tens_cigs = round(cigs_per_day/10,0))\n\ntest_cig_factor <- test %>% \n mutate(tens_cigs = round(cigs_per_day/10,0))\n\n\nEVs  <- c(\n   \"male\", \"age\", \"sys_bp\", \"glucose\", \"tens_cigs\"\n)\n\nphat <- do_logistic_regression(\n EVs, \n train_data = train_cig_factor, \n test_data = test_cig_factor, \n thresh = 0.1\n)\n\n\n\nCall:\nglm(formula = formula_obj, family = \"binomial\", data = train_data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1007  -0.5886  -0.4341  -0.2981   2.8165  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -8.636592   0.503982 -17.137  < 2e-16 ***\nmale1        0.448757   0.127401   3.522 0.000428 ***\nage          0.065813   0.007742   8.501  < 2e-16 ***\nsys_bp       0.018675   0.002597   7.191 6.45e-13 ***\nglucose      0.007510   0.002108   3.562 0.000368 ***\ntens_cigs    0.176640   0.051338   3.441 0.000580 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2114.4  on 2470  degrees of freedom\nResidual deviance: 1875.8  on 2465  degrees of freedom\nAIC: 1887.8\n\nNumber of Fisher Scoring iterations: 5\n\nMODEL INFO:\nObservations: 2471\nDependent Variable: ten_year_chd\nType: Generalized linear model\n  Family: binomial \n  Link function: logit \n\nMODEL FIT:\nχ²(5) = 238.54, p = 0.00\nPseudo-R² (Cragg-Uhler) = 0.16\nPseudo-R² (McFadden) = 0.11\nAIC = 1887.81, BIC = 1922.69 \n\nStandard errors: MLE\n-------------------------------------------------------------------\n                    exp(Est.)   2.5%   97.5%   z val.      p    VIF\n----------------- ----------- ------ ------- -------- ------ ------\n(Intercept)              0.00   0.00    0.00   -17.14   0.00       \nmale1                    1.57   1.22    2.01     3.52   0.00   1.16\nage                      1.07   1.05    1.08     8.50   0.00   1.17\nsys_bp                   1.02   1.01    1.02     7.19   0.00   1.14\nglucose                  1.01   1.00    1.01     3.56   0.00   1.01\ntens_cigs                1.19   1.08    1.32     3.44   0.00   1.20\n-------------------------------------------------------------------\n\n Using a threshold for prediction = 0.10 \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 588  38\n         1 757 197\n                                          \n               Accuracy : 0.4968          \n                 95% CI : (0.4719, 0.5218)\n    No Information Rate : 0.8513          \n    P-Value [Acc > NIR] : 1               \n                                          \n                  Kappa : 0.1218          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.8383          \n            Specificity : 0.4372          \n         Pos Pred Value : 0.2065          \n         Neg Pred Value : 0.9393          \n             Prevalence : 0.1487          \n         Detection Rate : 0.1247          \n   Detection Prevalence : 0.6038          \n      Balanced Accuracy : 0.6377          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nCode\nplot_ROC(test, phat)\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls < cases\n\n\n\n\n\n\n\nCode to manually calculate the Confusion Matrix and its derivatives. Values are not shown. This was just an excercise.\n\n\nCode\n# # Binary prediction from the estimated probability values \n# thresh <- 0.1\n# predicted <- ifelse(phat > thresh, 1, 0)\n# \n# actual_predicted <- tibble(\n#    actual = test$ten_year_chd, \n#    predicted = ifelse(phat > thresh, 1, 0) \n# )\n# \n# cm <- actual_predicted %>%\n#   summarise(\n#     TP = sum(actual == 1 & predicted == 1),\n#     TN = sum(actual == 0 & predicted == 0),\n#     FP = sum(actual == 0 & predicted == 1),\n#     FN = sum(actual == 1 & predicted == 0)\n#   )\n# \n# cm %>% print\n# \n# TP <- cm$TP\n# TN <- cm$TN\n# FP <- cm$FP\n# FN <- cm$FN\n# \n# \n# accuracy <- (TP + TN) / (TP+TN+FP+FN)\n# print(paste0(\"Accuracy = \", round(accuracy,4)))\n# \n# sensitivity <- TP / (TP + FN)\n# print(paste0(\"Sensitivity / Recall / TPR = \", round(sensitivity,4)))\n# \n# specificity <- TN / (TN + FP)\n# print(paste0(\"Specificity / TNR = \", round(specificity,4)))\n# \n# PPV <- TP / (TP+FP)\n# print(paste0(\"Positive Predictive Value = \", round(PPV,2)))\n# \n# NPV <- TN / (TN+FN)\n# print(paste0(\"Negative Predictive Value = \", round(NPV,2)))"
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#automatic-stepwise",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#automatic-stepwise",
    "title": "Framingham CHD logistic regression",
    "section": "Automatic stepwise",
    "text": "Automatic stepwise\nThis particular stepwise regression leads to a much more complex model. It is important to note that the particular order of the predictors can change the final estimated “best” model (based on AIC).\nHowever what is very important is that the cigs_per_day is identified as an important predictor, despite our EDA repeatedly disconfirmed so. The reasons are therefore to be explored, however for the time being we will try to include it into our manually built model (see above.)\n\n\nCode\nlibrary(MASS)\n\nfullModel = glm(\n   ten_year_chd ~ male + prevalent_hyp + bp_meds + age + sys_bp + glucose + tot_chol,\n   family = \"binomial\", data = train\n)\n\nfullModel = glm(ten_year_chd ~ ., family = \"binomial\", data = train)\n\nnullModel = glm(ten_year_chd ~ 1, family = \"binomial\", data = train)\n\nstep_fit <- stepAIC(\n   fullModel, direction = 'backward', \n   scope = list(upper = fullModel, lower = nullModel), trace = 0\n)\n\nstep_fit %>% summary()\n\n\n\nCall:\nglm(formula = ten_year_chd ~ male + age + education + cigs_per_day + \n    prevalent_stroke + prevalent_hyp + sys_bp + glucose, family = \"binomial\", \n    data = train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.0023  -0.5868  -0.4197  -0.2915   2.7940  \n\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       -7.655170   0.587371 -13.033  < 2e-16 ***\nmale1              0.387895   0.129655   2.992 0.002774 ** \nage                0.060750   0.007983   7.610 2.74e-14 ***\neducation2        -0.335970   0.150240  -2.236 0.025337 *  \neducation3        -0.352595   0.184679  -1.909 0.056232 .  \neducation4        -0.038528   0.195294  -0.197 0.843606    \ncigs_per_day       0.019113   0.005227   3.657 0.000256 ***\nprevalent_stroke1  0.969730   0.601408   1.612 0.106868    \nprevalent_hyp1     0.372800   0.161896   2.303 0.021295 *  \nsys_bp             0.013282   0.003412   3.893 9.92e-05 ***\nglucose            0.007787   0.002106   3.698 0.000217 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2114.4  on 2470  degrees of freedom\nResidual deviance: 1860.2  on 2460  degrees of freedom\nAIC: 1882.2\n\nNumber of Fisher Scoring iterations: 5\n\n\nCode\nstep_fit %>% jtools::summ(confint = T, vifs = T, digits = 2) %>% print()\n\n\nMODEL INFO:\nObservations: 2471\nDependent Variable: ten_year_chd\nType: Generalized linear model\n  Family: binomial \n  Link function: logit \n\nMODEL FIT:\nχ²(10) = 254.11, p = 0.00\nPseudo-R² (Cragg-Uhler) = 0.17\nPseudo-R² (McFadden) = 0.12\nAIC = 1882.24, BIC = 1946.18 \n\nStandard errors: MLE\n----------------------------------------------------------------------\n                           Est.    2.5%   97.5%   z val.      p    VIF\n----------------------- ------- ------- ------- -------- ------ ------\n(Intercept)               -7.66   -8.81   -6.50   -13.03   0.00       \nmale1                      0.39    0.13    0.64     2.99   0.00   1.19\nage                        0.06    0.05    0.08     7.61   0.00   1.23\neducation2                -0.34   -0.63   -0.04    -2.24   0.03   1.09\neducation3                -0.35   -0.71    0.01    -1.91   0.06   1.09\neducation4                -0.04   -0.42    0.34    -0.20   0.84   1.09\ncigs_per_day               0.02    0.01    0.03     3.66   0.00   1.22\nprevalent_stroke1          0.97   -0.21    2.15     1.61   0.11   1.01\nprevalent_hyp1             0.37    0.06    0.69     2.30   0.02   1.85\nsys_bp                     0.01    0.01    0.02     3.89   0.00   1.97\nglucose                    0.01    0.00    0.01     3.70   0.00   1.02\n----------------------------------------------------------------------\n\n\nCode\n# Estimated probability values from the logistic model\nphat_step_fit = predict(step_fit, newdata = test, type = \"response\")\n\n\nEvaluate predictivity of the stepwise model\n\n\nCode\n# Binary prediction from the estimated probability values \nthresh <- 0.1\npredicted <- ifelse(phat_step_fit > thresh, 1, 0)\n\nactual_predicted <- tibble(\n   actual = test$ten_year_chd, \n   predicted = ifelse(phat_step_fit > thresh, 1, 0) \n)\n\ncaret::confusionMatrix(\n   as.factor(predicted), # predicted\n   as.factor(test$ten_year_chd), # actual \n   positive = \"1\"  # the positive class in our case is \"1\"\n)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 608  38\n         1 737 197\n                                          \n               Accuracy : 0.5095          \n                 95% CI : (0.4845, 0.5344)\n    No Information Rate : 0.8513          \n    P-Value [Acc > NIR] : 1               \n                                          \n                  Kappa : 0.1304          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.8383          \n            Specificity : 0.4520          \n         Pos Pred Value : 0.2109          \n         Neg Pred Value : 0.9412          \n             Prevalence : 0.1487          \n         Detection Rate : 0.1247          \n   Detection Prevalence : 0.5911          \n      Balanced Accuracy : 0.6452          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nCode\nplot_ROC(test, phat_step_fit)\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls < cases"
  },
  {
    "objectID": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#samples-balanced-for-chd-risk",
    "href": "showcase/Framingham_CHD_logistic_regression/Graded_Task_Logistic_Regression_V5.html#samples-balanced-for-chd-risk",
    "title": "Framingham CHD logistic regression",
    "section": "Samples balanced for CHD risk",
    "text": "Samples balanced for CHD risk\nOur sample is very unbalanced. We will select an equal number of people with and without risk in both train and test set to assess whether this will help model performance.\n\n\nCode\nset.seed(9999)\n\nmin_train <- min(\n   nrow(train[train$ten_year_chd == 1,]),\n   nrow(train[train$ten_year_chd == 0,])   \n)\n\nbalanced_train <- train %>% \n   group_by(ten_year_chd) %>% \n   slice_sample(n = min_train)\n\n\nmin_test <- min(\n   nrow(test[test$ten_year_chd == 1,]),\n   nrow(test[test$ten_year_chd == 0,])   \n)\n\nbalanced_test <- test %>% \n   group_by(ten_year_chd) %>% \n   slice_sample(n = min_test)\n\n\n# Full model with the selected variables\nEVs  <- c(\n   \"male\", \"prevalent_hyp\", \"bp_meds\", \"education\",\n   \"age\", \"sys_bp\", \"bmi\", \"glucose\", \"tot_chol\"\n)\n\n\n# Remove education, bp_meds, bmi, tot_chol since they have\n# no effect on the model\nEVs  <- c(\n   \"male\", \"prevalent_hyp\",\n   \"age\", \"sys_bp\", \"glucose\"\n)\n\n\n# Remove prevalent_hyp since it is only marginally significant\nEVs  <- c(\n   \"male\", \"age\", \"sys_bp\", \"glucose\"\n)\n\n\n# The stepwise regression - below - identified cigs_per_day as an important factor.\n# I will therefore add it to the model.\nEVs  <- c(\n   \"male\", \"age\", \"sys_bp\", \"glucose\", \"cigs_per_day\"\n)\n\n\nphat <- do_logistic_regression(\n   EVs, \n   train_data = balanced_train, test_data = balanced_test,\n   thresh = 0.4\n)\n\n\n\nCall:\nglm(formula = formula_obj, family = \"binomial\", data = train_data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1032  -1.0034  -0.1717   1.0161   2.0910  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -7.238967   0.720868 -10.042  < 2e-16 ***\nmale1         0.319629   0.169510   1.886  0.05935 .  \nage           0.074519   0.010682   6.976 3.04e-12 ***\nsys_bp        0.015708   0.003688   4.259 2.06e-05 ***\nglucose       0.010113   0.003923   2.578  0.00994 ** \ncigs_per_day  0.030757   0.007706   3.991 6.57e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1048.04  on 755  degrees of freedom\nResidual deviance:  910.19  on 750  degrees of freedom\nAIC: 922.19\n\nNumber of Fisher Scoring iterations: 4\n\nMODEL INFO:\nObservations: 756\nDependent Variable: ten_year_chd\nType: Generalized linear model\n  Family: binomial \n  Link function: logit \n\nMODEL FIT:\nχ²(5) = 137.85, p = 0.00\nPseudo-R² (Cragg-Uhler) = 0.22\nPseudo-R² (McFadden) = 0.13\nAIC = 922.19, BIC = 949.96 \n\nStandard errors: MLE\n--------------------------------------------------------------------\n                     exp(Est.)   2.5%   97.5%   z val.      p    VIF\n------------------ ----------- ------ ------- -------- ------ ------\n(Intercept)               0.00   0.00    0.00   -10.04   0.00       \nmale1                     1.38   0.99    1.92     1.89   0.06   1.12\nage                       1.08   1.06    1.10     6.98   0.00   1.18\nsys_bp                    1.02   1.01    1.02     4.26   0.00   1.11\nglucose                   1.01   1.00    1.02     2.58   0.01   1.01\ncigs_per_day              1.03   1.02    1.05     3.99   0.00   1.20\n--------------------------------------------------------------------\n\n Using a threshold for prediction = 0.40 \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 115  44\n         1 120 191\n                                          \n               Accuracy : 0.6511          \n                 95% CI : (0.6061, 0.6941)\n    No Information Rate : 0.5             \n    P-Value [Acc > NIR] : 2.818e-11       \n                                          \n                  Kappa : 0.3021          \n                                          \n Mcnemar's Test P-Value : 4.727e-09       \n                                          \n            Sensitivity : 0.8128          \n            Specificity : 0.4894          \n         Pos Pred Value : 0.6141          \n         Neg Pred Value : 0.7233          \n             Prevalence : 0.5000          \n         Detection Rate : 0.4064          \n   Detection Prevalence : 0.6617          \n      Balanced Accuracy : 0.6511          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nCode\nplot_ROC(balanced_test, phat)\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls < cases"
  }
]